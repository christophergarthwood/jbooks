{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use Grid Search in scikit-learn\n",
    "\n",
    "Overview\n",
    "\n",
    "Hyperparameters are the variables which determines the network structure(Eg: Number of Hidden Units) and the variables which determine how the network is trained(Eg: Learning Rate).\n",
    "\n",
    "Hyperparameters are set before training(before optimizing the weights and bias).\n",
    "\n",
    "Iterate through hyper-parameters to try and gauge the best potential set for your final model execution / setup.  \n",
    "*Note: I've read multiple papers that span arguments ranging from a grid search pattern, to intuition, to intimate knowledge of the data and random selection.  There is no proven, specific way to tailor your hyper-parameters.\"\n",
    "\n",
    "References:\n",
    "+ https://machinelearningmastery.com/\n",
    "+ https://towardsdatascience.com/hyperparameters-in-deep-learning-927f7b2084dd\n",
    "+ https://towardsdatascience.com/what-are-hyperparameters-and-how-to-tune-the-hyperparameters-in-a-deep-neural-network-d0604917584a\n",
    "+ https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.7.3\n",
    "############################################\n",
    "# INCLUDES\n",
    "############################################\n",
    "#libraries specific to this example\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# seed the pseudorandom number generator\n",
    "from random import seed\n",
    "from random import random\n",
    "from random import randint\n",
    "\n",
    "#a set of libraries that perhaps should always be in Python source\n",
    "import os \n",
    "import datetime\n",
    "import sys\n",
    "import gc\n",
    "import getopt\n",
    "import inspect\n",
    "import math\n",
    "import warnings\n",
    "import types\n",
    "\n",
    "#Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#a darn useful library for creating paths and one I recommend you load to your environment\n",
    "from pathlib import Path\n",
    "\n",
    "# can type in the python console `help(name of function)` to get the documentation\n",
    "from pydoc import help                          \n",
    "\n",
    "#Import a custom library, in this case a fairly useful logging framework\n",
    "debug_lib_location = Path(\"./\")\n",
    "sys.path.append(str(debug_lib_location))\n",
    "import debug\n",
    "\n",
    "warnings.filterwarnings('ignore')               # don't print out warnings\n",
    "\n",
    "\n",
    "root_location=\"..\" + os.sep + \"data\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn on Eager Execution\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#JUPYTER NOTEBOOK OUTPUT CONTROL / FORMATTING\n",
    "############################################\n",
    "#set floating point to 4 places to things don't run loose\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# GLOBAL VARIABLES\n",
    "############################################\n",
    "DEBUG = 1                            #General ledger output so you know what's happening.\n",
    "DEBUG_DATA = 1                       #Extremely verbose output, change to zero (0) to supress the volume of output.\n",
    "\n",
    "# CODE CONSTRAINTS\n",
    "VERSION_NAME    = \"HyperParameterGridSearch\"\n",
    "VERSION_ACRONYM = \"ML-HPGS\"\n",
    "VERSION_MAJOR   = 0\n",
    "VERSION_MINOR   = 0\n",
    "VERSION_RELEASE = \"6\"\n",
    "VERSION_TITLE   = VERSION_NAME + \" (\" + VERSION_ACRONYM + \") \" + str(VERSION_MAJOR) + \".\" + str(VERSION_MINOR) + \".\" + str(VERSION_RELEASE) + \" generated SEED.\"\n",
    "\n",
    "ENCODING  =\"utf-8\"\n",
    "############################################\n",
    "# GLOBAL CONSTANTS\n",
    "############################################\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "############################################\n",
    "# APPLICATION VARIABLES\n",
    "############################################\n",
    "SEED_INIT = 7\n",
    "LAYER1_NEURONS=12\n",
    "LAYER1_INPUT_DIMS=8\n",
    "LAYER1_ACTIVATION='relu'\n",
    "LAYER2_NEURONS=1\n",
    "LAYER2_ACTIVATION='sigmoid'\n",
    "MODEL_LOSS='binary_crossentropy'\n",
    "MODEL_ACTIVATION='adam'\n",
    "MODEL_METRICS=['accuracy']\n",
    "\n",
    "DATASET_FILENAME=root_location+os.sep+'pima-indians-diabetes.csv'\n",
    "DATASET_DELIMITER=','\n",
    "\n",
    "GRID_SEARCH_NJOBS=-1\n",
    "\n",
    "############################################\n",
    "# GLOBAL CONFIGURATION\n",
    "############################################\n",
    "os.environ['PYTHONIOENCODING']=ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Function Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# WARNING / ERROR Management\n",
    "############################################\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "\n",
    "############################################\n",
    "# FUNCTIONS\n",
    "############################################\n",
    "\n",
    "def prototype(incMonth):\n",
    "\n",
    "    debug.msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
    "    debug.msg_info(\"The month you passed in was \" + str(incMonth))\n",
    "    debug.msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
    "    return 1\n",
    "\n",
    "def lib_diagnostics():\n",
    "    debug.msg_debug(\"System version    #:{:>12}\".format(sys.version))\n",
    "    netcdf4_version_info = nc.getlibversion().split(\" \")\n",
    "    debug.msg_debug(\"netCDF4 version   #:{:>12}\".format(netcdf4_version_info[0]))\n",
    "    debug.msg_debug(\"Matplotlib version#:{:>12}\".format(matplt.__version__))\n",
    "    debug.msg_debug(\"Numpy version     #:{:>12}\".format(np.__version__))\n",
    "    debug.msg_debug(\"Pandas version    #:{:>12}\".format(pd.__version__))\n",
    "    debug.msg_debug(\"SciPy version     #:{:>12}\".format(sp.__version__))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def get_full_version():\n",
    "\n",
    "    resultant = str(VERSION_NAME) + \"  v\" + str(VERSION_MAJOR) + \".\" + str(VERSION_MINOR) + \".\" + str(VERSION_RELEASE)\n",
    "    return resultant\n",
    "\n",
    "def get_version():\n",
    "\n",
    "    resultant = str(VERSION_MAJOR) + \".\" + str(VERSION_MINOR) + \".\" + str(VERSION_RELEASE)\n",
    "    return resultant\n",
    "\n",
    "def printversion():\n",
    "\n",
    "    print(get_full_version())\n",
    "\n",
    "def printusage():\n",
    "\n",
    "    print(\"\")\n",
    "    printversion()\n",
    "    print(\"  -v, --version    prints the version of this software package.\")\n",
    "    print(\"\")\n",
    "    print(\"  * - indicates required argument.\")\n",
    "\n",
    "######################################################################\n",
    "#Support routines to see columns in DataFrames\n",
    "######################################################################\n",
    "def show_columns_plain(inc_ary):\n",
    "    new_ary = []\n",
    "    for col in inc_ary:\n",
    "        new_ary.append(np.char.lower(col))\n",
    "    new_ary.sort\n",
    "    myOutputString = \" \"\n",
    "    for col in new_ary:\n",
    "        myOutputString = myOutputString + \" \" + str(col)\n",
    "    return myOutputString\n",
    "\n",
    "def show_columns_true(inc_ary):\n",
    "    new_ary = []\n",
    "    for col in inc_ary:\n",
    "        new_ary.append(col)\n",
    "    new_ary.sort\n",
    "    myOutputString = \" \"\n",
    "    for col in new_ary:\n",
    "        myOutputString = myOutputString + \" \" + str(col)\n",
    "    return myOutputString\n",
    "\n",
    "######################################################################\n",
    "#Input Validation\n",
    "######################################################################\n",
    "# valid string:\n",
    "#  We don't want the following:\n",
    "#   - at the start of the file name (might be construed as a switch)\n",
    "#  $, &, |, ;, <, >, `, !, *, \", \\ (to start with)\n",
    "###\n",
    "def validstring(testsubject):\n",
    "\n",
    "    if testsubject[0] == \"-\":\n",
    "        return 0\n",
    "    elif \"$\" in testsubject or \"&\" in testsubject or \"|\" in testsubject:\n",
    "        return 0\n",
    "    elif \";\" in testsubject or \"`\" in testsubject or \"!\" in testsubject:\n",
    "        return 0\n",
    "    elif \"*\" in testsubject or '\"' in testsubject or \"\\\\\" in testsubject:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./libs.py\n",
    "#Title:     Displays the libraries in current use.\n",
    "#Objective: Invocation is intended as function calls within another program.\n",
    "#Assumptions:\n",
    "#           1. Should be stored in standardized location such as:\n",
    "#                      /p/home/{user_name}/usr/PYTHONLIB\n",
    "#           2. Developer loads the module (Jupyter Lab).\n",
    "#Pre-Requisites:\n",
    "#           1. Python v3.*\n",
    "#           2. Jupyter Lab / Notebook (%load libs.py)\n",
    "#Usage:\n",
    "#       %load libs.py\n",
    "#       find_loaded_modules().HTML\n",
    "#\n",
    "#Version History:\n",
    "# ------------------------------------------------------------------------\n",
    "# Version   Date       Modification                              Author\n",
    "# ------------------------------------------------------------------------\n",
    "# 1.0       2020/04/29 Inception                                 Radiance\n",
    "# ------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#LIBRARIES\n",
    "#######################################################################\n",
    "import os\n",
    "import types\n",
    "\n",
    "def module_version(mod):\n",
    "    '''Return version string for module *mod*, or nothing if\n",
    "    it doesn't have a \"version\" or \"__version__\" attribute.'''\n",
    "    version = []\n",
    "    if hasattr(mod, '__dict__'):\n",
    "        keys = []\n",
    "        for key in mod.__dict__.keys():\n",
    "            if key.lower() == 'version' or key.lower() == '__version__':\n",
    "                v = mod.__dict__[key]\n",
    "                if (str):\n",
    "                    if isinstance(v, str):\n",
    "                        version.append(v)\n",
    "                else:\n",
    "                    version.append(\"No version\")\n",
    "        if keys:\n",
    "            print (mod, keys)\n",
    "    if version:\n",
    "        return ', '.join(version)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def find_loaded_modules(only_versioned_modules=True):\n",
    "\n",
    "    def list_of_lists_to_HTML(lists, header_row=None):\n",
    "        '''Convert a list of a list of strings to a HTML table.'''\n",
    "        s = '<table>'\n",
    "        if header_row:\n",
    "            s += '\\n\\t<tr>\\n\\t\\t'\n",
    "            s += ''.join(['<th>%s</th>' % item for item in header_row])\n",
    "            s += '\\n\\t</tr>'\n",
    "        for inner_list in lists:\n",
    "            s += '\\n\\t<tr>\\n\\t\\t'\n",
    "            s += ''.join(['<td>%s</td>' % item for item in inner_list])\n",
    "            s += '\\n\\t</tr>'\n",
    "        s += '\\n</table>'\n",
    "        return s\n",
    "    \n",
    "    class LoadedModules(list):\n",
    "        '''Very simple wrapper for a list of lists of strings, with an attribute\n",
    "        for display in IPython Notebooks.'''\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            list.__init__(self, *args, **kwargs)\n",
    "            \n",
    "        @property\n",
    "        def HTML(self):\n",
    "            from IPython.display import HTML\n",
    "            return HTML(\n",
    "                    list_of_lists_to_HTML(\n",
    "                            self, header_row=['Name', 'Version']))\n",
    "                    \n",
    "    objs = LoadedModules()\n",
    "    for i, mod in enumerate(globals().values()):\n",
    "        if isinstance(mod, types.ModuleType):\n",
    "            if hasattr(mod, '__name__'):\n",
    "                name = mod.__name__\n",
    "            else:\n",
    "                name = ''\n",
    "            \n",
    "            version = module_version(mod)\n",
    "            \n",
    "            objs.append([mod.__name__, version])\n",
    "    objs.sort(key=lambda r: r[0])\n",
    "    return objs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "\t<tr>\n",
       "\t\t<th>Name</th><th>Version</th>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>builtins</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>builtins</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>datetime</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>debug</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>gc</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>getopt</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>inspect</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>math</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>matplotlib</td><td>3.2.2</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>matplotlib.pyplot</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>numpy</td><td>1.19.5</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>os</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>pandas</td><td>1.2.3</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>scipy</td><td>1.7.3</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>scipy</td><td>1.7.3</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>sys</td><td>3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:37:49) [MSC v.1916 64 bit (AMD64)]</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>tensorflow</td><td>2.3.0</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>tensorflow.keras</td><td>2.4.0</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>tensorflow.keras.layers</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>types</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>warnings</td><td></td>\n",
       "\t</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_loaded_modules().HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Tune Batch Size and Number of Epochs\n",
    "\n",
    "In this first simple example, we look at tuning the batch size and number of epochs used when fitting the network.\n",
    "\n",
    "The batch size in iterative gradient descent is the number of patterns shown to the network before the weights are updated. It is also an optimization in the training of the network, defining how many patterns to read at a time and keep in memory.\n",
    "\n",
    "The number of epochs is the number of times that the entire training dataset is shown to the network during training. Some networks are sensitive to the batch size, such as LSTM recurrent neural networks and Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Function to create model, required for KerasClassifier\n",
    "#######################################################################\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(LAYER1_NEURONS, input_dim=LAYER1_INPUT_DIMS, activation=LAYER1_ACTIVATION))\n",
    "    model.add(Dense(LAYER2_NEURONS, activation=LAYER2_ACTIVATION))\n",
    "    # Compile model\n",
    "    model.compile(loss=MODEL_LOSS, optimizer=MODEL_ACTIVATION, metrics=MODEL_METRICS)\n",
    "    return model\n",
    "\n",
    "#######################################################################\n",
    "# fix random seed for reproducibility\n",
    "# Pop quiz!  Why specify the seed?  To ensure we get reproducible results\n",
    "# when evaluating our progress during this test / evaluation period.\n",
    "#######################################################################\n",
    "seed = SEED_INIT\n",
    "np.random.seed(seed)\n",
    "\n",
    "#######################################################################\n",
    "# load dataset\n",
    "#######################################################################\n",
    "dataset = np.loadtxt(DATASET_FILENAME, delimiter=DATASET_DELIMITER)\n",
    "\n",
    "#######################################################################\n",
    "# split into input (X) and output (Y) variables\n",
    "#######################################################################\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "#######################################################################\n",
    "# create model\n",
    "#######################################################################\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "#######################################################################\n",
    "# define the grid search parameters\n",
    "#######################################################################\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "\n",
    "#######################################################################\n",
    "# define the grid search parameters, instantiate GridSearchCV\n",
    "#######################################################################\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=GRID_SEARCH_NJOBS, cv=3)\n",
    "\n",
    "#######################################################################\n",
    "# fit the data to the model\n",
    "#######################################################################\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "#######################################################################\n",
    "# summarize results\n",
    "#######################################################################\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Tune the Training Optimization Algorithm\n",
    "\n",
    "Keras offers a suite of different state-of-the-art optimization algorithms.\n",
    "\n",
    "Tune the optimization algorithm used to train the network, each with default parameters.\n",
    "\n",
    "Evaluate the suite of optimization algorithms supported by the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.696615 using {'inc_optimizer': 'Adamax'}\n",
      "0.658854 (0.023939) with: {'inc_optimizer': 'SGD'}\n",
      "0.690104 (0.031466) with: {'inc_optimizer': 'RMSprop'}\n",
      "0.601562 (0.060851) with: {'inc_optimizer': 'Adagrad'}\n",
      "0.546875 (0.147854) with: {'inc_optimizer': 'Adadelta'}\n",
      "0.683594 (0.014616) with: {'inc_optimizer': 'Adam'}\n",
      "0.696615 (0.020505) with: {'inc_optimizer': 'Adamax'}\n",
      "0.695312 (0.016877) with: {'inc_optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# Function to create model, required for KerasClassifier\n",
    "#   argument of optimzer used to alter the model\n",
    "#######################################################################\n",
    "def create_model(inc_optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(LAYER1_NEURONS, input_dim=LAYER1_INPUT_DIMS, activation=LAYER1_ACTIVATION))\n",
    "    model.add(Dense(LAYER2_NEURONS, activation=LAYER2_ACTIVATION))\n",
    "    # Compile model\n",
    "    model.compile(loss=MODEL_LOSS, optimizer=inc_optimizer, metrics=MODEL_METRICS)\n",
    "    return model\n",
    "#######################################################################\n",
    "# fix random seed for reproducibility\n",
    "#######################################################################\n",
    "seed = SEED_INIT\n",
    "np.random.seed(seed)\n",
    "\n",
    "#######################################################################\n",
    "# load dataset\n",
    "#######################################################################\n",
    "dataset = np.loadtxt(DATASET_FILENAME, delimiter=DATASET_DELIMITER)\n",
    "\n",
    "#######################################################################\n",
    "# split into input (X) and output (Y) variables\n",
    "#######################################################################\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "#######################################################################\n",
    "# create model\n",
    "#######################################################################\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "#######################################################################\n",
    "# define the grid search parameters\n",
    "#######################################################################\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(inc_optimizer=optimizer)\n",
    "\n",
    "#######################################################################\n",
    "# Instantiate the GridSearch class and then fit the data\n",
    "# Note that \"grid\" invokes a modile build/compilation and iterates through\n",
    "# the parameters provided.\n",
    "#######################################################################\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=GRID_SEARCH_NJOBS, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "#######################################################################\n",
    "# summarize results\n",
    "#######################################################################\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Tune Learning Rate and Momentum\n",
    "\n",
    "It is common to pre-select an optimization algorithm to train your network and tune its parameters.\n",
    "\n",
    "By far the most common optimization algorithm is plain old Stochastic Gradient Descent (SGD) because it is so well understood. Optimize the SGD learning rate and momentum parameters.\n",
    "\n",
    "Learning rate controls how much to update the weight at the end of each batch and the momentum controls how much to let the previous update influence the current weight update.\n",
    "\n",
    "Try a suite of small standard learning rates and a momentum values from 0.2 to 0.8 in steps of 0.2, as well as 0.9 (because it can be a popular value in practice).\n",
    "\n",
    "Generally, it is a good idea to also include the number of epochs in an optimization like this as there is a dependency between the amount of learning per batch (learning rate), the number of updates per epoch (batch size) and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.697917 using {'learn_rate': 0.001, 'momentum': 0.6}\n",
      "0.602865 (0.062364) with: {'learn_rate': 0.001, 'momentum': 0.0}\n",
      "0.673177 (0.024360) with: {'learn_rate': 0.001, 'momentum': 0.2}\n",
      "0.695312 (0.016877) with: {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "0.697917 (0.009744) with: {'learn_rate': 0.001, 'momentum': 0.6}\n",
      "0.661458 (0.025976) with: {'learn_rate': 0.001, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.001, 'momentum': 0.9}\n",
      "0.666667 (0.014731) with: {'learn_rate': 0.01, 'momentum': 0.0}\n",
      "0.648438 (0.025315) with: {'learn_rate': 0.01, 'momentum': 0.2}\n",
      "0.649740 (0.026557) with: {'learn_rate': 0.01, 'momentum': 0.4}\n",
      "0.649740 (0.026557) with: {'learn_rate': 0.01, 'momentum': 0.6}\n",
      "0.649740 (0.026557) with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.01, 'momentum': 0.9}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.0}\n",
      "0.652344 (0.022999) with: {'learn_rate': 0.1, 'momentum': 0.2}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.6}\n",
      "0.649740 (0.024360) with: {'learn_rate': 0.1, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.9}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.0}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.2}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.6}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.9}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.0}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.2}\n",
      "0.649740 (0.026557) with: {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.6}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "#######################################################################\n",
    "# Function to create model, required for KerasClassifier\n",
    "#   argument of learning rate and momentum used to alter the model\n",
    "#   in this case the SGD optimizer is instantiated with arguments\n",
    "#######################################################################\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "  # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(LAYER1_NEURONS, input_dim=LAYER1_INPUT_DIMS, activation=LAYER1_ACTIVATION))\n",
    "    model.add(Dense(LAYER2_NEURONS, activation=LAYER2_ACTIVATION))\n",
    "    # Compile model\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    model.compile(loss=MODEL_LOSS, optimizer=optimizer, metrics=MODEL_METRICS) \n",
    "    return model\n",
    "\n",
    "#######################################################################\n",
    "# fix random seed for reproducibility\n",
    "#######################################################################\n",
    "seed = SEED_INIT\n",
    "np.random.seed(seed)\n",
    "\n",
    "#######################################################################\n",
    "# load dataset\n",
    "#######################################################################\n",
    "dataset = np.loadtxt(DATASET_FILENAME, delimiter=DATASET_DELIMITER)\n",
    "\n",
    "#######################################################################\n",
    "# split into input (X) and output (Y) variables\n",
    "#######################################################################\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "#######################################################################\n",
    "# create model\n",
    "#######################################################################\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "#######################################################################\n",
    "# define the grid search parameters\n",
    "#######################################################################\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "\n",
    "#######################################################################\n",
    "# Instantiate GridSearchCV which builds the model and iterates through\n",
    "# parameters performing a fit and aggregating results\n",
    "#######################################################################\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perturbing ALL of the Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 87318 candidates, totalling 261954 fits\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "#######################################################################\n",
    "# Function to create model, required for KerasClassifier\n",
    "#   argument of learning rate and momentum used to alter the model\n",
    "#   in this case the SGD optimizer is instantiated with arguments\n",
    "#######################################################################\n",
    "def create_model(inc_neurons, inc_activation, inc_optimizer, inc_loss, inc_learn_rate=0.01, inc_momentum=0):\n",
    "  # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(inc_neurons, input_dim=LAYER1_INPUT_DIMS, activation=inc_activation))\n",
    "    model.add(Dense(LAYER2_NEURONS, activation=LAYER2_ACTIVATION))\n",
    "    # Compile model\n",
    "    #optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    model.compile(loss=inc_loss, optimizer=inc_optimizer, metrics=MODEL_METRICS) \n",
    "    return model\n",
    "\n",
    "#######################################################################\n",
    "# fix random seed for reproducibility\n",
    "#######################################################################\n",
    "seed = SEED_INIT\n",
    "np.random.seed(seed)\n",
    "\n",
    "#######################################################################\n",
    "# load dataset\n",
    "#######################################################################\n",
    "dataset = np.loadtxt(DATASET_FILENAME, delimiter=DATASET_DELIMITER)\n",
    "\n",
    "#######################################################################\n",
    "# split into input (X) and output (Y) variables\n",
    "#######################################################################\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "#######################################################################\n",
    "# create model\n",
    "#######################################################################\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "#######################################################################\n",
    "# define the grid search parameters\n",
    "#######################################################################\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "\n",
    "batch_size = [10]\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "\n",
    "epochs = [10]\n",
    "epochs = [10, 50, 100]\n",
    "\n",
    "optimizer = ['SGD']\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "losses =['binary_crossentropy']\n",
    "losses =['binary_crossentropy', 'sparse_categorical_crossentropy','poisson',\n",
    "         'mean_squared_error','mean_absolute_error','mean_absolute_percentage_error', \n",
    "         'mean_squared_logarithmic_error','cosine_similarity', 'huber_loss']\n",
    "\n",
    "\n",
    "neurons=[1, 5]\n",
    "neurons=[1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "activation=['relu']\n",
    "activation=['relu', 'sigmoid', 'softmax', 'softplus', 'softsign', 'tanh', 'selu']\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  inc_optimizer=optimizer,\n",
    "                  inc_loss=losses,\n",
    "                  inc_activation=activation,\n",
    "                  inc_neurons=neurons)\n",
    "\n",
    "#######################################################################\n",
    "# Instantiate GridSearchCV which builds the model and iterates through\n",
    "# parameters performing a fit and aggregating results\n",
    "# verbose=(the greater the positive value the more detail)\n",
    "#######################################################################\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=10)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
