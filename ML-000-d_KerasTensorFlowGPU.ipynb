{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow GPU\n",
    "\n",
    "TensorFlow code, and tf.keras models will transparently run on a single GPU with no code changes required.\n",
    "Note: Use tf.config.experimental.list_physical_devices('GPU') to confirm that TensorFlow is using the GPU.\n",
    "\n",
    "The simplest way to run on multiple GPUs, on one or many machines, is using Distribution Strategies.\n",
    "\n",
    "Reference: \n",
    "+ https://www.tensorflow.org/guide/gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.7.3\n",
    "############################################\n",
    "# INCLUDES\n",
    "############################################\n",
    "#libraries specific to this example\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# seed the pseudorandom number generator\n",
    "from random import seed\n",
    "from random import random\n",
    "from random import randint\n",
    "\n",
    "#a set of libraries that perhaps should always be in Python source\n",
    "import os \n",
    "import datetime\n",
    "import sys\n",
    "import gc\n",
    "import getopt\n",
    "import inspect\n",
    "import math\n",
    "import warnings\n",
    "import types\n",
    "\n",
    "#Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib as matplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# can type in the python console `help(name of function)` to get the documentation\n",
    "from pydoc import help                          \n",
    "\n",
    "#a darn useful library for creating paths and one I recommend you load to your environment\n",
    "from pathlib import Path\n",
    "\n",
    "#Import a custom library, in this case a fairly useful logging framework\n",
    "if os.environ.get('LIB_LOCATION') is not None:\n",
    "    debug_lib_location = Path(os.getenv('LIB_LOCATION'))\n",
    "else:\n",
    "    debug_lib_location = Path(\"./\")\n",
    "                              \n",
    "if os.environ.get('DATA_LOCATION') is not None:\n",
    "    root_location = os.getenv('DATA_LOCATION')\n",
    "else:\n",
    "    root_location=\"..\" + os.sep + \"data\";                              \n",
    "\n",
    "\n",
    "sys.path.append(str(debug_lib_location))\n",
    "import debug\n",
    "\n",
    "warnings.filterwarnings('ignore')               # don't print out warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# GLOBAL VARIABLES\n",
    "############################################\n",
    "DEBUG = 1\n",
    "DEBUG_DATA = 0\n",
    "\n",
    "# CODE CONSTRAINTS\n",
    "VERSION_NAME    = \"TensorFlowGPU\"\n",
    "VERSION_ACRONYM = \"ML-TFGPU\"\n",
    "VERSION_MAJOR   = 0\n",
    "VERSION_MINOR   = 0\n",
    "VERSION_RELEASE = \"0d\"\n",
    "VERSION_TITLE   = VERSION_NAME + \" (\" + VERSION_ACRONYM + \") \" + str(VERSION_MAJOR) + \".\" + str(VERSION_MINOR) + \".\" + str(VERSION_RELEASE) + \" generated SEED.\"\n",
    "\n",
    "#used for values outside standard ASCII, just do it, you'll need it\n",
    "ENCODING  =\"utf-8\"\n",
    "\n",
    "############################################\n",
    "# GLOBAL CONSTANTS\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "# APPLICATION VARIABLES\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "# GLOBAL CONFIGURATION\n",
    "############################################\n",
    "os.environ['PYTHONIOENCODING']=ENCODING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Defining a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lib_diagnostics():\n",
    "    debug.msg_debug(\"System version    #:{:>12}\".format(sys.version))\n",
    "    try:\n",
    "        netcdf4_version_info = nc.getlibversion().split(\" \")\n",
    "        debug.msg_debug(\"netCDF4 version   #:{:>12}\".format(netcdf4_version_info[0]))\n",
    "    except:\n",
    "        print(\"NetCDF4 lib not present.\")\n",
    "    debug.msg_debug(\"Matplotlib version#:{:>12}\".format(matplt.__version__))\n",
    "    debug.msg_debug(\"Numpy version     #:{:>12}\".format(np.__version__))\n",
    "    debug.msg_debug(\"Pandas version    #:{:>12}\".format(pd.__version__))\n",
    "    debug.msg_debug(\"SciPy version     #:{:>12}\".format(sp.__version__))\n",
    "    debug.msg_debug(\"TensorFlow version#:{:>12}\".format(tf.__version__))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Invocation\n",
    "### Note that it's also useful to use this code so that you carry around a list of version dependencies and know how you did something (version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-29 16:46:21 UTC]   DEBUG: System version    #:3.9.15 (main, Nov  4 2022, 16:13:54) \n",
      "[GCC 11.2.0] \n",
      "NetCDF4 lib not present.\n",
      "[2022-12-29 16:46:21 UTC]   DEBUG: Matplotlib version#:       3.5.3 \n",
      "[2022-12-29 16:46:21 UTC]   DEBUG: Numpy version     #:      1.23.4 \n",
      "[2022-12-29 16:46:21 UTC]   DEBUG: Pandas version    #:       1.5.1 \n",
      "[2022-12-29 16:46:21 UTC]   DEBUG: SciPy version     #:       1.9.3 \n",
      "[2022-12-29 16:46:21 UTC]   DEBUG: TensorFlow version#:       2.9.1 \n"
     ]
    }
   ],
   "source": [
    "lib_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12718437781381223688\n",
      "xla_global_id: -1\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5198276259448055985\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 16:46:22.934741: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-29 16:46:22.943555: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x6d03220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-29 16:46:22.943604: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Host, Default Version\n",
      "2022-12-29 16:46:22.945790: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib:/usr/lib64:/common/intel/composer_xe_2015.2.164/compiler/lib/intel64/:/common/NCL_6.5.0_gcc485/lib/\n",
      "2022-12-29 16:46:22.945818: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-29 16:46:22.945844: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-64b99e): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What GPU's are present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-29 16:46:29 UTC]   DEBUG: Num GPUs Available: 0 \n"
     ]
    }
   ],
   "source": [
    "debug.msg_debug(\"Num GPUs Available: \" + str(len(tf.config.experimental.list_physical_devices('GPU'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What CPU's are present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-29 16:46:30 UTC]   DEBUG: Num CPUs Available: 1 \n"
     ]
    }
   ],
   "source": [
    "debug.msg_debug(\"Num CPUs Available: \" + str(len(tf.config.experimental.list_physical_devices('CPU'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "TensorFlow supports running computations on a variety of types of devices, including CPU and GPU. They are represented with string identifiers for example:\n",
    "\n",
    "    \"/device:CPU:0\": The CPU of your machine.\n",
    "    \"/GPU:0\": Short-hand notation for the first GPU of your machine that is visible to TensorFlow.\n",
    "    \"/job:localhost/replica:0/task:0/device:GPU:1\": Fully qualified name of the second GPU of your machine that is visible to TensorFlow.\n",
    "\n",
    "If a TensorFlow operation has both CPU and GPU implementations, by default the GPU devices will be given priority when the operation is assigned to a device. For example, tf.matmul has both CPU and GPU kernels. On a system with devices CPU:0 and GPU:0, the GPU:0 device will be selected to run tf.matmul unless you explicitly request running it on another device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging device placement\n",
    "\n",
    "To find out which devices your operations and tensors are assigned to, put tf.debugging.set_log_device_placement(True) as the first statement of your program. Enabling device placement logging causes any Tensor allocations or operations to be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op _MklMatMul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "[2022-12-29 16:46:34 UTC]   DEBUG: tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32) \n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "debug.msg_debug(str(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual device placement\n",
    "\n",
    "If you would like a particular operation to run on a device of your choice instead of what's automatically selected for you, you can use with tf.device to create a device context, and all the operations within that context will run on the same designated device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op _MklMatMul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "[2022-12-29 16:46:37 UTC]   DEBUG: tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32) \n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Place tensors on the CPU\n",
    "with tf.device('/CPU:0'):\n",
    "  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "c = tf.matmul(a, b)\n",
    "debug.msg_debug(str(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting GPU memory growth\n",
    "\n",
    "By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the tf.config.experimental.set_visible_devices method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    debug.msg_debug( str( str(len(gpus)) + \" Physical GPUs,\" + str(len(logical_gpus)) + \" Logical GPU\") )\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    debug.msg_warning(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases it is desirable for the process to only allocate a subset of the available memory, or to only grow the memory usage as is needed by the process. TensorFlow provides two methods to control this.\n",
    "\n",
    "The first option is to turn on memory growth by calling tf.config.experimental.set_memory_growth, which attempts to allocate only as much GPU memory as needed for the runtime allocations: it starts out allocating very little memory, and as the program gets run and more GPU memory is needed, we extend the GPU memory region allocated to the TensorFlow process. Note we do not release memory, since it can lead to memory fragmentation. To turn on memory growth for a specific GPU, use the following code prior to allocating any tensors or executing any ops.\n",
    "\n",
    "Another way to enable this option is to set the environmental variable TF_FORCE_GPU_ALLOW_GROWTH to true. This configuration is platform specific.\n",
    "\n",
    "The second method is to configure a virtual GPU device with tf.config.experimental.set_virtual_device_configuration and set a hard limit on the total memory to allocate on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    debug.msg_debug( str( str(len(gpus)) + \" Physical GPUs,\" + str(len(logical_gpus)) + \" Logical GPU\") )\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    debug.msg_warning(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    debug.msg_debug( str( str(len(gpus)) + \" Physical GPUs,\" + str(len(logical_gpus)) + \" Logical GPU\") )\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    debug.msg_warning(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful if you want to truly bound the amount of GPU memory available to the TensorFlow process. This is common practice for local development when the GPU is shared with other applications such as a workstation GUI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a single GPU on a multi-GPU system\n",
    "\n",
    "If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default. If you would like to run on a different GPU, you will need to specify the preference explicitly.\n",
    "\n",
    "Using the API call with tf.device does not appear to work.  You have to isolate the GPU via the OS environment variables and call GPU:0 from the code itself as follows:\n",
    "\n",
    "`export CUDA_VISIBLE_DEVICES=1`\n",
    "\n",
    "`export TF_FORCE_GPU_ALLOW_GROWTH=True`\n",
    "\n",
    "Note that if you have 4 GPU's and you want to assign an individual job to each GPU per program you would change the value in CUDA_VISIBLE_DEVICES directly and then call tf.device() in the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "try:\n",
    "  # Specify an invalid GPU device\n",
    "  #with tf.device('/device:GPU:2'):\n",
    "  #make sure you set the OS env var first before invocation!!!\n",
    "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "  if gpus:\n",
    "    with tf.device('/device:GPU:'):\n",
    "      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "      c = tf.matmul(a, b)\n",
    "except RuntimeError as e:\n",
    "   debug.msg_warning(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like TensorFlow to automatically choose an existing and supported device to run the operations in case the specified one doesn't exist, you can call tf.config.set_soft_device_placement(True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op _MklMatMul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "[2022-12-29 16:47:19 UTC]   DEBUG: tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32) \n"
     ]
    }
   ],
   "source": [
    "tf.config.set_soft_device_placement(True)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Creates some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "debug.msg_debug(str(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using multiple GPUs\n",
    "\n",
    "Developing for multiple GPUs will allow a model to scale with the additional resources. If developing on a system with a single GPU, we can simulate multiple GPUs with virtual devices. This enables easy testing of multi-GPU setups without requiring additional resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Create 2 virtual GPUs with 1GB memory each\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\n",
    "         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    debug.msg_debug( str( len(gpus) + \" Physical GPUs,\" + len(logical_gpus) + \" Logical GPU\") )\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    debug.msg_warning(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have multiple logical GPUs available to the runtime, we can utilize the multiple GPUs with tf.distribute.Strategy or with manual placement.\n",
    "With tf.distribute.Strategy\n",
    "\n",
    "The best practice for using multiple GPUs is to use tf.distribute.Strategy. Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "  inputs = tf.keras.layers.Input(shape=(1,))\n",
    "  predictions = tf.keras.layers.Dense(1)(inputs)\n",
    "  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program will run a copy of your model on each GPU, splitting the input data between them, also known as \"data parallelism\".\n",
    "\n",
    "### Manual placement\n",
    "\n",
    "tf.distribute.Strategy works under the hood by replicating computation across devices. You can manually implement replication by constructing your model on each GPU. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "if gpus:\n",
    "  # Replicate your computation on multiple GPUs\n",
    "  c = []\n",
    "  for gpu in gpus:\n",
    "    with tf.device(gpu.name):\n",
    "      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "      c.append(tf.matmul(a, b))\n",
    "\n",
    "  with tf.device('/CPU:0'):\n",
    "    matmul_sum = tf.add_n(c)\n",
    "\n",
    "  debug.msg_debug(str(matmul_sum))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
