{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f30ae4-30b3-4f82-9112-c940fab55746",
   "metadata": {},
   "source": [
    "# Transformer/Diffusion Generative AI Model\n",
    "\n",
    "## Flux\n",
    "\n",
    "**What is Flux?**\n",
    "\n",
    "Flux is a new AI image generation model developed by Black Forest Labs. It represents a significant advancement in AI-generated art, utilizing a “hybrid architecture” that combines transformer and diffusion techniques, scaled up to 12 billion parameters.\n",
    "\n",
    "The model offers state-of-the-art performance image generation with top of the line prompt following, visual quality, image detail and output diversity.\n",
    "\n",
    "***Note: If you don't have a GPU with more than 24 GB of memory you will likley become very frustrated and there is NO guarantee this code will work.***\n",
    "\n",
    "**References:**\n",
    "\n",
    "+ https://medium.com/@drmarcosv/how-does-flux-work-the-new-image-generation-ai-that-rivals-midjourney-7f81f6f354da#:~:text=Flux%20is%20a%20new%20AI,up%20to%2012%20billion%20parameters.\n",
    "+ https://huggingface.co/black-forest-labs/FLUX.1-dev\n",
    "+ https://github.com/black-forest-labs/flux/issues/39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08900041-ad12-4ec4-addd-df2245f98c16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating environment for the following pip packages: ['pillow', 'torch', 'diffusers', 'transformers', 'accelerate', 'sentencepiece', 'bitsandbytes']\n",
      "...installing library pillow\n",
      "...library torch already installed.\n",
      "...library diffusers already installed.\n",
      "...library transformers already installed.\n",
      "...library accelerate already installed.\n",
      "...library sentencepiece already installed.\n",
      "...library bitsandbytes already installed.\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "#- Minimal imports to start\n",
    "###########################################\n",
    "try:\n",
    "    import sys\n",
    "    import subprocess\n",
    "    import importlib.util\n",
    "    import atexit\n",
    "except ImportError as e:\n",
    "    print(\"There was a problem importing the most basic libraries necessary for this code.\")\n",
    "    print(repr(e))\n",
    "    raise SystemExit(\"Stop right there!\")\n",
    "\n",
    "###########################################\n",
    "#- Final Exit Routine\n",
    "###########################################\n",
    "@atexit.register\n",
    "def goodbye():\n",
    "    print(\"GOODBYE\")\n",
    "\n",
    "\n",
    "        \n",
    "libraries=[\"pillow\", \"torch\", \"diffusers\", \"transformers\", \"accelerate\", \"sentencepiece\", \"bitsandbytes\"]    \n",
    "print(f\"Validating environment for the following pip packages: {libraries}\")\n",
    "\n",
    "#load environment for non-generative libraries\n",
    "try:\n",
    "    for library in libraries:\n",
    "      if library == \"Pillow\":\n",
    "        spec = importlib.util.find_spec(\"PIL\")\n",
    "      else:\n",
    "        spec = importlib.util.find_spec(library)\n",
    "      if spec is None:\n",
    "        print(\"...installing library \" + library)\n",
    "        subprocess.run([\"pip\", \"install\" , library, \"--quiet\"])\n",
    "      else:\n",
    "        print(\"...library \" + library + \" already installed.\")\n",
    "except (subprocess.CalledProcessError, Exception) as e:\n",
    "    print(\"Error: Failed to install required packages, your code might not run properly.\")\n",
    "    print(repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd36cb-350f-42e0-8253-bae2218597b3",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7b88dd-2c8e-4782-988f-d044b0083275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENCODING  =\"utf-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc23aad-d1d5-4bb6-9811-40ee97c53a93",
   "metadata": {},
   "source": [
    "# Imports and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca8abb5-6710-4f9e-9b2e-8b642e6507e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version                           #: 2.4.0+cu121         \n",
      "     GPUs available?                    #: True\n",
      "     count                              #: 1\n",
      "     current                            #: 0\n",
      "      device                            #: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:    \n",
    "    import torch\n",
    "except ImportError as ie:\n",
    "    debug.msg_warning(\"Failed to Pytorch, this could seriously impact functionality, be forewarned.\")\n",
    "    debug.msg_warning(f\"...{repr(ie)}\")\n",
    "    pass\n",
    "except Exception as e:\n",
    "    debug.msg_warning(\"Failed to Pytorch, this could seriously impact functionality, be forewarned.\")\n",
    "    debug.msg_warning(f\"...{repr(e)}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ['PYTHONIOENCODING']=ENCODING\n",
    "\n",
    "#Set GPU access\n",
    "THE_DEVICE=0\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=f\"{THE_DEVICE}\"\n",
    "device = torch.device(f\"cuda:{THE_DEVICE}\") \n",
    "\n",
    "\n",
    "try:\n",
    "    print(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
    "    print(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
    "    print(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
    "    print(f\"{'     current':<40}#: {torch.cuda.current_device()}\")\n",
    "    print(f\"{'      device':<40}#: {torch.cuda.get_device_name(0)}\")\n",
    "    #device = torch.device(\"cuda:1\")  # Use the second GPU (index starts from 0)\n",
    "except Exception as e:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51afc57-6b24-4c73-b93a-d026cf294fa1",
   "metadata": {},
   "source": [
    "# Authenticate with Hugging FAce\n",
    "\n",
    "Required as part of the agreement.  Follow the directions of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e25a9ac-0687-4d0b-bc0a-dfcf290a9d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e513f3a28c640d488c66b7b8e6d9fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/jupyter/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import accelerate\n",
    "\n",
    "# Method 1: Using the login() function\n",
    "login() # This will prompt you for your Hugging Face token\n",
    "\n",
    "# Method 2: Passing the token directly\n",
    "token = os.environ['HUGGINGFACE_API_KEY_FOR_FLUX']\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb089f16-7a48-4caa-94f4-188ed84bab8b",
   "metadata": {},
   "source": [
    "# Load Libraries in direct support of Flux and establish a \"pipe\"\n",
    "\n",
    "A pipeline in Hugging Face is a function that takes raw text as input and processes it in a sequence of computing elements. It's a way to use models for inference, and it's designed to be easy to use even if you don't have experience with the underlying code. \n",
    "\n",
    "Here are some things a pipeline can do: \n",
    "\n",
    "+ Tokenize: Break the text into words or sub-words \n",
    "+ Feed to a model: Pass the tokenized text to a model that performs a specific task, such as entity recognition or text summarization \n",
    "+ Abstract complex code: Hide most of the complex code from the library with a simple API \n",
    "+ Work with multiple tasks: Perform tasks such as named entity recognition, masked language modeling, sentiment analysis, feature extraction, and question answering \n",
    "+ Work with multiple modalities: Use a pipeline for audio, vision, and multimodal tasks \n",
    "\n",
    "**Reference:**\n",
    "\n",
    "+ https://huggingface.co/docs/transformers/main_classes/pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d36ce53-8abb-49fc-bdec-e7d533a68c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#import libraries and local model into your system using Hugging Face\\'s transformers.\\nfrom diffusers import FluxPipeline\\nfrom diffusers import DiffusionPipeline\\nimport accelerate\\n \\n#pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\")\\n\\n\\n#Reference: https://github.com/black-forest-labs/flux/issues/39\\n#pipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\\npipe.enable_sequential_cpu_offload()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries and local model into your system using Hugging Face's transformers.\n",
    "from diffusers import FluxPipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "import accelerate\n",
    " \n",
    "\n",
    "#DEV version is more advanced but takes more resources.\n",
    "#pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "#Schnell is more streamlined\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\")\n",
    "\n",
    "\n",
    "#pipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\n",
    "pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03330d2b-8fbe-4bf3-80fb-75364d02fbaf",
   "metadata": {},
   "source": [
    "# Invoke the model through the pipeline\n",
    "\n",
    "Expect 2-5 minutes for output generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3865061-2656-4fde-8d8e-6647a42fddde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#invoke the model and generate the output saving the file and showing the image\\nimage = pipe(\\n    prompt,\\n    height=1024,\\n    width=1024,\\n    guidance_scale=3.5,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    #generator=torch.Generator(\"cpu\").manual_seed(0)\\n).images[0]\\nimage.save(\"flux-dev.png\")\\nimage.show()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#invoke the model and generate the output saving the file and showing the image\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=50,\n",
    "    max_sequence_length=512,\n",
    "    #generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    ").images[0]\n",
    "image.save(\"flux-dev.png\")\n",
    "image.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
