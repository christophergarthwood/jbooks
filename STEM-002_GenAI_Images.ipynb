{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jg3iJooMQjWA"
   },
   "source": [
    "# Artificial Intelligence Image Generator\n",
    "## Generative AI (GenAI) - 002\n",
    "\n",
    "\n",
    "<center>\n",
    "<table align=\"center\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/christophergarthwood/jbooks/blob/main/STEM-002_GenAI_Images.ipynb\">\n",
    "      <img src=\"./img/GoogleColab-logo.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/notebooks?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Link to Colab Enterprise\n",
    "    </a>\n",
    "  </td>   \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/christophergarthwood/jbooks/blob/main/STEM-002-GenAI_Images.ipynb\">\n",
    "      <img src=\"./img/GitHub-logo.jpg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Link to Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "</center>\n",
    "</br></br></br>\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Christopher G Wood](https://github.com/christophergarthwood)  |\n",
    "\n",
    "# Overview\n",
    "What is AI image generation? AI image generators utilize trained artificial neural networks to create images from scratch. These generators have the capacity to create original, realistic visuals based on textual input provided in natural language.  What makes them particularly remarkable is their ability to fuse styles, concepts, and attributes to fabricate artistic and contextually relevant imagery. This is made possible through Generative AI, a subset of artificial intelligence focused on content creation.\n",
    "\n",
    "OpenAI Dall-E, Midjourney, Stable Diffusion, Gemini, Canva, Firefly, Flux (and others) are text-to-image models developed by various vendors using deep learning methodologies to generate digital images from natural language descriptions, called \"prompts\".\n",
    "\n",
    "### How AI image generators work\n",
    "\n",
    "AI image generators understand text prompts using a process that translates textual data into a machine-friendly language — numerical representations or embeddings. This conversion is initiated by a Natural Language Processing (NLP) model, such as the Contrastive Language-Image Pre-training (CLIP) model used in diffusion models like DALL-E.\n",
    "\n",
    "This mechanism transforms the input text into high-dimensional vectors that capture the semantic meaning and context of the text. Each coordinate on the vectors represents a distinct attribute of the input text.\n",
    "\n",
    "Consider an example where a user inputs the text prompt \"a red apple on a tree\" to an image generator. The NLP model encodes this text into a numerical format that captures the various elements — \"red,\" \"apple,\" and \"tree\" — and the relationship between them. This numerical representation acts as a navigational map for the AI image generator.\n",
    "\n",
    "During the image creation process, this map is exploited to explore the extensive potentialities of the final image. It serves as a rulebook that guides the AI on the components to incorporate into the image and how they should interact. In the given scenario, the generator would create an image with a red apple and a tree, positioning the apple on the tree, not next to it or beneath it.\n",
    "\n",
    "This smart transformation from text to numerical representation, and eventually to images, enables AI image generators to interpret and visually represent text prompts.\n",
    "\n",
    "### Generative Adversarial Networks (GANs)\n",
    "\n",
    "Generative Adversarial Networks, commonly called GANs, are a class of machine learning algorithms that harness the power of two competing neural networks – the generator and the discriminator. The term “adversarial” arises from the concept that these networks are pitted against each other in a contest that resembles a zero-sum game.\n",
    "\n",
    "### Diffusion Models\n",
    "\n",
    "Diffusion models are a type of generative model in machine learning that create new data, such as images or sounds, by imitating the data they have been trained on. They accomplish this by applying a process similar to diffusion, hence the name. They progressively add noise to the data and then learn how to reverse it to create new, similar data.\n",
    "\n",
    "Think of diffusion models as master chefs who learn to make dishes that taste just like the ones they've tried before. The chef tastes a dish, understands the ingredients, and then makes a new dish that tastes very similar. Similarly, diffusion models can generate data (like images) that are very much like the ones they’ve been trained on.\n",
    "\n",
    "### Neural Style Transfer (NST)\n",
    "\n",
    "Neural Style Transfer (NST) is a deep learning application that fuses the content of one image with the style of another image to create a brand-new piece of art.\n",
    "\n",
    "\n",
    "For more information, see the references below.\n",
    "\n",
    "### Reference for Image Generators:\n",
    "\n",
    "+ https://gemini.google.com/app\n",
    "  + https://cloud.google.com/vertex-ai/generative-ai/docs/image/img-gen-prompt-guide\n",
    "  + https://tech.co/news/use-google-bard-ai-image-generator\n",
    "+ https://www.midjourney.com/explore?tab=top\n",
    "+ https://openai.com/index/dall-e-2/\n",
    "+ https://builtin.com/artificial-intelligence/prompt-engineering\n",
    "+ https://www.altexsoft.com/blog/ai-image-generation/\n",
    "+ https://flux-ai.io/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1754337914341,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "jljx58WFBpXo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's define some variables (information holders) for our project overall\n",
    "\n",
    "global PROJECT_ID, BUCKET_NAME, LOCATION\n",
    "BUCKET_NAME =\"jbooks_ai_ml_public\"\n",
    "PROJECT_ID  =\"testproject-366516\"\n",
    "LOCATION    = \"us-central1\"\n",
    "\n",
    "BOLD_START=\"\\033[1m\"\n",
    "BOLD_END=\"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508,
     "referenced_widgets": [
      "d8f14abcdc9146af9b4b2f03ae1258b3",
      "93b5d775ee464fa899033d81988594dd",
      "b84fbf1d54f647a18dd6c06eb912beac",
      "3d358d10669747f1b54f9354626c6314",
      "4a3a4880686e485ca70559f06fe5b6a3",
      "b6790af6b13f48bea82c2fecff4b4f52",
      "97c197e56e0a4286ad284bb37fafa9de",
      "da4e81ac3d5644d09c9f0427ee13462d",
      "2bf22026190c451db6a6c6165fbb381c",
      "da964b5cdbd64cd5be0ecd833265f9ee",
      "4ded5d23ab0941aca7d9a205d38cc8df",
      "7984b906d9bc4b7e85f6a834169062bc",
      "010755ba3c0848538745068e8b3875c9",
      "36c9fbe76ab540dfa076872e7c7452bb",
      "a7bd471d2e544e6aaae0094cd036a09d",
      "8766d20c06f245fc8b3550cf5eb05aeb"
     ]
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1754337914668,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "mYVyn28htHDT",
    "outputId": "29861020-4efb-4e21-8658-0b868efa40a1"
   },
   "outputs": [],
   "source": [
    "# Now create a means of enforcing project id selection\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def wait_for_button_press():\n",
    "\n",
    "    button_pressed = False\n",
    "\n",
    "    # Create widgets\n",
    "    html_widget = widgets.HTML(\n",
    "\n",
    "    value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "\n",
    "        <table><tr><td>\n",
    "            <span style=\"font-family: Tahoma;font-size: 18\">\n",
    "              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n",
    "              Please verify that you are in the appropriate project and that the:</br>\n",
    "              <center><code><b>PROJECT_ID</b></code> </br></center>\n",
    "              aligns with the Project Id in the upper left corner of this browser and that the location:\n",
    "              <center><code><b>LOCATION</b></code> </br></center>\n",
    "              aligns with the instructions provided.\n",
    "            </span>\n",
    "          </td></tr></table></br></br>\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    project_list=[\"ai-bootcamp\", \"ai-advanced-training\", \"I will setup my own\"]\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=project_list,\n",
    "        value=project_list[0],\n",
    "        description='Set Your Project:',\n",
    "    )\n",
    "\n",
    "    html_widget2 = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "          \"\"\")\n",
    "\n",
    "    button = widgets.Button(description=\"Accept\")\n",
    "\n",
    "    # Function to handle the selection change\n",
    "    def on_change(change):\n",
    "        global PROJECT_ID\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            #print(\"Selected option:\", change['new'])\n",
    "            PROJECT_ID=change['new']\n",
    "\n",
    "    # Observe the dropdown for changes\n",
    "    dropdown.observe(on_change)\n",
    "\n",
    "    def on_button_click(b):\n",
    "        nonlocal button_pressed\n",
    "        global PROJECT_ID\n",
    "        button_pressed = True\n",
    "        #button.disabled = True\n",
    "        button.close()  # Remove the button from display\n",
    "        with output:\n",
    "          #print(f\"Button pressed...continuing\")\n",
    "          #print(f\"Selected option: {dropdown.value}\")\n",
    "          PROJECT_ID=dropdown.value\n",
    "\n",
    "    button.on_click(on_button_click)\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Create centered layout\n",
    "    centered_layout = widgets.VBox([\n",
    "                                    html_widget,\n",
    "                                    widgets.HBox([dropdown, button]),\n",
    "                                    html_widget2,\n",
    "    ], layout=widgets.Layout(\n",
    "                              display='flex',\n",
    "                              flex_flow='column',\n",
    "                              align_items='center',\n",
    "                              width='100%'\n",
    "    ))\n",
    "    # Display the layout\n",
    "    display(centered_layout)\n",
    "\n",
    "\n",
    "wait_for_button_press()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zramkw-P93C-"
   },
   "source": [
    "## Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1754337914669,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "shY7a4DVQjWB",
    "outputId": "a0aec64e-3f95-413b-dc5e-968239ef8b2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#- Google Colab Check\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "import datetime\n",
    "\n",
    "RunningInCOLAB = False\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "current_time   = datetime.datetime.now()\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    print(f\"You are running this notebook in Google Colab at {current_time} in the {PROJECT_ID} lab.\")\n",
    "else:\n",
    "    print(f\"You are likely running this notebook with Jupyter iPython runtime at {current_time} in the {PROJECT_ID} lab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icCD7NYJtUpu"
   },
   "source": [
    "### Library Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1754337914669,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "1uAYolHTtVWS"
   },
   "outputs": [],
   "source": [
    "# Import key libraries necessary to support dynamic installation of additional libraries\n",
    "import sys\n",
    "# Use subprocess to support running operating system commands from the program, using the \"bang\" (!)\n",
    "# symbology is supported, however that does not translate to an actual python script, this is a more\n",
    "# agnostic approach.\n",
    "import subprocess\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTy5Mpzztf3h"
   },
   "source": [
    "#### Install Libraries for Runtime\n",
    "\n",
    "Creates an install process that installs missing libraries and/or verifies their existence without re-install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8476,
     "status": "ok",
     "timestamp": 1754337923138,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "XSF9N6jRtgxW",
    "outputId": "96c2e234-e8e7-4381-906d-b8f3ff8e51e9"
   },
   "outputs": [],
   "source": [
    "# Identify the libraries you'd like to add to this Runtime environment.\n",
    "libraries=[\"backoff\", \"nltk\", \"bs4\", \"wordcloud\", \"pathlib\", \"numpy\", \"Pillow\", \"pandas\",\n",
    "           \"python-dotenv\", \"rich\", \"rich[jupyter]\", \"piexif\", \"PyMuPDF\",\"unidecode\",\n",
    "           \"watermark\", \"watermark[GPU]\",]\n",
    "\n",
    "# Loop through each library and test for existence, if not present install quietly\n",
    "for library in libraries:\n",
    "    if library == \"Pillow\":\n",
    "      spec = importlib.util.find_spec(\"PIL\")\n",
    "    else:\n",
    "      spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "      print(\"Installing library \" + library)\n",
    "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"], check=True)\n",
    "    else:\n",
    "      print(\"Library \" + library + \" already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5kZDFBsXcoq"
   },
   "source": [
    "# GCP Gemini Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11681,
     "status": "ok",
     "timestamp": 1754337934814,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "jsDkV2olXfGJ",
    "outputId": "d5fa9312-f53f-4d73-caa7-1ea1eb4b72cc"
   },
   "outputs": [],
   "source": [
    "#Download Google Vextex/AI Libraries\n",
    "subprocess.run([\"pip\", \"install\" , \"--upgrade\", \"google-cloud-aiplatform\", \"--quiet\"], check=True)\n",
    "\n",
    "\n",
    "libraries=[\"google-generativeai\", \"google-cloud-secret-manager\", \"openai\", \"google-genai\"]\n",
    "\n",
    "for library in libraries:\n",
    "    spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "      print(\"Installing library \" + library)\n",
    "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"], check=True)\n",
    "    else:\n",
    "      print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import vertexai.preview\n",
    "from google.cloud import secretmanager\n",
    "import vertexai\n",
    "import openai\n",
    "from google.auth import default, transport\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO_Hq5eq9joH"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1754337935652,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "PJuXEPlkSo9p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#- Import additional libraries that add value to the project related to NLP\n",
    "\n",
    "# Beautiful Soup (BS4) is used to parse HTML documents.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Word cloud building library\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#- Set of libraries that perhaps should always be in Python source\n",
    "import backoff\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "import gc\n",
    "import getopt\n",
    "import glob\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "import socket\n",
    "import sys\n",
    "import textwrap\n",
    "import tqdm\n",
    "import traceback\n",
    "import warnings\n",
    "import time\n",
    "from time import perf_counter\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.traceback import install\n",
    "import locale\n",
    "\n",
    "#- Displays system info\n",
    "from watermark import watermark as the_watermark\n",
    "from py3nvml import py3nvml\n",
    "\n",
    "#- Additional libraries for this work\n",
    "import math\n",
    "from base64 import b64decode\n",
    "from IPython.display import Image, Markdown\n",
    "import IPython.display as display, io, jinja2, base64\n",
    "import pandas\n",
    "import requests\n",
    "import unidecode\n",
    "\n",
    "#- Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#- Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.cbook import get_sample_data\n",
    "from matplotlib.offsetbox import (AnnotationBbox, DrawingArea, OffsetImage,\n",
    "                                  TextArea)\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.patches import Circle\n",
    "from PIL import Image as PIL_Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "#- Image meta-data for Section 508 compliance\n",
    "import piexif\n",
    "from piexif.helper import UserComment\n",
    "\n",
    "\n",
    "#- Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9gpU3zJ9l9H"
   },
   "source": [
    "## Application Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1754337935653,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "DoQDWB9s9n7H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API Parameters for things like WordCloud, variables help hold information for later use\n",
    "# The \"constants\" represent variables that we don't anticipate changing over the course of the program.\n",
    "IMG_BACKGROUND=\"black\"     #options are black, white, another color or None\n",
    "IMG_FONT_SIZE_MIN=10\n",
    "IMG_WIDTH=1024\n",
    "IMG_HEIGHT=768\n",
    "IMG_INTERP=\"bilinear\"\n",
    "IMG_ALPHA=0.8\n",
    "IMG_ASPECT=\"equal\"\n",
    "FIGURE_WIDTH=11\n",
    "FIGURE_HEIGHT=8.5\n",
    "WORD_FREQ=10\n",
    "\n",
    "# specify how image formats will be saved\n",
    "IMG_EXT=\".jpg\"\n",
    "\n",
    "# used to fully display the error stack, set to 1 if you want to see a ridiculous amount of debugging information\n",
    "DEBUG_STACKTRACE=0\n",
    "\n",
    "# location of our working files\n",
    "WORKING_FOLDER=\"./content/folderOnColab\"\n",
    "\n",
    "# Notebook Author details\n",
    "AUTHOR_NAME=\"Christopher G Wood\"\n",
    "GITHUB_USERNAME=\"christophergarthwood\"\n",
    "AUTHOR_EMAIL=\"christopher.g.wood@gmail.com\"\n",
    "\n",
    "# GenAI\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "TEXT_WIDTH=77\n",
    "IMG_SCALE=0.75\n",
    "\n",
    "# Encoding\n",
    "ENCODING  =\"utf-8\"\n",
    "os.environ['PYTHONIOENCODING']=ENCODING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FUa8QJT9tw_"
   },
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1754337935653,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "v_CqUVLZ98Mz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions are like legos that do one thing, this function outputs library version history of effort.\n",
    "def lib_diagnostics() -> None:\n",
    "\n",
    "    import pkg_resources\n",
    "\n",
    "    package_name_length=20\n",
    "    package_version_length=10\n",
    "\n",
    "    # Show notebook details\n",
    "    #%watermark?\n",
    "    #%watermark --github_username christophergwood --email christopher.g.wood@gmail.com --date --time --iso8601 --updated --python --conda --hostname --machine --githash --gitrepo --gitbranch --iversions --gpu\n",
    "    # Watermark\n",
    "    rprint(the_watermark(author=f\"{AUTHOR_NAME}\", github_username=f\"GITHUB_USERNAME\", email=f\"{AUTHOR_EMAIL}\",iso8601=True, datename=True, current_time=True, python=True, updated=True, hostname=True, machine=True, gitrepo=True, gitbranch=True, githash=True))\n",
    "\n",
    "\n",
    "    print(f\"{BOLD_START}Packages:{BOLD_END}\")\n",
    "    print(\"\")\n",
    "    # Get installed packages\n",
    "    the_packages=[\"nltk\", \"numpy\", \"os\", \"pandas\", \"seaborn\"]\n",
    "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "    for package_idx, package_name in enumerate(installed):\n",
    "         if package_name in the_packages:\n",
    "             installed_version = installed[package_name]\n",
    "             rprint(f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\")\n",
    "\n",
    "    try:\n",
    "        rprint(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
    "        rprint(f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\")\n",
    "        rprint(f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        rprint(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
    "        rprint(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
    "        rprint(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
    "        rprint(f\"{'     current':<40}#: {torch.cuda.current_device()}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "      print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
    "    except Exception as e:\n",
    "      pass\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1754337935654,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "05Szk-PyuEPn"
   },
   "outputs": [],
   "source": [
    "# Routines designed to support adding ALT text to an image generated through Matplotlib.\n",
    "\n",
    "def capture(figure):\n",
    "   buffer = io.BytesIO()\n",
    "   figure.savefig(buffer)\n",
    "   #return F\"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "   return F\"data:image/jpg;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "\n",
    "def make_accessible(figure, template, **kwargs):\n",
    "   return display.Markdown(F\"\"\"![]({capture(figure)} \"{template.render(**globals(), **kwargs)}\")\"\"\")\n",
    "\n",
    "\n",
    "# requires JPG's or TIFFs\n",
    "def add_alt_text(image_path, alt_text):\n",
    "    try:\n",
    "        if os.path.isfile(image_path):\n",
    "          img = PIL_Image.open(image_path)\n",
    "          if \"exif\" in img.info:\n",
    "              exif_dict = piexif.load(img.info[\"exif\"])\n",
    "          else:\n",
    "              exif_dict={}\n",
    "\n",
    "          w, h = img.size\n",
    "          if \"0th\" not in exif_dict:\n",
    "            exif_dict[\"0th\"]={}\n",
    "          exif_dict[\"0th\"][piexif.ImageIFD.XResolution] = (w, 1)\n",
    "          exif_dict[\"0th\"][piexif.ImageIFD.YResolution] = (h, 1)\n",
    "\n",
    "          software_version=\" \".join([\"STEM-001 with Python v\", str(sys.version).split(\" \")[0]])\n",
    "          exif_dict[\"0th\"][piexif.ImageIFD.Software]=software_version.encode(\"utf-8\")\n",
    "\n",
    "          if \"Exif\" not in exif_dict:\n",
    "            exif_dict[\"Exif\"]={}\n",
    "          exif_dict[\"Exif\"][piexif.ExifIFD.UserComment] = UserComment.dump(alt_text, encoding=\"unicode\")\n",
    "\n",
    "          exif_bytes = piexif.dump(exif_dict)\n",
    "          img.save(image_path, \"jpeg\", exif=exif_bytes)\n",
    "        else:\n",
    "          rprint(f\"Cound not fine {image_path} for ALT text modification, please check your paths.\")\n",
    "\n",
    "    except (FileExistsError, FileNotFoundError, Exception) as e:\n",
    "        process_exception(e)\n",
    "\n",
    "# Appears to solve a problem associated with GPU use on Colab, see: https://github.com/explosion/spaCy/issues/11909\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1754337935654,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "BO1mZX1PuHlI"
   },
   "outputs": [],
   "source": [
    "# this function displays the stack trace on errors from a central location making adjustments to the display on an error easier to manage\n",
    "# functions perform useful solutions for highly repetitive code\n",
    "def process_exception(inc_exception: Exception) -> None:\n",
    "  if DEBUG_STACKTRACE==1:\n",
    "    traceback.print_exc()\n",
    "    console.print_exception(show_locals=True)\n",
    "  else:\n",
    "    rprint(repr(inc_exception))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trrDJmb3uLlV"
   },
   "source": [
    "### Setup Instances of Variables from Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1754337935654,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "VOSlJpVbuNjw"
   },
   "outputs": [],
   "source": [
    "# Setup the rich print console for future use\n",
    "if DEBUG_STACKTRACE==1:\n",
    "  console = Console()\n",
    "\n",
    "# Use the 'Agg' backend for non-interactive environments\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Ensure UTF-8 Encoding is set\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvXPiTuswHwg"
   },
   "source": [
    "### Make sure you have a place to keep results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1754337935654,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "-9I2Xd1vwJ43",
    "outputId": "14ff73a1-3bc4-4602-e983-2c411d49821b"
   },
   "outputs": [],
   "source": [
    "# Create the folder that will hold our content.\n",
    "target_folder=WORKING_FOLDER\n",
    "rprint(f\"Creating a folder ({target_folder}) to store project data.\")\n",
    "\n",
    "try:\n",
    "  if os.path.isfile(target_folder):\n",
    "    raise OSError(\"Cannot create your folder a file of the same name already exists there, work with your instructor or remove it yourself.\")\n",
    "  elif os.path.isdir(target_folder):\n",
    "    print(f\"The folder named ({target_folder}) {BOLD_START}already exists{BOLD_END}, we won't try to create a new folder.\")\n",
    "  else:\n",
    "    subprocess.run([\"mkdir\", \"-p\" , target_folder], check=True)\n",
    "except (subprocess.CalledProcessError, Exception) as e:\n",
    "  process_exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG0mFzUX-DV1"
   },
   "source": [
    "## Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1754337935897,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "SSOOEwn8-FKg",
    "outputId": "3c3ad283-0128-454d-f789-6d924c85760b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib_diagnostics()\n",
    "wrapper = textwrap.TextWrapper(width=TEXT_WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46JMSTY2QjWD"
   },
   "source": [
    "## Model Parameters and Model Selection\n",
    "\n",
    "Imagen 3 sets a new standard in quality and control over your generated images. This text-to-image model produces photorealistic visuals with exceptional composition, sharpness, color accuracy, and resolution. With Imagen 3, you can explore a wider spectrum of artistic styles and formats. From photorealistic masterpieces to whimsical claymation scenes, the model's expanded range of styles and formats provides the tools to express your unique artistic vision.\n",
    "\n",
    "**Text rendering**\n",
    "\n",
    "Imagen 3 also brings new possibilities when it comes to rendering text within images. A fun way to play around with this feature is to generate images of greeting cards, posters, and social media posts with captions in various fonts and colors. This feature is as easy as adding a short text description you would like to see to the prompt. Let’s say you would like to add a title and regenerate a cookbook cover.\n",
    "\n",
    "**Closer to your intent**\n",
    "\n",
    "Imagen 3's prompt comprehension translates your natural language descriptions, no matter how nuanced, into closely matched visuals. You can specify everything from specific camera angles to types of lenses to image compositions in your description. Imagen 3 adheres closely to the prompt, which helps close the gap between your mental picture and the final image. You can provide the model with simple subject-action-setting prompts or intricate, multi-layered descriptions, and the model adapts to your creative process to enable a broad range of styles.\n",
    "\n",
    "Since Imagen 3 does well with elaborate prompts, providing robust details usually yields higher quality and more precise results. Below are a few options to consider when crafting your prompts:\n",
    "\n",
    "Arrangement: Direct the scene by specifying where you want subjects positioned.\n",
    "\n",
    "Lighting: Create atmosphere with soft or harsh lighting, and control its direction and focus.\n",
    "\n",
    "Angles & lenses: Add depth and perspective with camera angles and lens choices.\n",
    "\n",
    "Styles: Go beyond photorealism and generate digital art, cinematic, vintage, minimalist images, and more.\n",
    "\n",
    "***Reduced latency***\n",
    "\n",
    "While Imagen 3 is our highest quality model to date, we are also offering Imagen 3 Fast, which is optimized for generation speed. Imagen 3 Fast is suitable for creating brighter, higher contrast images. Compared to Imagen 2, you can see a 40% decrease in latency. To demonstrate these two models, you can generate two images with the same prompt. Let’s generate two options for a photo of a salad to add to the same cookbook from earlier.\n",
    "\n",
    "**Protect your work and create responsibly**\n",
    "\n",
    "Imagen 3 has built in safeguards that let you focus on your artistic vision without compromising control. In partnership with Google DeepMind, Imagen 3 utilizes SynthID, a technology which embeds an invisible watermark at the pixel level. By default, a digital watermark is added to all Imagen 3 generated images, but you can explicitly enable this feature with the add_watermark parameter. You can also use the API to verify whether an image was generated using Imagen. This verifies the authenticity of your AI-generated images, providing transparency and helping to safeguard your work from misuse.\n",
    "\n",
    "With Imagen 3's advanced safety filters, you can also control the types of images generated to make sure they meet your brand values or principles. To configure safety filter thresholds for generated images, modify the safety_filter_level. The safety level can be changed to “block_most”, “block_some”, or “block_few”. To change the safety setting that controls the type of people generated, modify person_generation to “allow_all”, “allow_adult”, or “dont_allow”.\n",
    "\n",
    "### Use cases\n",
    "+ Generate images: Enter text prompts to generate a series of images. For example, \"A French cafe with the Golden Gate Bridge in the background.\"\n",
    "\n",
    "+ Edit images: Edit images with built-in segmentation features that enable you to segment the image’s background, foreground, or ~175 subject classes\n",
    "\n",
    "+ Product editing: Provide an image of a product and a text prompt describing a new scenery to easily enhance product images\n",
    "+ Inpainting (removal): Provide an image and a mask that highlights a subject that you want to remove from the image\n",
    "\n",
    "+ Inpainting (insert): Provide an image and a mask to insert a subject based on your input text prompt\n",
    "Outpainting: Provide an image and a mask to extend the image to a custom size that you desire\n",
    "\n",
    "### Temperature Settings\n",
    "\n",
    "The temperature is a numerical value (often set between 0 and 1, but sometimes higher) that adjusts how much the model takes risks or plays it safe in its choices. It modifies the probability distribution of the next word.\n",
    "\n",
    "The different LLM temperature parameters:\n",
    "\n",
    "**Low Temperature (<1.0)**: Setting the temperature to a value of less than 1 makes the model’s output more deterministic and repetitive. Lower temperatures lead to the model picking the most likely next word more often, reducing the variability of the output. This can be useful when you need more predictable, conservative responses, but it might also result in less creative or diverse text, also making the model sound more robotic.\n",
    "\n",
    "**High Temperature (>1.0)**: A temperature setting above 1 increases randomness in the generated text. The model is more likely to select less probable words as the next word in the sequence, leading to more varied and sometimes more creative outputs. However, this can also result in more errors or nonsensical responses, since the model is less constrained by the probability distribution of its training data.\n",
    "\n",
    "**Temperature of 1.0**: This is often the default setting, aiming for a balance between randomness and determinism. The model generates text that is neither too predictable nor too random, based on the probability distribution learned during its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1754337935897,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "NtRDeZQ_Ysyj"
   },
   "outputs": [],
   "source": [
    "# Variables help hold information for later use\n",
    "# Model parameters are those values you can change at runtime, meaning when using the AI solution.\n",
    "# Changing the model can influence the type of response you get at the end.\n",
    "\n",
    "\n",
    "#AI_MODEL_TYPE = \"imagegeneration@006\"\n",
    "#AI_MODEL_TYPE = \"imagen-3.0-generate-001\"\n",
    "AI_MODEL_TYPE = \"imagen-3.0-generate-002\"\n",
    "\n",
    "model_temperature=0.5                     #Model temperature is a parameter that controls the randomness and creativity of a language model's output.\n",
    "                                          #It's a key factor in the quality of the text generated by the model, and is used in many natural language processing (NLP) tasks,\n",
    "                                          #such as summarization, translation, and text generation.\n",
    "\n",
    "model_max_tokens=8000                     #Model max tokens refers to the maximum number of tokens a language model can process in a single input, including both the prompt\n",
    "                                          #provided and the generated output, essentially setting the upper limit on the length of the text the model can generate in a single\n",
    "                                          #response; exceeding this limit will result in the model truncating the output or potentially returning an error message\n",
    "\n",
    "model_max_token_response=8000             #Maximum reponse you're preparing to return with, sets limits for future calculations.\n",
    "\n",
    "model_top_p=1                             #Top P specifies the cumulative probability score threshold that the tokens must reach.\n",
    "                                          #For example, if you set Top P to 0.6, then only the first two tokens, for and to, are sampled\n",
    "                                          #because their probabilities (0.4 and 0.25) add up to 0.65.\n",
    "\n",
    "model_top_k=1                             #Top-k sampling samples tokens with the highest probabilities until the specified number of\n",
    "                                          #tokens is reached. Top-p sampling samples tokens with the highest probability scores until\n",
    "                                          #the sum of the scores reaches the specified threshold value. (Top-p sampling is also called nucleus sampling.)\n",
    "\n",
    "summary_token_max=150\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m57mUb7BulBE"
   },
   "source": [
    "### AVAILABLE MODELS\n",
    "\n",
    "+ https://firebase.google.com/docs/vertex-ai/gemini-models\n",
    "\n",
    "Select ai model type, for detailed information about each model see: https://firebase.google.com/docs/vertex-ai/gemini-models#detailed-info\n",
    "\n",
    "Model names for Gemini, see: https://firebase.google.com/docs/vertex-ai/gemini-models#available-model-names\n",
    "\n",
    "All model reference, see: https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models\n",
    "\n",
    "Image Model Developer's Guide, see: https://cloud.google.com/blog/products/ai-machine-learning/a-developers-guide-to-imagen-3-on-vertex-ai\n",
    "\n",
    "Example Notebook, see: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/imagen3_image_generation.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ADGObWZZN3B"
   },
   "source": [
    "# AI Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1754337935898,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "mYX18Vt9ZPrs"
   },
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    ")\n",
    "\n",
    "# The Gemini API provides safety settings that you can adjust based on your application's needs.\n",
    "# Reference: https://ai.google.dev/gemini-api/docs/safety-settings\n",
    "\n",
    "safety = [\n",
    "    SafetySetting(\n",
    "        category = HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category = HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category = HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category = HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvlYNerhGp7f"
   },
   "source": [
    "## Large Language Model (LLM) ~ Gemini Visual Model (Google)\n",
    "\n",
    "See https://ai.google.dev/api/python/google/generativeai/GenerativeModel for more details about utilization of this resource.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1754337935898,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "x7J5-zMedkVL",
    "outputId": "d641dba9-596c-483f-84d8-ad7fdafd5416"
   },
   "outputs": [],
   "source": [
    "# Make sure you're authenticated\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "#else:\n",
    "#    subprocess.run([\"gcloud\", \"auth\" , \"login\"], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1754337936133,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "Fz4Rr_f2jZ_z",
    "outputId": "d82f3b5d-4d5f-487b-caa7-5ea6ddfc5342"
   },
   "outputs": [],
   "source": [
    "# Import the Generative Model and then setup the model connection point to backend Google Services.\n",
    "\n",
    "from vertexai.preview.vision_models import ImageGenerationModel\n",
    "\n",
    "try:\n",
    "    model = ImageGenerationModel.from_pretrained(AI_MODEL_TYPE)\n",
    "except (ValueError, Exception) as e:\n",
    "    process_exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pI0FSnAjNU3"
   },
   "source": [
    "## Create Your Picture Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1754337936134,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "zMxl-xK1jAZn",
    "outputId": "b69a67a6-7106-41e0-9c95-676196e41afb"
   },
   "outputs": [],
   "source": [
    "# These are arrays.  Arrays hold clusters of information much like a carton holds eggs.\n",
    "# The following are ptions for creating an image and give insight into different styles you might want to leverage\n",
    "# when creating your prompt.\n",
    "\n",
    "style=[\"normal\", \"abstract\", \"surrealism\", \"cubism\", \"impressionism\", \"3d\", \"expressionist painting\", \"manga\", \"pop art\", \"pencil sketch\", \"watercolor\"]\n",
    "detail=[\"normal\", \"realistic\", \"ultrarealistic\", \"photorealistic\", \"intricate\", \"highly detailed\"]\n",
    "position=[\"normal\", \"on the wall\", \"in the background\", \"in the foreground\", \"in the middle\", \"in the left\", \"in the right\"]\n",
    "mood=[\"normal\", \"stunning\", \"elegant\", \"radiant\", \"delicate\", \"cute\", \"striking\" \"glamorous\"]\n",
    "lights=[\"normal\", \"warm lighting\", \"natural lighting\", \"cold lighting\", \"dark aesthetic\", \"red tones\", \"blue tones\"]\n",
    "viewpoint=[\"portrait\", \"landscape\", \"close-up\", \"headshot\", \"mid-range\", \"3/4 shot\", \"full body shot\", \"wide shot\", \"low angle shot\", \"high angle shot\"]\n",
    "\n",
    "subject=[\"penguin\", \"bear\", \"tree\", \"dog\", \"cat\", \"plan\", \"spotted lantern fly\", \"eastern longhorn beetle\"]\n",
    "verb=[\"running\", \"surfing\", \"on the computer\", \"sleeping\", \"eating\", \"playing\", \"dancing\", \"sitting\", \"standing\", \"walking\"]\n",
    "\n",
    "# Generate a picture from random choices above\n",
    "\n",
    "picture_prompt=f\"{style[random.randint(0, len(style)-1)]} \\\n",
    " {detail[random.randint(0, len(detail)-1)]} \\\n",
    " {position[random.randint(0, len(position)-1)]} \\\n",
    " {mood[random.randint(0, len(mood)-1)]} \\\n",
    " {lights[random.randint(0, len(lights)-1)]} \\\n",
    " {viewpoint[random.randint(0, len(viewpoint)-1)]}\\\n",
    " {subject[random.randint(0, len(subject)-1)]} \\\n",
    " {verb[random.randint(0, len(verb)-1)]}\"\n",
    "\n",
    "# Additional examples of verified prompts\n",
    "#picture_prompt=\"magazine style, 4k, photorealistic, modern red armchair, natural\"\n",
    "#picture_prompt=\"hyper-realistic, 4k, bear smoking a cigar holding a tommy gun while standing surfing on a shark\"\n",
    "\n",
    "string = wrapper.fill(text=picture_prompt)\n",
    "rprint(f\"Random text created: {picture_prompt}\")\n",
    "\n",
    "# REPLACE picture_prompt with your own input if you want the image created to change\n",
    "# picture_prompt=\"put your prompt between the quotes\".   #<- Change here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3_zbPFkjUHy"
   },
   "source": [
    "## Execute the Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 892
    },
    "executionInfo": {
     "elapsed": 11233,
     "status": "ok",
     "timestamp": 1754337947362,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "xLguZitVjWAu",
    "outputId": "a2dfacdb-a708-43e1-fb26-908877284fe7"
   },
   "outputs": [],
   "source": [
    "# Invoke the model passing your prompt and see the imae saved.\n",
    "%matplotlib inline\n",
    "\n",
    "#from PIL import Image\n",
    "#from IPython.display import display\n",
    "try:\n",
    "  images = model.generate_images(\n",
    "      prompt=picture_prompt,\n",
    "\n",
    "      # Optional parameters\n",
    "      number_of_images=1,\n",
    "      language=\"en\",\n",
    "\n",
    "      # You can't use a seed value and watermark at the same time.\n",
    "      add_watermark=False,\n",
    "      #seed=100,\n",
    "      aspect_ratio=\"1:1\",\n",
    "      safety_filter_level=\"block_some\",\n",
    "      person_generation=\"allow_all\",\n",
    "  )\n",
    "\n",
    "  # Save the image locally\n",
    "  try:\n",
    "    output_file=target_folder+os.sep+f\"stem-002-genai-images-output{IMG_EXT}\"\n",
    "    images[0].save(location=output_file, include_generation_parameters=False)\n",
    "    rprint(f\"Your save location is {output_file}.\")\n",
    "    print(f\"For prompt: {BOLD_START}{picture_prompt}{BOLD_END}\")\n",
    "    print(\"\")\n",
    "    caption_text=f\"Image created with {AI_MODEL_TYPE} and the following prompt: {picture_prompt}.\"\n",
    "  except Exception as e:\n",
    "    rprint(\"There was a problem saving your image.\")\n",
    "    rprint(f\"Your save location was {output_file}.\")\n",
    "    process_exception(e)\n",
    "\n",
    "  #add an alt tag\n",
    "  try:\n",
    "    add_alt_text(target_folder+os.sep+f\"stem-002-genai-images-output{IMG_EXT}\", caption_text);\n",
    "  except Exception as e:\n",
    "    process_exception(e)\n",
    "\n",
    "  try:\n",
    "    plt.figure(figsize=(FIGURE_HEIGHT, FIGURE_WIDTH))\n",
    "    plt.title(f\"GenAI Image from Prompt\")\n",
    "    plt.imshow(PIL_Image.open(output_file))\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    rprint(caption_text)\n",
    "    plt.close()\n",
    "\n",
    "  except Exception as e:\n",
    "    print(\"There was a problem displaying your image.  Contact your instructor for assistance.\")\n",
    "    process_exception(e)\n",
    "\n",
    "except Exception as e:\n",
    "  print(\"There was a problem creating your image.  Contact your instructor for assistance.\")\n",
    "  process_exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyh-6i-abGNB"
   },
   "source": [
    "# Do you understand the stochastic nature of AI?\n",
    "\n",
    "Definition of stochastic: randomly determined; having a random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely.\n",
    "\n",
    "***The behavior and performance of many machine learning algorithms are referred to as stochastic.***\n",
    "\n",
    "Stochastic refers to a variable process where the outcome involves some randomness and has some uncertainty. It is a mathematical term and is closely related to “randomness” and “probabilistic” and can be contrasted to the idea of “deterministic.”\n",
    "\n",
    "The stochastic nature of machine learning algorithms is an important foundational concept in machine learning and is required to be understand in order to effectively interpret the behavior of many predictive models.\n",
    "\n",
    "+ A variable or process is stochastic if there is uncertainty or randomness involved in the outcomes.\n",
    "\n",
    "+ Stochastic is a synonym for random and probabilistic, although is different from non-deterministic.\n",
    "\n",
    "+ Many machine learning algorithms are stochastic because they explicitly use randomness during optimization or learning.\n",
    "\n",
    "#### *** If this same prompt runs multiple times, do you expect to see the same picture every time....??? ***\n",
    "\n",
    "References:\n",
    "+ https://www.geeksforgeeks.org/deterministic-vs-stochastic-environment-in-ai/\n",
    "+ https://machinelearningmastery.com/stochastic-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMcSowRunjjr"
   },
   "source": [
    "# Advanced Class\n",
    "\n",
    "1. Utilize a configuration management schema such as: dotenv, config_parser, or similar API's to support loading model parameters in a configured manner.  Utilize that configuration to feed your program.\n",
    "\n",
    "2.  Utilize STEM-003 to create a prompt to support your image creation.\n",
    "\n",
    "3.  Embeed a watermark in your image.\n",
    "\n",
    "4.  Add a caption (alt text) to your image using [Gemini's capabilities](https://ai.google.dev/gemini-api/docs/vision?lang=python).\n",
    "\n",
    "5. Utilize the image manipulation techniques identified as [inpainting](https://cloud.google.com/vertex-ai/generative-ai/docs/image/edit-inpainting) to modify the image you create."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "STEM-002_GenAI_Images.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/christophergarthwood/jbooks/blob/main/STEM-001_WordClouds.ipynb",
     "timestamp": 1716214402332
    }
   ]
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "010755ba3c0848538745068e8b3875c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bf22026190c451db6a6c6165fbb381c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Accept",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_a7bd471d2e544e6aaae0094cd036a09d",
      "style": "IPY_MODEL_8766d20c06f245fc8b3550cf5eb05aeb",
      "tooltip": ""
     }
    },
    "36c9fbe76ab540dfa076872e7c7452bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d358d10669747f1b54f9354626c6314": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ded5d23ab0941aca7d9a205d38cc8df",
      "placeholder": "​",
      "style": "IPY_MODEL_7984b906d9bc4b7e85f6a834169062bc",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n          "
     }
    },
    "4a3a4880686e485ca70559f06fe5b6a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "4ded5d23ab0941aca7d9a205d38cc8df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7984b906d9bc4b7e85f6a834169062bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8766d20c06f245fc8b3550cf5eb05aeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "93b5d775ee464fa899033d81988594dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6790af6b13f48bea82c2fecff4b4f52",
      "placeholder": "​",
      "style": "IPY_MODEL_97c197e56e0a4286ad284bb37fafa9de",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n\n        <table><tr><td>\n            <span style=\"font-family: Tahoma;font-size: 18\">\n              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n              Please verify that you are in the appropriate project and that the:</br>\n              <center><code><b>PROJECT_ID</b></code> </br></center>\n              aligns with the Project Id in the upper left corner of this browser and that the location:\n              <center><code><b>LOCATION</b></code> </br></center>\n              aligns with the instructions provided.\n            </span>\n          </td></tr></table></br></br>\n\n    "
     }
    },
    "97c197e56e0a4286ad284bb37fafa9de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7bd471d2e544e6aaae0094cd036a09d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6790af6b13f48bea82c2fecff4b4f52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b84fbf1d54f647a18dd6c06eb912beac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da4e81ac3d5644d09c9f0427ee13462d",
       "IPY_MODEL_2bf22026190c451db6a6c6165fbb381c"
      ],
      "layout": "IPY_MODEL_da964b5cdbd64cd5be0ecd833265f9ee"
     }
    },
    "d8f14abcdc9146af9b4b2f03ae1258b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93b5d775ee464fa899033d81988594dd",
       "IPY_MODEL_b84fbf1d54f647a18dd6c06eb912beac",
       "IPY_MODEL_3d358d10669747f1b54f9354626c6314"
      ],
      "layout": "IPY_MODEL_4a3a4880686e485ca70559f06fe5b6a3"
     }
    },
    "da4e81ac3d5644d09c9f0427ee13462d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "ai-bootcamp",
       "ai-advanced-training",
       "I will setup my own"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Set Your Project:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_010755ba3c0848538745068e8b3875c9",
      "style": "IPY_MODEL_36c9fbe76ab540dfa076872e7c7452bb"
     }
    },
    "da964b5cdbd64cd5be0ecd833265f9ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
