{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg3iJooMQjWA"
      },
      "source": [
        "# Artificial Intelligence\n",
        "## Computer Vision / Convolutional Neural Networks (CNN)\n",
        "### Harmful Species Identification (Spotted Lantern Fly vs. Asian Longhorn Beetle)\n",
        "\n",
        "### What is a convolutional neural network (CNN)?\n",
        "\n",
        "A convolutional neural network (CNN) is a category of machine learning model, namely a type of deep learning algorithm well suited to analyzing visual data. CNNs -- sometimes referred to as convnets -- use principles from linear algebra, particularly convolution operations, to extract features and identify patterns within images. Although CNNs are predominantly used to process images, they can also be adapted to work with audio and other signal data.\n",
        "\n",
        "CNN architecture is inspired by the connectivity patterns of the human brain -- in particular, the visual cortex, which plays an essential role in perceiving and processing visual stimuli. The artificial neurons in a CNN are arranged to efficiently interpret visual information, enabling these models to process entire images. Because CNNs are so effective at identifying objects, they are frequently used for computer vision tasks such as image recognition and object detection, with common use cases including self-driving cars, facial recognition and medical image analysis.\n",
        "\n",
        "Unlike CNNs, older forms of neural networks often needed to process visual data in a piecemeal manner, using segmented or lower-resolution input images. A CNN's comprehensive approach to image recognition lets it outperform a traditional neural network on a range of image-related tasks and, to a lesser extent, speech and audio processing.\n",
        "\n",
        "### How do convolutional neural networks work?\n",
        "CNNs use a series of layers, each of which detects different features of an input image. Depending on the complexity of its intended purpose, a CNN can contain dozens, hundreds or even thousands of layers, each building on the outputs of previous layers to recognize detailed patterns.\n",
        "\n",
        "\n",
        "Reference:\n",
        "+ https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats\n",
        "+ https://pythonawesome.com/an-animated-and-smart-progress-bar-for-python/\n",
        "+ https://www.tensorflow.org/tutorials/images/cnn\n",
        "+ https://www.geeksforgeeks.org/computer-vision/\n",
        "+ https://www.machinelearningnuggets.com/cnn-tensorflow/\n",
        "+ https://www.datacamp.com/tutorial/cnn-tensorflow-python\n",
        "+ https://pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/\n",
        "+ https://www.baeldung.com/cs/ml-loss-accuracy\n",
        "+ https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew\n",
        "+https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3qlCehNBu-_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME     = \"ai-training-2024-08-09-bucket\"\n",
        "PROJECT_ID      = \"ai-training-2024-08-09\"\n",
        "LOCATION        = \"us-central1\"\n",
        "secret_name     = \"ai-training-key-secret\"\n",
        "secret_version  = \"latest\"\n",
        "project_id      = \"usfs-tf-admin\"\n",
        "resource_name   = f\"projects/{project_id}/secrets/{secret_name}/versions/{secret_version}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zramkw-P93C-"
      },
      "source": [
        "## Environment Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shY7a4DVQjWB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#- Google Colab Check\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "RunningInCOLAB = False\n",
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if RunningInCOLAB:\n",
        "    print(\"You are running this notebook in Google Colab.\")\n",
        "else:\n",
        "    print(\"You are running this notebook with Jupyter iPython runtime.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZVkISRuLURi"
      },
      "source": [
        "# Library Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "logDyNfnLURj"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib.util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldXG-5fhsV1e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "libraries=[\"keras\", \"imageio\", \"seaborn\", \"alive-progress\", \"numpy\", \"Pillow\", \"numba\", \"tensorrt\"]\n",
        "#\"tf-keras\",\n",
        "\n",
        "for library in libraries:\n",
        "    if library == \"Pillow\":\n",
        "      spec = importlib.util.find_spec(\"PIL\")\n",
        "    else:\n",
        "      spec = importlib.util.find_spec(library)\n",
        "    if spec is None:\n",
        "      print(\"Installing library \" + library)\n",
        "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"])\n",
        "    else:\n",
        "      print(\"Library \" + library + \" already installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO_Hq5eq9joH"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJuXEPlkSo9p",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# a set of libraries that perhaps should always be in Python source\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import socket\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import itertools\n",
        "import datetime\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import io\n",
        "import os\n",
        "import datetime\n",
        "import sys\n",
        "import gc\n",
        "import getopt\n",
        "import inspect\n",
        "import math\n",
        "import warnings\n",
        "import textwrap\n",
        "import random\n",
        "import glob\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Additional libraries for this work\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import math\n",
        "from base64 import b64decode\n",
        "from IPython.display import Image\n",
        "from IPython.display import clear_output\n",
        "import requests\n",
        "from random import random\n",
        "from random import randint\n",
        "from random import seed\n",
        "from collections import Counter\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Graphics\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from PIL import Image\n",
        "import PIL.ImageOps\n",
        "from imageio import imread\n",
        "import seaborn as sns\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Data Science Libraries\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Tensorflow and related AI libraries\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import tensorflow as tf\n",
        "from tensorflow import data as tf_data\n",
        "from tensorflow import keras\n",
        "import tensorrt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "#existing trained model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# progress bar\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "from alive_progress import alive_bar\n",
        "#from alive_progress.styles import showtime, Show\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "#################################\n",
        "# - Setup some basic timers material\n",
        "#################################\n",
        "from time import perf_counter\n",
        "\n",
        "warnings.filterwarnings('ignore')               # don't print out warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9gpU3zJ9l9H"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoQDWB9s9n7H",
        "tags": []
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "# GLOBAL VARIABLES\n",
        "############################################\n",
        "DEBUG = 1\n",
        "DEBUG_DATA = 0\n",
        "\n",
        "# CODE CONSTRAINTS\n",
        "VERSION_NAME    = \"MLCNN\"\n",
        "VERSION_MAJOR   = 0\n",
        "VERSION_MINOR   = 0\n",
        "VERSION_RELEASE = 1\n",
        "\n",
        "#used for values outside standard ASCII, just do it, you'll need it\n",
        "ENCODING  =\"utf-8\"\n",
        "\n",
        "############################################\n",
        "# GLOBAL CONSTANTS & APP VARIABLES\n",
        "############################################\n",
        "BATCH_SIZE = 32\n",
        "TRAIN_DATA_DIR=\"./folderOnColab/ENTOMOLOGY/train\"\n",
        "VALIDATION_DATA_DIR=\"./folderOnColab/ENTOMOLOGY/validation\"\n",
        "VALIDATION_SPLIT=0.2\n",
        "IMG_HEIGHT=224\n",
        "IMG_WIDTH=224\n",
        "EPOCHS=50\n",
        "BOLD_START = \"\\033[1m\"\n",
        "BOLD_END = \"\\033[0;0m\"\n",
        "\n",
        "METRICS = metrics=['accuracy',\n",
        "                  \ttf.keras.metrics.Precision(name='precision'),\n",
        "                  \ttf.keras.metrics.Recall(name='recall')]\n",
        "\n",
        "#You can also adjust the verbosity by changing the value of TF_CPP_MIN_LOG_LEVEL:\n",
        "#\n",
        "#0 = all messages are logged (default behavior)\n",
        "#1 = INFO messages are not printed\n",
        "#2 = INFO and WARNING messages are not printed\n",
        "#3 = INFO, WARNING, and ERROR messages are not printed\n",
        "TF_CPP_MIN_LOG_LEVEL_SETTING=0\n",
        "\n",
        "TEXT_WIDTH=77\n",
        "\n",
        "############################################\n",
        "# APPLICATION VARIABLES\n",
        "############################################\n",
        "categories=[\"ASIAN_LONGHORN_BEETLE\", \"SPOTTED_LANTERN_FLY\"]\n",
        "categories_short_name=[\"alb\", \"slf\"]\n",
        "range_max=5\n",
        "plot_max=330\n",
        "start = \"\\033[1m\"\n",
        "end = \"\\033[0;0m\"\n",
        "THE_DEVICE_NAME=\"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
        "############################################\n",
        "# GLOBAL CONFIGURATION\n",
        "############################################\n",
        "os.environ['PYTHONIOENCODING']=ENCODING\n",
        "\n",
        "############################################\n",
        "# Set the Seed for the experiment (ask me why?)\n",
        "############################################\n",
        "# seed the pseudorandom number generator\n",
        "# THIS IS ESSENTIAL FOR CONSISTENT MODEL OUTPUT, remember these are random in nature.\n",
        "SEED_INIT=7\n",
        "\n",
        "seed(SEED_INIT)\n",
        "\n",
        "tf.random.set_seed(SEED_INIT)\n",
        "\n",
        "np.random.seed(SEED_INIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FUa8QJT9tw_"
      },
      "source": [
        "## Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_CqUVLZ98Mz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Outputs library version history of effort.\n",
        "#\n",
        "def lib_diagnostics() -> None:\n",
        "\n",
        "    import pkg_resources\n",
        "\n",
        "    package_name_length=40\n",
        "    package_version_length=20\n",
        "\n",
        "    # Get installed packages\n",
        "    the_packages=[\"nltk\", \"numpy\", \"os\", \"pandas\", \"keras\", \"seaborn\"]\n",
        "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
        "    for package_idx, package_name in enumerate(installed):\n",
        "         if package_name in the_packages:\n",
        "             installed_version = installed[package_name]\n",
        "             print(f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
        "        print(f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\")\n",
        "        print(f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        print(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
        "        print(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
        "        print(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
        "        print(f\"{'     current':<40}#: {torch.cuda.current_device()}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "\n",
        "    try:\n",
        "      print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
        "    except Exception as e:\n",
        "      pass\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG0mFzUX-DV1"
      },
      "source": [
        "## Function Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSOOEwn8-FKg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "lib_diagnostics()\n",
        "wrapper = textwrap.TextWrapper(width=TEXT_WIDTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check your resources from a CPU/GPU perspective"
      ],
      "metadata": {
        "id": "vTrnAspo3CWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{BOLD_START}List Devices{BOLD_END} #########################################\")\n",
        "try:\n",
        "  from tensorflow.python.client import device_lib\n",
        "  print(device_lib.list_local_devices())\n",
        "  print(\"\")\n",
        "except RuntimeError as e:\n",
        "  # Visible devices must be set before GPUs have been initialized\n",
        "  print(str(repr(e)))\n",
        "\n",
        "print(f\"{BOLD_START}Devices Counts{BOLD_END} ########################################\")\n",
        "try:\n",
        "  print(f\"Num GPUs Available: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\" )\n",
        "  print(f\"Num CPUs Available: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\" )\n",
        "  print(\"\")\n",
        "except RuntimeError as e:\n",
        "  # Visible devices must be set before GPUs have been initialized\n",
        "  print(str(repr(e)))\n",
        "\n",
        "print(f\"{BOLD_START}Optional Enablement{BOLD_END} ####################################\")\n",
        "try:\n",
        "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "except RuntimeError as e:\n",
        "  # Visible devices must be set before GPUs have been initialized\n",
        "  print(str(repr(e)))\n",
        "\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print( str( str(len(gpus)) + \" Physical GPUs,\" + str(len(logical_gpus)) + \" Logical GPU\") )\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(str(repr(e)))\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "hmGUvC7M3B0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Release memory from current GPU's"
      ],
      "metadata": {
        "id": "yNG1Rq0bKjl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "\n",
        "try:\n",
        "    device = cuda.get_current_device()\n",
        "    device.reset()\n",
        "except Exception as e:\n",
        "    print(str(repr(e)))"
      ],
      "metadata": {
        "id": "xy3kZgyCKmbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "tpk0bQejxXW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JMSTY2QjWD"
      },
      "source": [
        "## Input Sources\n",
        "### Copy a repository of images to the local Google Colab instance\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_folder=\"./folderOnColab\""
      ],
      "metadata": {
        "id": "G7_LfwTJmEB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZcbq467sgrc"
      },
      "outputs": [],
      "source": [
        "\n",
        "#!rm -rf ./folderOnColab && echo \"Ok, removed.\" || { echo \"No folder to remove.\"; exit 1; }\n",
        "\n",
        "#BEARS\n",
        "#!mkdir -p ./folderOnColab && echo \"Folder created.\" || { echo \"Failed to create folder, perhaps it already exists.\";   }\n",
        "#!gsutil -m cp -r gs://usfs-gcp-rand-test-data-usc1/public_source/computervision/bear ./folderOnColab\n",
        "\n",
        "#ENTOMOLOGY\n",
        "#!mkdir -p ./folderOnColab && echo \"Folder created.\" || { echo \"Failed to create folder, perhaps it already exists.\";   }\n",
        "#!gsutil -m cp -r gs://usfs-gcp-rand-test-data-usc1/public_source/computervision/ENTOMOLOGY ./folderOnColab\n",
        "\n",
        "print(f\"Creating a folder ({target_folder}) to store project data.\")\n",
        "subprocess.run([\"mkdir\", \"-p\" , target_folder], check=True)\n",
        "\n",
        "print(f\"Copying file to target folder: {target_folder}\")\n",
        "subprocess.run([\"gsutil\", \"-m\" , \"cp\", \"-r\", f\"gs://{BUCKET_NAME}/public_source/computervision/ENTOMOLOGY\",  target_folder], check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDLoEWmVR9Sk"
      },
      "source": [
        "### List the files copied, are they there?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZrJ8FhpR78m"
      },
      "outputs": [],
      "source": [
        "print(f\"Display of files in target folder: {target_folder}\")\n",
        "!ls -hF ./folderOnColab/ENTOMOLOGY | head -40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flrUnfF9SRp-"
      },
      "source": [
        "### Show some pictures\n",
        "\n",
        "Let's look at the first set of images in the dataset.\n",
        "\n",
        "What happens if you change \"range_max\" to another number, like 12?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqs3zKR6STGu"
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "#- Show pictures of the image dataset\n",
        "###########################################\n",
        "\n",
        "for idx, category in enumerate(categories_short_name):\n",
        "  print(f\"{start}{category.upper()}{end}\")\n",
        "  # define location of dataset\n",
        "  folder = TRAIN_DATA_DIR + os.sep + category\n",
        "\n",
        "  # plot first few images\n",
        "  for i in range(range_max):\n",
        "    idx=i+1\n",
        "    with alive_bar(range_max,bar=\"blocks\", spinner=\"fish2\", force_tty=True) as bar:\n",
        "      bar(idx)\n",
        "      plt.subplot(plot_max + 1 + i)\n",
        "      filename = folder + os.sep + f'00{idx}.jpg'\n",
        "      image = imread(filename)\n",
        "      plt.imshow(image)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OS Configuration Options"
      ],
      "metadata": {
        "id": "O-CaUT8B3T8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(TF_CPP_MIN_LOG_LEVEL_SETTING)\n",
        "\n",
        "#To disable GPU access to the current runtime enable -1 for \"no GPUs\"\n",
        "#confirmed to work on the Command Line Interface (CLI) of all systems but this one...\n",
        "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
      ],
      "metadata": {
        "id": "dwPl2ODJ3Tic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM7Z5aaQbOX9"
      },
      "source": [
        "## Activation Functions\n",
        "\n",
        "An activation function is applied on the last fully connected layer depending on the number of categories in the images. The sigmoid activation function is used in a binary problem, while the softmax activation function is applied in a multiclass task.    \n",
        "\n",
        "## Compiling the model\n",
        "To compile the neural network the gradient descent is applied. This is the optimization strategy that reduces the errors as the network is learning. There are various optimization strategies but sgd and adam are common approachs. In the compile stage, we also define the loss function and the metrics. We use sparse categorical cross-entropy because the labels are integers. The categorical cross-entropy is used when the labels are one-hot encoded.\n",
        "\n",
        "## Hyperparameters\n",
        "\n",
        "Machine learning algorithms have hyperparameters that can be configured to tailor the algorithm to a specific dataset.\n",
        "\n",
        "Although the dynamics of many hyperparameters are known, the specific effect they will have on the performance of the resulting model on a given dataset is not known. As such, it is a standard practice to test a suite of values for key algorithm hyperparameters for a chosen machine learning algorithm.\n",
        "\n",
        "This is called hyperparameter tuning or hyperparameter optimization.\n",
        "\n",
        "It is common to use a naive optimization algorithm for this purpose, such as a random search algorithm or a grid search algorithm.\n",
        "\n",
        "Hyperparameter Tuning: Function inputs are algorithm hyperparameters, optimization problems that require an iterative global search algorithm.\n",
        "For more on this topic, see the tutorial:\n",
        "\n",
        "Hyperparameter Optimization With Random Search and Grid Search\n",
        "Nevertheless, it is becoming increasingly common to use an iterative global search algorithm for this optimization problem. A popular choice is a Bayesian optimization algorithm that is capable of simultaneously approximating the target function that is being optimized (using a surrogate function) while optimizing it.\n",
        "\n",
        "This is desirable as evaluating a single combination of model hyperparameters is expensive, requiring fitting the model on the entire training dataset one or many times, depending on the choice of model evaluation procedure (e.g. repeated k-fold cross-validation).\n",
        "\n",
        "## Optimization Algorithms\n",
        "\n",
        "Optimization algorithms are the backbone of machine learning models as they enable the modeling process to learn from a given data set. These algorithms are used in order to find the minimum or maximum of an objective function which in machine learning context stands for error or loss.\n",
        "\n",
        "## Over/Under-fitting\n",
        "\n",
        "**Overfitting**\n",
        "\n",
        "In machine learning, overfitting is when a model is too closely trained to a specific set of data, causing it to perform poorly on new data. This can happen when:\n",
        "+ The model trains for too long on the training data\n",
        "+ The model is too complex\n",
        "+ The model memorizes irrelevant information in the training data\n",
        "\n",
        "An overfitted model is similar to an invention that works well in a lab but not in the real world. It can't make accurate predictions or conclusions from any data other than the training data.\n",
        "\n",
        "**Underfitting**\n",
        "\n",
        "Underfitting in machine learning occurs when a model is unable to accurately capture the relationship between input and output data. This can lead to poor model performance and unreliable predictions.\n",
        "\n",
        "Here are some characteristics of underfitting:\n",
        "+ High bias\n",
        "  + Underfit models produce inaccurate results for both the training data and test set.\n",
        "+ Low variance\n",
        "  + Underfitting is characterized by low variance.\n",
        "+ Too simple model\n",
        "  + Underfitting can occur when a model is too simple, such as a linear regression model trained on a dataset with a polynomial relationship.\n",
        "+ Not enough training data\n",
        "  + Underfitting can occur if the model hasn't been trained for long enough or on enough data points.\n",
        "\n",
        "## References:\n",
        "+ https://machinelearningmastery.com/why-optimization-is-important-in-machine-learning/\n",
        "+ https://towardsdatascience.com/understanding-optimization-algorithms-in-machine-learning-edfdb4df766b\n",
        "+ https://geeksforgeeks.org/optimization-algorithms-in-machine-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaZUjXEMoE12"
      },
      "source": [
        "## Develop a Baseline CNN Model\n",
        "\n",
        "A baseline model will establish a minimum model performance to which all of our other models can be compared, as well as a model architecture that we can use as the basis of study and improvement.\n",
        "\n",
        "A good starting point is the general architectural principles of the VGG models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fx40WcUMkzy"
      },
      "outputs": [],
      "source": [
        "# define cnn model with a single layer\n",
        "def define_model_1Block():\n",
        "\n",
        "\t\tmodel = Sequential()\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
        "\t\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\t\tmodel.add(Dense(2, activation='softmax'))\n",
        "\n",
        "\t\t# compile model\n",
        "\t\topt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "    #model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    #model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\t\tmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=METRICS)\n",
        "\t\tmodel.summary()\n",
        "\n",
        "\t\treturn model\n",
        "\n",
        "start_t=perf_counter()\n",
        "\n",
        "try:\n",
        "  with tf.device(THE_DEVICE_NAME):\n",
        "    model1 = define_model_1Block()\n",
        "except RuntimeError as e:\n",
        "  print(str(repr(e)))\n",
        "\n",
        "end_t=perf_counter()\n",
        "print(f\"Model setup time: {end_t - start_t}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTJJMK8nMg0j"
      },
      "outputs": [],
      "source": [
        "# define cnn model with two layers\n",
        "def define_model_2Block():\n",
        "\t\tmodel = Sequential()\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
        "\t\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\t\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', ))\n",
        "\t\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\t\tmodel.add(Dense(2, activation='softmax'))\n",
        "\t\t# compile model\n",
        "\t\topt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "\t\t#model.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
        "\t\tmodel.compile(optimizer=\"adam\", loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
        "\t\tmodel.summary()\n",
        "\t\treturn model\n",
        "\n",
        "start_t=perf_counter()\n",
        "\n",
        "try:\n",
        "  with tf.device(THE_DEVICE_NAME):\n",
        "    model2 = define_model_2Block()\n",
        "except RuntimeError as e:\n",
        "  print(str(repr(e)))\n",
        "\n",
        "end_t=perf_counter()\n",
        "print(f\"Model setup time: {end_t - start_t}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLZzN-o3a8fc"
      },
      "outputs": [],
      "source": [
        "# define cnn model with 3 layers\n",
        "# Adding dropout regularization is a computationally cheap way to regularize a deep neural network.\n",
        "# Dropout works by probabilistically removing, or “dropping out,” inputs to a layer, which may be input variables\n",
        "# in the data sample or activations from a previous layer. It has the effect of simulating a large number of networks\n",
        "# with very different network structures and, in turn, making nodes in the network generally more robust to the inputs.\n",
        "# https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c\n",
        "\n",
        "def define_model_3Block():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dropout(0.24))\n",
        "  model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "  # compile model\n",
        "  model.compile(optimizer=\"adam\", loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "start_t=perf_counter()\n",
        "\n",
        "try:\n",
        "  with tf.device(THE_DEVICE_NAME):\n",
        "    model3 = define_model_3Block()\n",
        "except RuntimeError as e:\n",
        "  print(str(repr(e)))\n",
        "\n",
        "end_t=perf_counter()\n",
        "print(f\"Model setup time: {end_t - start_t}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNT8whktNCZb"
      },
      "outputs": [],
      "source": [
        "#define a new model using VGG16 with Transfer Learning\n",
        "def define_model_TransferLearning():\n",
        "  #transfer learning (VGG16)\n",
        "  model = VGG16(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "\n",
        "  # mark loaded layers as un-trainable, keep the original weights\n",
        "  for layer in model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # add new classifier layers\n",
        "  flat1 = Flatten()(model.layers[-1].output)\n",
        "  class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "\n",
        "  #this layer defines the classification, outputs much match total labels\n",
        "  output = Dense(2, activation='softmax')(class1)\n",
        "\n",
        "  # define new model\n",
        "  model = Model(inputs=model.inputs, outputs=output)\n",
        "\n",
        "  # optimization\n",
        "  opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "\n",
        "  # compile model\n",
        "  #model.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
        "  model.compile(optimizer=\"adam\", loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "\n",
        "start_t=perf_counter()\n",
        "\n",
        "try:\n",
        "  with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
        "    modelTransfer = define_model_TransferLearning()\n",
        "except RuntimeError as e:\n",
        "  print(str(repr(e)))\n",
        "\n",
        "end_t=perf_counter()\n",
        "print(f\"Transfer learning model setup time: {end_t - start_t}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no7ztdkrRcG8"
      },
      "source": [
        "## Data Marshalling and Augmentation\n",
        "\n",
        "Reference: https://keras.io/examples/vision/image_classification_from_scratch/\n",
        "\n",
        "Note that embedding the image augmentation within the model build/fit also allows the image manipulations (only available during training, not permanently stored to disk) to benefit from GPU as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxMkhxpXoc3O"
      },
      "outputs": [],
      "source": [
        "print(f\"{start}Preparing Training Datasets{end}\")\n",
        "print('\\n')\n",
        "\n",
        "try:\n",
        "  with tf.device(THE_DEVICE_NAME):\n",
        "\n",
        "      \"\"\"\n",
        "      #deprecated, use tf.data methods to replace this functionality, see: https://www.tensorflow.org/tutorials/load_data/images\n",
        "      train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "      #data augmentation is a way to improve results, uncomment this line, replacing train_datagen and re-run your experiment\n",
        "      train_datagen = ImageDataGenerator(width_shift_range=0.1,\n",
        "                                        height_shift_range=0.1,\n",
        "                                        rescale=1./255,\n",
        "                                        shear_range=0.2,\n",
        "                                        zoom_range=0.2,\n",
        "                                        rotation_range=45,\n",
        "                                        horizontal_flip=True,\n",
        "                                        vertical_flip=True,\n",
        "                                        validation_split = .2)\n",
        "      \"\"\"\n",
        "\n",
        "      #see: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory\n",
        "      train_it = keras.utils.image_dataset_from_directory(\n",
        "          directory=TRAIN_DATA_DIR,\n",
        "          labels=\"inferred\",\n",
        "          image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "          batch_size=BATCH_SIZE,\n",
        "          #class_mode='binary',\n",
        "          label_mode='categorical',\n",
        "          verbose=True,\n",
        "          #pad_to_aspect_ratio=False,\n",
        "          #crop_to_aspect_ratio=False,\n",
        "          #seed=SEED_INIT,\n",
        "          #shuffle=True,\n",
        "          #validation_split=VALIDATION_SPLIT,\n",
        "          #subset=\"training\",\n",
        "          )\n",
        "\n",
        "      #deprecated\n",
        "      #test_it = train_datagen.flow_from_directory(\n",
        "      test_it = keras.utils.image_dataset_from_directory(\n",
        "          directory=VALIDATION_DATA_DIR, # same directory as training data\n",
        "          labels=\"inferred\",\n",
        "          image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "          batch_size=BATCH_SIZE,\n",
        "          #class_mode='binary',\n",
        "          label_mode='categorical',\n",
        "          verbose=True,\n",
        "          #pad_to_aspect_ratio=False,\n",
        "          #crop_to_aspect_ratio=False,\n",
        "          #seed=SEED_INIT,\n",
        "          #shuffle=True,\n",
        "          #validation_split=VALIDATION_SPLIT,\n",
        "          #subset=\"validation\",\n",
        "          )\n",
        "      #batch size is a hyper-parameter and has influence over training as well.  Larger batch sizes get done faster but take more memory, smaller batch sizes take longer but consume less memory.\n",
        "\n",
        "except (Exception, RuntimeError) as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "\n",
        "print(\"\")\n",
        "class_names = test_it.class_names\n",
        "print(f\"{BOLD_START} Class Names (labels){BOLD_END} ##################################\")\n",
        "print(class_names)\n",
        "print(\"\")\n",
        "\n",
        "# it's a good practice to artificially introduce sample diversity by applying\n",
        "#random yet realistic transformations to the training images, such as random horizontal\n",
        "#flipping or small random rotations. This helps expose the model to different aspects of\n",
        "#the training data while slowing down overfitting.\n",
        "# Apply `data_augmentation` to the training images, see: https://www.tensorflow.org/tutorials/images/data_augmentation\n",
        "# Note that this is a CPU intensive operation, see the reference above if you\n",
        "# want to add this transformation to the GPU instead.\n",
        "\n",
        "print(\"Augmenting data...\")\n",
        "data_augmentation_layers = [\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomContrast(0.1, seed=SEED_INIT,),\n",
        "    layers.RandomCrop(IMG_HEIGHT, IMG_WIDTH, seed=SEED_INIT,),\n",
        "    layers.RandomZoom(0.1,seed=SEED_INIT,)\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "def data_augmentation(images):\n",
        "    for layer in data_augmentation_layers:\n",
        "        images = layer(images)\n",
        "    return images\n",
        "\n",
        "\n",
        "train_it = train_it.map(\n",
        "                        lambda img, label: (data_augmentation(img), label),\n",
        "                        num_parallel_calls=tf_data.AUTOTUNE,\n",
        ")\n",
        "\n",
        "#prepare the dataset for maximum performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
        "train_it= train_it.prefetch(tf_data.AUTOTUNE)\n",
        "test_it = test_it.prefetch(tf_data.AUTOTUNE)\n",
        "\n",
        "end_t=perf_counter()\n",
        "print(f\"Data read setup time: {end_t - start_t}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTtJhQ2hnL0V"
      },
      "source": [
        "## EPOCHS\n",
        "\n",
        "In machine learning, an epoch is a complete pass of a training dataset through a learning algorithm. It's a fundamental concept that's important because it directly affects how well a model learns and generalizes to new data.\n",
        "\n",
        "**Number of epochs**\n",
        "The number of epochs is a hyperparameter that defines how many times the algorithm will work through the entire dataset. The number of epochs can range from 10 to thousands.\n",
        "\n",
        "## Batch-Size\n",
        "\n",
        "The choice of batch size can have a significant impact on the learning process. A smaller batch size can lead to faster convergence and can help the model escape from local minima. However, it also introduces more noise into the gradient estimate, which can lead to instability in the learning process.\n",
        "\n",
        "A larger batch size, on the other hand, can provide a more stable learning process and a more accurate gradient estimate. However, it also requires more computational resources and may lead to slower convergence. Furthermore, it may increase the risk of the model getting stuck in local minima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op_8ukuEO2IA"
      },
      "source": [
        "## Create a Custom Class for Visualization during Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T81dBVbyzcxR"
      },
      "outputs": [],
      "source": [
        "class PlotLearning(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Callback to plot the learning curves of the model during training.\n",
        "    \"\"\"\n",
        "    def on_train_begin(self, logs={}):\n",
        "        print(\"EPOCH start\")\n",
        "        self.metrics = {}\n",
        "        for metric in logs:\n",
        "            self.metrics[metric] = []\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        print(\"EPOCH end\")\n",
        "        # Storing metrics\n",
        "        print(logs)\n",
        "        for metric in logs:\n",
        "            if metric in self.metrics:\n",
        "                self.metrics[metric].append(logs.get(metric))\n",
        "            else:\n",
        "                self.metrics[metric] = [logs.get(metric)]\n",
        "\n",
        "        # Plotting\n",
        "        metrics = [x for x in logs if 'val' not in x]\n",
        "\n",
        "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        #plot metrics, if accuracy show percent else show sliding scale\n",
        "        for i, metric in enumerate(metrics):\n",
        "            axs[i].plot(range(1, epoch + 2),\n",
        "                        self.metrics[metric],\n",
        "                        label=metric,\n",
        "                        )\n",
        "            if metric==\"accuracy\":\n",
        "              #current_ax = plt.gca()\n",
        "              #current_ax.set_ylim([0.0, 1.0])\n",
        "              #current_ax.set_ylabel(\"Percent (0.0 - 1.0)\")\n",
        "              axs[i].set_ylim([0.0, 1.0])\n",
        "              axs[i].set_ylabel(\"Percent (0.0 - 1.0)\")\n",
        "\n",
        "\n",
        "            if logs['val_' + metric]:\n",
        "                axs[i].plot(range(1, epoch + 2),\n",
        "                            self.metrics['val_' + metric],\n",
        "                            label='val_' + metric)\n",
        "                if metric==\"val_\" + metric:\n",
        "                  current_ax = plt.gca()\n",
        "                  axs[i].set_ylim([0.0, 1.0])\n",
        "                  axs[i].set_ylabel(\"Percent (0.0 - 1.0)\")\n",
        "\n",
        "            axs[i].legend()\n",
        "            axs[i].grid()\n",
        "            #current_ax = plt.gca()\n",
        "            #axs[i].set_ylim([0.0, 1.0])\n",
        "            #axs[i].set_ylabel(\"Percent (0.0 - 1.0)\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-sMQ4DVw1mn"
      },
      "source": [
        "## What is Loss and Accruacy?\n",
        "\n",
        "The lower the loss, the better a model (unless the model has over-fitted to the training data). The loss is calculated on training and validation and its interperation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets.\n",
        "\n",
        "In the case of neural networks, the loss is usually negative log-likelihood and residual sum of squares for classification and regression respectively. Then naturally, the main objective in a learning model is to reduce (minimize) the loss function's value with respect to the model's parameters by changing the weight vector values through different optimization methods, such as backpropagation in neural networks.\n",
        "\n",
        "Loss value implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of loss after each, or several, iteration(s).\n",
        "\n",
        "The accuracy of a model is usually determined after the model parameters are learned and fixed and no learning is taking place. Then the test samples are fed to the model and the number of mistakes (zero-one loss) the model makes are recorded, after comparison to the true targets. Then the percentage of misclassification is calculated.\n",
        "\n",
        "For example, if the number of test samples is 1000 and model classifies 952 of those correctly, then the model's accuracy is 95.2%.\n",
        "\n",
        "Precision and recall are metrics used in machine learning to evaluate how well a model performs at identifying positive class samples:\n",
        "\n",
        "+ **Precision**: The proportion of positive class predictions that are correct.\n",
        "+ **Recall**: The proportion of actual positive class samples that are identified by the model.\n",
        "\n",
        "Precision and recall are useful for identifying knowledge gaps in a model, especially when the dataset is imbalanced. However, there is a trade-off between the two metrics, so increasing one will usually decrease the other. For example, if a model has 100% recall, but only 50% precision, then it means that the model correctly identified all positive samples, but also incorrectly classified some other observations as positive.\n",
        "When choosing between precision and recall, it's important to consider the class balance and the cost of different errors. A common metric that combines both precision and recall is the F-score, which is the harmonic average of the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psNDTVBEy07O"
      },
      "source": [
        "### Define an Early Stopper to prevent Overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYjB8RtPyxTj"
      },
      "outputs": [],
      "source": [
        "#specify a call back\n",
        "\n",
        "#simple callback to show results real-time\n",
        "#callbacks_list = [PlotLearning()]\n",
        "\n",
        "#Watch the learning after execution\n",
        "#call this on the CLI after execution: tensorboard --logdir=./folderOnColab/logs\n",
        "tensorboard_callback = TensorBoard(log_dir=\"./folderOnColab/logs\")\n",
        "\n",
        "#another way to increase performance is to stop early, try commenting out the previous callback list\n",
        "#and uncommenting this one.  See the EPOCHS go from full execution to a shortened cycle.\n",
        "es_callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "#how to you persist your model, how to you deal with overfitting?\n",
        "mc_callback = ModelCheckpoint('best_model1.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOC03czMO0CK"
      },
      "source": [
        "## Train 1 Block Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wsDSMW3oqsB"
      },
      "outputs": [],
      "source": [
        "# configure callbacks (performed per epoch) for this particular model\n",
        "callbacks_list = [PlotLearning(), es_callback, mc_callback, tensorboard_callback]\n",
        "\n",
        "try:\n",
        "  with tf.device(THE_DEVICE_NAME):\n",
        "    # fit model, actual training against data, see: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
        "    history1 = model1.fit(train_it,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          epochs=EPOCHS,\n",
        "                          callbacks=callbacks_list,\n",
        "                          validation_data=test_it,\n",
        "                          validation_split=VALIDATION_SPLIT,\n",
        "                          class_weight=None,             #you would be a set of weights per category if you had an unbalanced classification problem\n",
        "                          verbose=2,                     #0 silent, 1 more, 2 max\n",
        "                          )\n",
        "\n",
        "except (Exception, RuntimeError) as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "\n",
        "\n",
        "end_t=perf_counter()\n",
        "print(f\"Model fit time: {end_t - start_t}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rh__nOOPKm1"
      },
      "source": [
        "## Train Model 2 Block (2 layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ_5Sy27PPFo"
      },
      "outputs": [],
      "source": [
        "mc_callback = ModelCheckpoint('best_model2.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "callbacks_list = [PlotLearning(), es_callback, mc_callback, tensorboard_callback]\n",
        "\n",
        "try:\n",
        "  with tf.device(THE_DEVICE_NAME):\n",
        "    # fit model, actual training against data, see: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
        "    history2 = model2.fit(train_it,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          epochs=EPOCHS,\n",
        "                          callbacks=callbacks_list,\n",
        "                          validation_data=test_it,\n",
        "                          validation_split=VALIDATION_SPLIT,\n",
        "                          class_weight=None,             #you would be a set of weights per category if you had an unbalanced classification problem\n",
        "                          verbose=2,                     #0 silent, 1 more, 2 max\n",
        "                          )\n",
        "\n",
        "except (Exception, RuntimeError) as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "\n",
        "\n",
        "end_t=perf_counter()\n",
        "print(f\"Model fit time: {end_t - start_t}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJMB2zTuPSGk"
      },
      "source": [
        "## Train Model 3 Block (3 layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAsmzk_WPRXC"
      },
      "outputs": [],
      "source": [
        "mc_callback = ModelCheckpoint('best_model3.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "callbacks_list = [PlotLearning(), es_callback, mc_callback, tensorboard_callback]\n",
        "\n",
        "try:\n",
        "  with tf.device(THE_DEVICE_NAME):\n",
        "    # fit model, actual training against data, see: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
        "    history3 = model3.fit(train_it,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          epochs=EPOCHS,\n",
        "                          callbacks=callbacks_list,\n",
        "                          validation_data=test_it,\n",
        "                          validation_split=VALIDATION_SPLIT,\n",
        "                          class_weight=None,             #you would be a set of weights per category if you had an unbalanced classification problem\n",
        "                          verbose=2,                     #0 silent, 1 more, 2 max\n",
        "                          )\n",
        "\n",
        "except (Exception, RuntimeError) as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "\n",
        "\n",
        "end_t=perf_counter()\n",
        "print(f\"Model fit time: {end_t - start_t}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWsTR8ZfPWM2"
      },
      "source": [
        "## Tranfer Learning Model (VGG 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddvLxqNDPZOJ"
      },
      "outputs": [],
      "source": [
        "es_callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "mc_callback = ModelCheckpoint('best_modelVGG16.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "callbacks_list = [PlotLearning(), es_callback, mc_callback, tensorboard_callback]\n",
        "\n",
        "try:\n",
        "  with tf.device(THE_DEVICE_NAME):\n",
        "    # fit model, actual training against data, see: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
        "    historyTransfer = modelTransfer.fit(train_it,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        epochs=EPOCHS,\n",
        "                                        callbacks=callbacks_list,\n",
        "                                        validation_data=test_it,\n",
        "                                        validation_split=VALIDATION_SPLIT,\n",
        "                                        class_weight=None,             #you would be a set of weights per category if you had an unbalanced classification problem\n",
        "                                        verbose=2,                     #0 silent, 1 more, 2 max\n",
        "                                       )\n",
        "\n",
        "except (Exception, RuntimeError) as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "\n",
        "\n",
        "end_t=perf_counter()\n",
        "print(f\"Model fit time: {end_t - start_t}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AdW3Tx-o2hS"
      },
      "outputs": [],
      "source": [
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics_seaborn(history, title):\n",
        "  # Create pandas DataFrame\n",
        "  df_history = pd.DataFrame(history.history)\n",
        "  #print(df_history)\n",
        "\n",
        "  #turn values into real percents\n",
        "  df_history['Training_Accuracy'] = df_history[\"accuracy\"] * 100.0\n",
        "  df_history['Validation_Accuracy'] = df_history[\"val_accuracy\"] * 100.0\n",
        "\n",
        "  #color palette\n",
        "  palette = ['r','b','g']\n",
        "\n",
        "  # Plot using Seaborn\n",
        "  sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
        "  sns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\n",
        "  fig = plt.figure(figsize=[7,5])\n",
        "  ax = plt.subplot(111)\n",
        "  my_plot = sns.lineplot(data=df_history[[\"Training_Accuracy\",\"Validation_Accuracy\"]],\n",
        "                         markers=True, dashes=False,palette=palette)\n",
        "\n",
        "  my_plot.set_xlabel('Epochs')\n",
        "  my_plot.set_ylim(0,100)\n",
        "  my_plot.yaxis.set_major_formatter(mpl.ticker.PercentFormatter())\n",
        "  my_plot.set_ylabel('Accuracy')\n",
        "\n",
        "  plt.title('Training and Validation Loss \\n' + title)\n",
        "  ttl = ax.title\n",
        "  ttl.set_weight('bold')\n",
        "\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0_mPjkfdeJs"
      },
      "outputs": [],
      "source": [
        "def show_performance_curve(training_result, metric, metric_label):\n",
        "\n",
        "\ttrain_perf = training_result.history[str(metric)]\n",
        "\tvalidation_perf = training_result.history['val_'+str(metric)]\n",
        "\tintersection_idx = np.argwhere(np.isclose(train_perf,\n",
        "                                            \tvalidation_perf, atol=1e-2)).flatten()[0]\n",
        "\tintersection_value = train_perf[intersection_idx]\n",
        "\n",
        "\tplt.plot(train_perf, label=metric_label)\n",
        "\tplt.plot(validation_perf, label = 'val_'+str(metric))\n",
        "\tplt.axvline(x=intersection_idx, color='r', linestyle='--', label='Intersection')\n",
        "\n",
        "\tplt.annotate(f'Optimal Value: {intersection_value:.4f}',\n",
        "         \txy=(intersection_idx, intersection_value),\n",
        "         \txycoords='data',\n",
        "         \tfontsize=10,\n",
        "         \tcolor='green')\n",
        "\n",
        "\tplt.xlabel('Epoch')\n",
        "\tplt.ylabel(metric_label)\n",
        "\tplt.legend(loc='lower right')\n",
        "\tplt.title(metric)\n",
        "\tplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRZ-3hKmJO5v"
      },
      "source": [
        "## Demonstrate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZoK-f5bo5T0"
      },
      "outputs": [],
      "source": [
        "# model 1\n",
        "try:\n",
        "    print(f'  {start}1 Layer Model{end}')\n",
        "    #loss, accuracy, precision, recall = model1.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
        "    loss, accuracy, precision, recall = model1.evaluate(test_it, verbose=2)\n",
        "\n",
        "    print(f'  {start}Loss{end} > {loss: .2f}')\n",
        "    print(f'{start}Accuracy{end}> {accuracy: .2%}')\n",
        "    print(\"\\n\\n\")\n",
        "    summarize_diagnostics_seaborn(history1, \"One Layer\")\n",
        "    #show_performance_curve(history1, 'accuracy', 'accuracy')\n",
        "    #show_performance_curve(history1, 'precision', 'precision')\n",
        "except Exception as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_aMxcRJIwf9"
      },
      "outputs": [],
      "source": [
        "#model 2\n",
        "print(\"\\n\")\n",
        "print(f'  {start}2 Layer Model{end}')\n",
        "loss, accuracy, precision, recall = model2.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "print(f'  {start}Loss{end} > {loss: .2f}')\n",
        "print(f'{start}Accuracy{end}> {accuracy: .2%}')\n",
        "print(\"\\n\\n\")\n",
        "summarize_diagnostics_seaborn(history2, \"Two Layers\")\n",
        "#show_performance_curve(history2, 'accuracy', 'accuracy')\n",
        "#show_performance_curve(history2, 'precision', 'precision')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXocrLceI0yl"
      },
      "outputs": [],
      "source": [
        "# model 3\n",
        "print(\"\\n\")\n",
        "print(f'  {start}3 Layer Model{end}')\n",
        "loss, accuracy, precision, recall = model3.evaluate(test_it, steps=len(test_it), verbose=2)\n",
        "print(f'  {start}Loss{end} > {loss: .2f}')\n",
        "print(f'{start}Accuracy{end}> {accuracy: .2%}')\n",
        "print(\"\\n\\n\")\n",
        "summarize_diagnostics_seaborn(history3, \"Three Layers\")\n",
        "#show_performance_curve(history3, 'accuracy', 'accuracy')\n",
        "#show_performance_curve(history3, 'precision', 'precision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwHyjZLgI56W"
      },
      "outputs": [],
      "source": [
        "#model transfer learning\n",
        "try:\n",
        "  print(f'{start}VGG16 Model{end}')\n",
        "  print(\"\\n\")\n",
        "  loss, accuracy, precision, recall = modelTransfer.evaluate(test_it, steps=len(test_it), verbose=2)\n",
        "  print(f'  {start}Loss{end} > {loss: .2f}')\n",
        "  print(f'{start}Accuracy{end}> {accuracy: .2%}')\n",
        "  print(\"\\n\\n\")\n",
        "  summarize_diagnostics_seaborn(historyTransfer, \"Transfer Learning\")\n",
        "  #show_performance_curve(historyTransfer, 'accuracy', 'accuracy')\n",
        "  #show_performance_curve(historyTransfer, 'precision', 'precision')\n",
        "except Exception as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9qFriF7bnh"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfEyJlYgCRiT"
      },
      "outputs": [],
      "source": [
        "print(f\"{start}Loading saved model definitions and weights.{end}\")\n",
        "try:\n",
        "    best_model1 = load_model('./best_model1.keras')\n",
        "    best_model2 = load_model('./best_model2.keras')\n",
        "    best_model3 = load_model('./best_model3.keras')\n",
        "    best_modelVGG16 = load_model('./best_modelVGG16.keras')\n",
        "except Exception as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJPdM40r7dA1"
      },
      "outputs": [],
      "source": [
        "# load and prepare the image\n",
        "def load_image(img):\n",
        "\n",
        "  #resize the image\n",
        "  newsize = (IMG_HEIGHT, IMG_WIDTH)\n",
        "  img1 = img.resize(newsize)\n",
        "\n",
        "  # convert to array\n",
        "  img2 = img_to_array(img1)\n",
        "\n",
        "  # reshape into a single sample with 3 channels\n",
        "  img = img2.reshape(1, IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "\n",
        "  # ensure datatype\n",
        "  img = img.astype('float32')\n",
        "\n",
        "  # scale image, we did this to the training data, you must scale the inference input!!!\n",
        "  img = img / 255.0\n",
        "  return img\n",
        "\n",
        "# load an image and predict the class\n",
        "def run_example(inc_url, inc_model):\n",
        "\n",
        "  # load the image\n",
        "  img = load_image(inc_url)\n",
        "\n",
        "  # predict the class\n",
        "  result = inc_model.predict(img)\n",
        "  return result\n",
        "\n",
        "print(\"Load a sample image.\")\n",
        "filename=\"./folderOnColab/ENTOMOLOGY/validation/alb/001.jpg\"\n",
        "#filename=\"./folderOnColab/ENTOMOLOGY/validation/slf/001.jpg\"\n",
        "img = load_img(filename, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "\n",
        "print(\"Obtain an inference from each model.\")\n",
        "try:\n",
        "    results1 = run_example(img, best_model1)\n",
        "    results2 = run_example(img, best_model2)\n",
        "    results3 = run_example(img, best_model3)\n",
        "    resultsVGG16 = run_example(img, best_modelVGG16)\n",
        "except Exception as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "\n",
        "the_logo = Image.open(filename)\n",
        "\n",
        "#turn the image into a displayed graphic\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.imshow(the_logo)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZiWxJsWFQWB"
      },
      "outputs": [],
      "source": [
        "print(f\"{start}Evaluate the Models{end}\")\n",
        "print('\\n')\n",
        "\n",
        "#model 1\n",
        "try:\n",
        "  class_names = categories\n",
        "  predicted_class_index = np.argmax(results1)\n",
        "  predicted_class = class_names[predicted_class_index]\n",
        "  print(f'  {start}1 Layer Model{end}')\n",
        "  print(f'{start}{predicted_class}{end}> {results1}')\n",
        "except Exception as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "print('\\n')\n",
        "\n",
        "#model 2\n",
        "try:\n",
        "  class_names = categories\n",
        "  predicted_class_index = np.argmax(results2)\n",
        "  predicted_class = class_names[predicted_class_index]\n",
        "  print(\"\\n\")\n",
        "  print(f'  {start}2 Layer Model{end}')\n",
        "  #print(f'{start}{predicted_class}{end}> {predicted_class_index: .2%}')\n",
        "  print(f'{start}{predicted_class}{end}> {results2}')\n",
        "except Exception as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "print('\\n')\n",
        "\n",
        "#model 3\n",
        "try:\n",
        "  class_names = categories\n",
        "  predicted_class_index = np.argmax(results3)\n",
        "  predicted_class = class_names[predicted_class_index]\n",
        "  print(\"\\n\")\n",
        "  print(f'  {start}3 Layer Model{end}')\n",
        "  #print(f'{start}{predicted_class}{end}> {predicted_class_index: .2%}')\n",
        "  print(f'{start}{predicted_class}{end}> {results3}')\n",
        "except Exception as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "print('\\n')\n",
        "\n",
        "#VGG16\n",
        "try:\n",
        "  class_names = categories\n",
        "  predicted_class_index = np.argmax(resultsVGG16)\n",
        "  predicted_class = class_names[predicted_class_index]\n",
        "  print(\"\\n\")\n",
        "  print(f'  {start}VGG16 Model{end}')\n",
        "  #print(f'{start}{predicted_class}{end}> {predicted_class_index: .2%}')\n",
        "  print(f'{start}{predicted_class}{end}> {resultsVGG16}')\n",
        "except Exception as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P66v05Uq94C_"
      },
      "outputs": [],
      "source": [
        "# Interpret the predictions\n",
        "try:\n",
        "  print(f\"{start}Model VGG16 Analysis{end}\")\n",
        "  class_names = categories\n",
        "  predicted_class_index = np.argmax(resultsVGG16)\n",
        "  predicted_class = class_names[predicted_class_index]\n",
        "\n",
        "  print(f\"{start}Predicted class:{end}\", predicted_class)\n",
        "  print(f\"{start}Predicted class index:{end}\", predicted_class_index)\n",
        "  print(f\"{start}Predicted class probability:{end}, {resultsVGG16[0][predicted_class_index]:.1%}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ino8BmE7fdhJ"
      },
      "source": [
        "## Inference through Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm1t3aZtbKY9"
      },
      "outputs": [],
      "source": [
        "#get a list of all validation files\n",
        "import os\n",
        "import random\n",
        "\n",
        "alb_files=os.listdir(\"./folderOnColab/ENTOMOLOGY/validation/alb\")\n",
        "slf_files=os.listdir(\"./folderOnColab/ENTOMOLOGY/validation/slf\")\n",
        "\n",
        "#divisor, cut down just how many images we show\n",
        "divisor=5\n",
        "\n",
        "#determine how many you have\n",
        "alb_num=len(alb_files)\n",
        "slf_num=len(slf_files)\n",
        "print(f\"There are {alb_num} files in the Asian Longhorn Bettle domain.\")\n",
        "print(f\"There are {slf_num} files in the Spotted Lantern Fly domain.\")\n",
        "\n",
        "minimal_count = int(min([alb_num, slf_num])/divisor)  #let's not make too many images\n",
        "print(f\"There are {minimal_count} total files you can work with in either folder.\")\n",
        "print(f\"Creating random list of files to choose from.\")\n",
        "\n",
        "#build an array to hold the randomly created names\n",
        "y_true_filenames=[]\n",
        "y_true = []\n",
        "for idx in range(0,minimal_count):\n",
        "  category_idx=int(random.randrange(0, 2))\n",
        "  file_idx=int(random.randrange(0,minimal_count))\n",
        "  if category_idx==0:\n",
        "    category=\"alb\"\n",
        "  else:\n",
        "    category=\"slf\"\n",
        "  if file_idx < 10:\n",
        "    filename=\"00\"\n",
        "  else:\n",
        "    filename=\"0\"\n",
        "  the_filename=f\"./folderOnColab/ENTOMOLOGY/validation/{category}/{filename}{file_idx}.jpg\"\n",
        "  y_true_filenames.append(the_filename)\n",
        "  y_true.append(class_names[category_idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoOka4yfWmYs"
      },
      "outputs": [],
      "source": [
        "#create arrays to hold the results.\n",
        "y_pred_Model1 = []\n",
        "y_pred_Model2 = []\n",
        "y_pred_Model3 = []\n",
        "y_pred_VGG16 = []\n",
        "\n",
        "\n",
        "class_names = categories\n",
        "#now iterate through each image, mark the identification, gather the results and produce a confusion matrix to show true quality of the image model\n",
        "for idx, target_filename, in enumerate(y_true_filenames):\n",
        "    img = load_img(target_filename, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    try:\n",
        "        results1=run_example(img, best_model1)\n",
        "    except Exception as e:\n",
        "        print(f\"Model 1 had issues with inference, see: {str(repr(e))}\")\n",
        "\n",
        "    try:\n",
        "        results2=run_example(img, best_model2)\n",
        "    except Exception as e:\n",
        "        print(f\"Model 2 had issues with inference, see: {str(repr(e))}\")\n",
        "\n",
        "    try:\n",
        "        results3=run_example(img, best_model3)\n",
        "    except Exception as e:\n",
        "        print(f\"Model 3 had issues with inference, see: {str(repr(e))}\")\n",
        "\n",
        "    try:\n",
        "        resultsVGG16 = run_example(img, best_modelVGG16)\n",
        "    except Exception as e:\n",
        "        print(f\"Model VGG16 had issues with inference, see: {str(repr(e))}\")\n",
        "\n",
        "\n",
        "    print(f\"TRUTH == {y_true[idx]}\")\n",
        "\n",
        "    try:\n",
        "        predicted_class_index = np.argmax(results1)\n",
        "        predicted_class = class_names[predicted_class_index]\n",
        "        y_pred_Model1.append( predicted_class )\n",
        "        print(f\"Iteration[{idx}] = Model#1 {start}Predicted class:{end}: {predicted_class}[{predicted_class_index}], {resultsVGG16[0][predicted_class_index]:.1%} \")\n",
        "    except Exception as e:\n",
        "        print(f\"Model 1 had issues, see: {str(repr(e))}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        predicted_class_index = np.argmax(results2)\n",
        "        predicted_class = class_names[predicted_class_index]\n",
        "        y_pred_Model2.append( predicted_class )\n",
        "        print(f\"Iteration[{idx}] = Model#1 {start}Predicted class:{end}: {predicted_class}[{predicted_class_index}], {results2[0][predicted_class_index]:.1%} \")\n",
        "    except Exception as e:\n",
        "        print(f\"Model 2 had issues, see: {str(repr(e))}\")\n",
        "\n",
        "    try:\n",
        "        predicted_class_index = np.argmax(results3)\n",
        "        predicted_class = class_names[predicted_class_index]\n",
        "        y_pred_Model3.append( predicted_class )\n",
        "        print(f\"Iteration[{idx}] = Model#1 {start}Predicted class:{end}: {predicted_class}[{predicted_class_index}], {results3[0][predicted_class_index]:.1%} \")\n",
        "    except Exception as e:\n",
        "        print(f\"Model 3 had issues, see: {str(repr(e))}\")\n",
        "\n",
        "    try:\n",
        "        predicted_class_index = np.argmax(resultsVGG16)\n",
        "        predicted_class = class_names[predicted_class_index]\n",
        "        y_pred_VGG16.append( predicted_class )\n",
        "        print(f\"Iteration[{idx}] = VGG16 {start}Predicted class:{end}: {predicted_class}[{predicted_class_index}], {resultsVGG16[0][predicted_class_index]:.1%} \")\n",
        "    except Exception as e:\n",
        "        print(f\"Model VGG16 had issues, see: {str(repr(e))}\")\n",
        "\n",
        "    the_logo = Image.open(target_filename)\n",
        "    #turn the image into a displayed graphic\n",
        "    plt.figure(figsize=(5,3))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(the_logo)\n",
        "    plt.show()\n",
        "    print(\"#######################################################################################\")\n",
        "    print(\"\\n\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1aeHbpkWao9"
      },
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "The confusion matrix is a 2 dimensional array comparing predicted category labels to the true label. For binary classification, these are the True Positive, True Negative, False Positive and False Negative categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFmxY0g7fnuQ"
      },
      "outputs": [],
      "source": [
        "# Magic function that renders the figure in a jupyter notebook\n",
        "# instead of displaying a figure object\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "#build the confusion matrices\n",
        "cms={}\n",
        "try:\n",
        "    cms['Model 1']=confusion_matrix(y_true, y_pred_Model1, labels=class_names)\n",
        "except Exception as e:\n",
        "    print(f\"Model 1 had issues with inference, see: {str(repr(e))}\")\n",
        "\n",
        "try:\n",
        "    cms['Model 2']=confusion_matrix(y_true, y_pred_Model2, labels=class_names)\n",
        "except Exception as e:\n",
        "    print(f\"Model 2 had issues with inference, see: {str(repr(e))}\")\n",
        "\n",
        "try:\n",
        "    cms['Model 3']=confusion_matrix(y_true, y_pred_Model3, labels=class_names)\n",
        "except Exception as e:\n",
        "    print(f\"Model 3 had issues with inference, see: {str(repr(e))}\")\n",
        "\n",
        "try:\n",
        "    cms['Model VGG16']=confusion_matrix(y_true, y_pred_VGG16, labels=class_names)\n",
        "except Exception as e:\n",
        "    print(f\"Model 1 had issues with inference, see: {str(repr(e))}\")\n",
        "\n",
        "elements=len(cms)\n",
        "\n",
        "# create the plot\n",
        "f, axes = plt.subplots(1, elements, figsize=(8.5, 11.0), sharey='row')\n",
        "# Setting default size of the plot\n",
        "plt.rcParams['figure.figsize'] = (8.5, 11.0)\n",
        "# Setting default fontsize used in the plot\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Setting fontsize for xticks and yticks\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "for idx, key in enumerate(cms.keys()):\n",
        "    disp = ConfusionMatrixDisplay(cms[key],\n",
        "                                  display_labels=class_names)\n",
        "    disp.plot(ax=axes[idx], xticks_rotation=45)\n",
        "    disp.ax_.set_title(key)\n",
        "    disp.im_.colorbar.remove()\n",
        "    disp.ax_.set_xlabel('')\n",
        "    if idx!=0:\n",
        "        disp.ax_.set_ylabel('')\n",
        "\n",
        "f.text(0.4, 0.1, 'Predicted label', ha='left')\n",
        "plt.subplots_adjust(wspace=0.70, hspace=0.5)\n",
        "\n",
        "# Giving name to the plot\n",
        "#plt.title('Confusion Matrix for Invasive Species', fontsize=24)\n",
        "\n",
        "f.colorbar(disp.im_, ax=axes)\n",
        "plt.show()\n",
        "# Saving plot\n",
        "#plt.savefig('confusion_matrix', transparent=True, dpi=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54Sd886JkDwP"
      },
      "source": [
        "## Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbSBkeX6kGVS"
      },
      "outputs": [],
      "source": [
        "#Showing the main classification metrics\n",
        "print(f\"{start}Model 1 Classification Report{end}\")\n",
        "print(classification_report(y_true, y_pred_Model1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Showing the main classification metrics\n",
        "print(f\"{start}Model 2 Classification Report{end}\")\n",
        "print(classification_report(y_true, y_pred_Model2))"
      ],
      "metadata": {
        "id": "Ug2VnZ0u3iWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Showing the main classification metrics\n",
        "print(f\"{start}Model 3 Classification Report{end}\")\n",
        "print(classification_report(y_true, y_pred_Model3))"
      ],
      "metadata": {
        "id": "qduHLOKf5QAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH-p1p6kkdAQ"
      },
      "outputs": [],
      "source": [
        "#Showing the main classification metrics\n",
        "print(f\"{start}Model VGG16 Classification Report{end}\")\n",
        "print(classification_report(y_true, y_pred_VGG16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cDfqFeyehJ6"
      },
      "source": [
        "## Show the Model Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luR_YlDqRVaW"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    #best_model1, best_model2, best_model3, best_modelTransfer\n",
        "    target_model='best_model1'\n",
        "    tf.keras.utils.plot_model(\n",
        "        load_model(target_model+'.keras'),\n",
        "        to_file='./'+target_model+'.png',\n",
        "        show_shapes=True,\n",
        "        show_layer_names=True,\n",
        "        rankdir=\"TB\",\n",
        "        expand_nested=True,\n",
        "        dpi=96,\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  img=Image.open('./'+target_model+'.png')\n",
        "  img.show()\n",
        "except Exception as e:\n",
        "    print(f\"{start}ERROR (Exception encountered): Unable to display image.{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "STEM-005_ComputerVision.ipynb",
      "provenance": []
    },
    "environment": {
      "kernel": "conda-env-tensorflow-tensorflow",
      "name": "workbench-notebooks.m124",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
    },
    "kernelspec": {
      "display_name": "TensorFlow 2-11 (Local)",
      "language": "python",
      "name": "conda-env-tensorflow-tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}