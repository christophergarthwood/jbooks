{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Develop a Deep CNN for Multi-Label Classification of Photos\n",
    "\n",
    "Overview\n",
    "\n",
    "Read the reference URI for details.  Given Planet satellite images that are already trained and classified, create a multi-label response for a given image.  \n",
    "\n",
    "*Note: You must register for a Kaggle account to get access to the data.*\n",
    "\n",
    "Again, read the article about getting the data:\n",
    "\n",
    "https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data\n",
    "\n",
    "References:\n",
    "+ https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.7.3\n",
    "############################################\n",
    "# INCLUDES\n",
    "############################################\n",
    "#libraries specific to this example\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from os import listdir\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.metrics import fbeta_score\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# seed the pseudorandom number generator\n",
    "from random import seed\n",
    "from random import random\n",
    "from random import randint\n",
    "\n",
    "#a set of libraries that perhaps should always be in Python source\n",
    "import os \n",
    "import datetime\n",
    "import sys\n",
    "import gc\n",
    "import getopt\n",
    "import inspect\n",
    "import math\n",
    "import warnings\n",
    "import types\n",
    "\n",
    "#Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#a darn useful library for creating paths and one I recommend you load to your environment\n",
    "from pathlib import Path\n",
    "\n",
    "# can type in the python console `help(name of function)` to get the documentation\n",
    "from pydoc import help                          \n",
    "\n",
    "#Import a custom library, in this case a fairly useful logging framework\n",
    "debug_lib_location = Path(\"./\")\n",
    "sys.path.append(str(debug_lib_location))\n",
    "import debug\n",
    "\n",
    "warnings.filterwarnings('ignore')               # don't print out warnings\n",
    "\n",
    "\n",
    "root_location=\".\" + os.sep + \"data\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn on Eager Execution\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#JUPYTER NOTEBOOK OUTPUT CONTROL / FORMATTING\n",
    "############################################\n",
    "#set floating point to 4 places to things don't run loose\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# GLOBAL VARIABLES\n",
    "############################################\n",
    "DEBUG = 1                            #General ledger output so you know what's happening.\n",
    "DEBUG_DATA = 1                       #Extremely verbose output, change to zero (0) to supress the volume of output.\n",
    "\n",
    "# CODE CONSTRAINTS\n",
    "VERSION_NAME    = \"CNN\"\n",
    "VERSION_ACRONYM = \"ML-CNN\"\n",
    "VERSION_MAJOR   = 0\n",
    "VERSION_MINOR   = 0\n",
    "VERSION_RELEASE = \"7\"\n",
    "VERSION_TITLE   = VERSION_NAME + \" (\" + VERSION_ACRONYM + \") \" + str(VERSION_MAJOR) + \".\" + str(VERSION_MINOR) + \".\" + str(VERSION_RELEASE) + \" generated SEED.\"\n",
    "\n",
    "ENCODING  =\"utf-8\"\n",
    "############################################\n",
    "# GLOBAL CONSTANTS\n",
    "############################################\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "############################################\n",
    "# APPLICATION VARIABLES\n",
    "############################################\n",
    "\n",
    "\n",
    "############################################\n",
    "# GLOBAL CONFIGURATION\n",
    "############################################\n",
    "os.environ['PYTHONIOENCODING']=ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Function Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# WARNING / ERROR Management\n",
    "############################################\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "\n",
    "############################################\n",
    "# FUNCTIONS\n",
    "############################################\n",
    "\n",
    "def prototype(incMonth):\n",
    "\n",
    "    debug.msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
    "    debug.msg_info(\"The month you passed in was \" + str(incMonth))\n",
    "    debug.msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
    "    return 1\n",
    "\n",
    "def lib_diagnostics():\n",
    "    debug.msg_debug(\"System version    #:{:>12}\".format(sys.version))\n",
    "    netcdf4_version_info = nc.getlibversion().split(\" \")\n",
    "    debug.msg_debug(\"netCDF4 version   #:{:>12}\".format(netcdf4_version_info[0]))\n",
    "    debug.msg_debug(\"Matplotlib version#:{:>12}\".format(matplt.__version__))\n",
    "    debug.msg_debug(\"Numpy version     #:{:>12}\".format(np.__version__))\n",
    "    debug.msg_debug(\"Pandas version    #:{:>12}\".format(pd.__version__))\n",
    "    debug.msg_debug(\"SciPy version     #:{:>12}\".format(sp.__version__))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def get_full_version():\n",
    "\n",
    "    resultant = str(VERSION_NAME) + \"  v\" + str(VERSION_MAJOR) + \".\" + str(VERSION_MINOR) + \".\" + str(VERSION_RELEASE)\n",
    "    return resultant\n",
    "\n",
    "def get_version():\n",
    "\n",
    "    resultant = str(VERSION_MAJOR) + \".\" + str(VERSION_MINOR) + \".\" + str(VERSION_RELEASE)\n",
    "    return resultant\n",
    "\n",
    "def printversion():\n",
    "\n",
    "    print(get_full_version())\n",
    "\n",
    "def printusage():\n",
    "\n",
    "    print(\"\")\n",
    "    printversion()\n",
    "    print(\"  -v, --version    prints the version of this software package.\")\n",
    "    print(\"\")\n",
    "    print(\"  * - indicates required argument.\")\n",
    "\n",
    "######################################################################\n",
    "#Support routines to see columns in DataFrames\n",
    "######################################################################\n",
    "def show_columns_plain(inc_ary):\n",
    "    new_ary = []\n",
    "    for col in inc_ary:\n",
    "        new_ary.append(np.char.lower(col))\n",
    "    new_ary.sort\n",
    "    myOutputString = \" \"\n",
    "    for col in new_ary:\n",
    "        myOutputString = myOutputString + \" \" + str(col)\n",
    "    return myOutputString\n",
    "\n",
    "def show_columns_true(inc_ary):\n",
    "    new_ary = []\n",
    "    for col in inc_ary:\n",
    "        new_ary.append(col)\n",
    "    new_ary.sort\n",
    "    myOutputString = \" \"\n",
    "    for col in new_ary:\n",
    "        myOutputString = myOutputString + \" \" + str(col)\n",
    "    return myOutputString\n",
    "\n",
    "######################################################################\n",
    "#Input Validation\n",
    "######################################################################\n",
    "# valid string:\n",
    "#  We don't want the following:\n",
    "#   - at the start of the file name (might be construed as a switch)\n",
    "#  $, &, |, ;, <, >, `, !, *, \", \\ (to start with)\n",
    "###\n",
    "def validstring(testsubject):\n",
    "\n",
    "    if testsubject[0] == \"-\":\n",
    "        return 0\n",
    "    elif \"$\" in testsubject or \"&\" in testsubject or \"|\" in testsubject:\n",
    "        return 0\n",
    "    elif \";\" in testsubject or \"`\" in testsubject or \"!\" in testsubject:\n",
    "        return 0\n",
    "    elif \"*\" in testsubject or '\"' in testsubject or \"\\\\\" in testsubject:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./libs.py\n",
    "#Title:     Displays the libraries in current use.\n",
    "#Objective: Invocation is intended as function calls within another program.\n",
    "#Assumptions:\n",
    "#           1. Should be stored in standardized location such as:\n",
    "#                      /p/home/{user_name}/usr/PYTHONLIB\n",
    "#           2. Developer loads the module (Jupyter Lab).\n",
    "#Pre-Requisites:\n",
    "#           1. Python v3.*\n",
    "#           2. Jupyter Lab / Notebook (%load libs.py)\n",
    "#Usage:\n",
    "#       %load libs.py\n",
    "#       find_loaded_modules().HTML\n",
    "#\n",
    "#Version History:\n",
    "# ------------------------------------------------------------------------\n",
    "# Version   Date       Modification                              Author\n",
    "# ------------------------------------------------------------------------\n",
    "# 1.0       2020/04/29 Inception                                 Radiance\n",
    "# ------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#LIBRARIES\n",
    "#######################################################################\n",
    "import os\n",
    "import types\n",
    "\n",
    "def module_version(mod):\n",
    "    '''Return version string for module *mod*, or nothing if\n",
    "    it doesn't have a \"version\" or \"__version__\" attribute.'''\n",
    "    version = []\n",
    "    if hasattr(mod, '__dict__'):\n",
    "        keys = []\n",
    "        for key in mod.__dict__.keys():\n",
    "            if key.lower() == 'version' or key.lower() == '__version__':\n",
    "                v = mod.__dict__[key]\n",
    "                if (str):\n",
    "                    if isinstance(v, str):\n",
    "                        version.append(v)\n",
    "                else:\n",
    "                    version.append(\"No version\")\n",
    "        if keys:\n",
    "            print (mod, keys)\n",
    "    if version:\n",
    "        return ', '.join(version)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def find_loaded_modules(only_versioned_modules=True):\n",
    "\n",
    "    def list_of_lists_to_HTML(lists, header_row=None):\n",
    "        '''Convert a list of a list of strings to a HTML table.'''\n",
    "        s = '<table>'\n",
    "        if header_row:\n",
    "            s += '\\n\\t<tr>\\n\\t\\t'\n",
    "            s += ''.join(['<th>%s</th>' % item for item in header_row])\n",
    "            s += '\\n\\t</tr>'\n",
    "        for inner_list in lists:\n",
    "            s += '\\n\\t<tr>\\n\\t\\t'\n",
    "            s += ''.join(['<td>%s</td>' % item for item in inner_list])\n",
    "            s += '\\n\\t</tr>'\n",
    "        s += '\\n</table>'\n",
    "        return s\n",
    "    \n",
    "    class LoadedModules(list):\n",
    "        '''Very simple wrapper for a list of lists of strings, with an attribute\n",
    "        for display in IPython Notebooks.'''\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            list.__init__(self, *args, **kwargs)\n",
    "            \n",
    "        @property\n",
    "        def HTML(self):\n",
    "            from IPython.display import HTML\n",
    "            return HTML(\n",
    "                    list_of_lists_to_HTML(\n",
    "                            self, header_row=['Name', 'Version']))\n",
    "                    \n",
    "    objs = LoadedModules()\n",
    "    for i, mod in enumerate(globals().values()):\n",
    "        if isinstance(mod, types.ModuleType):\n",
    "            if hasattr(mod, '__name__'):\n",
    "                name = mod.__name__\n",
    "            else:\n",
    "                name = ''\n",
    "            \n",
    "            version = module_version(mod)\n",
    "            \n",
    "            objs.append([mod.__name__, version])\n",
    "    objs.sort(key=lambda r: r[0])\n",
    "    return objs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "\t<tr>\n",
       "\t\t<th>Name</th><th>Version</th>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>builtins</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>builtins</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>datetime</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>debug</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>gc</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>getopt</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>inspect</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>math</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>matplotlib</td><td>3.2.1</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>matplotlib.pyplot</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>numpy</td><td>1.18.4</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>os</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>pandas</td><td>1.0.3</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>scipy</td><td>1.4.1</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>scipy</td><td>1.4.1</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>sys</td><td>3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) \n",
       "[GCC 7.3.0]</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>tensorflow</td><td>2.3.0</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>tensorflow.keras</td><td>2.4.0</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>tensorflow.keras.backend</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>tensorflow.keras.layers</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>types</td><td></td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>warnings</td><td></td>\n",
       "\t</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_loaded_modules().HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dataset\n",
    "\n",
    "After downloading, untarring and looking at the data, let's do a visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ee1b7b2648>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee26887948>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ee26887b08>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee26948188>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ee26948708>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee26980208>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ee26980b88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee269b8888>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ee269bf8c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee269f1d88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ee269f1f08>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee26a29e48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ee26a34508>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee26825f48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ee26a6f788>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee26aa3c08>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ee26aa9c48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee26aa3788>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the first 9 images in the planet dataset\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.image import imread\n",
    "# define location of dataset\n",
    "folder = root_location+os.sep+'train-jpg/'\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    # define filename\n",
    "    filename = folder + 'train_' + str(i) + '.jpg'\n",
    "    # load image pixels\n",
    "    image = imread(filename)\n",
    "    # plot raw pixel data\n",
    "    pyplot.imshow(image)\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Mappings\n",
    "\n",
    "The next step involves understanding the labels that may be assigned to each image.\n",
    "\n",
    "Running the example first summarizes the shape of the training dataset. We can see that there are indeed 40,479 training images known to the mapping file.\n",
    "\n",
    "Next, the first 10 rows of the file are summarized. We can see that the second column of the file contains a space-separated list of tags to assign to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-05-04 13:29:07 Central Daylight Time]   DEBUG: (40479, 2) \n",
      "[2021-05-04 13:29:07 Central Daylight Time]   DEBUG:   image_name                                         tags\n",
      "0    train_0                                 haze primary\n",
      "1    train_1              agriculture clear primary water\n",
      "2    train_2                                clear primary\n",
      "3    train_3                                clear primary\n",
      "4    train_4    agriculture clear habitation primary road\n",
      "5    train_5                           haze primary water\n",
      "6    train_6  agriculture clear cultivation primary water\n",
      "7    train_7                                 haze primary\n",
      "8    train_8        agriculture clear cultivation primary\n",
      "9    train_9   agriculture clear cultivation primary road \n"
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "# load and summarize the mapping file for the planet dataset\n",
    "#####################################################################################\n",
    "\n",
    "#####################################################################################\n",
    "# Use Pandas to read the csv\n",
    "#####################################################################################\n",
    "\n",
    "filename = root_location+os.sep+'train_v2.csv' + os.sep + 'train_v2.csv'\n",
    "mapping_csv = read_csv(filename)\n",
    "\n",
    "#####################################################################################\n",
    "# Summarize the properties\n",
    "#####################################################################################\n",
    "debug.msg_debug(str(mapping_csv.shape))\n",
    "debug.msg_debug((mapping_csv[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the set of all known tags to be assigned to images, as well as a unique and consistent integer to apply to each tag. This is so that we can develop a target vector for each image with a one hot encoding, e.g. a vector with all zeros and a one at the index for each tag applied to the image.\n",
    "\n",
    "This can be achieved by looping through each row in the “tags” column, splitting the tags by space, and storing them in a set. We will then have a set of all known tags. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture',\n",
       " 'artisinal_mine',\n",
       " 'bare_ground',\n",
       " 'blooming',\n",
       " 'blow_down',\n",
       " 'clear',\n",
       " 'cloudy',\n",
       " 'conventional_mine',\n",
       " 'cultivation',\n",
       " 'habitation',\n",
       " 'haze',\n",
       " 'partly_cloudy',\n",
       " 'primary',\n",
       " 'road',\n",
       " 'selective_logging',\n",
       " 'slash_burn',\n",
       " 'water'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a set of labels\n",
    "labels = set()\n",
    "for i in range(len(mapping_csv)):\n",
    "    # convert spaced separated tags into an array of tags\n",
    "    tags = mapping_csv['tags'][i].split(' ')\n",
    "    # add tags to the set of known labels\n",
    "    labels.update(tags)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can then be ordered alphabetically and each tag assigned an integer based on this alphabetic rank.\n",
    "\n",
    "This will mean that the same tag will always be assigned the same integer for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agriculture',\n",
       " 'artisinal_mine',\n",
       " 'bare_ground',\n",
       " 'blooming',\n",
       " 'blow_down',\n",
       " 'clear',\n",
       " 'cloudy',\n",
       " 'conventional_mine',\n",
       " 'cultivation',\n",
       " 'habitation',\n",
       " 'haze',\n",
       " 'partly_cloudy',\n",
       " 'primary',\n",
       " 'road',\n",
       " 'selective_logging',\n",
       " 'slash_burn',\n",
       " 'water']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert set of labels to a list to list\n",
    "labels = list(labels)\n",
    "# order set alphabetically\n",
    "labels.sort()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a dictionary that maps tags to integers so that we can encode the training dataset for modeling.\n",
    "\n",
    "We can also create a dictionary with the reverse mapping from integers to string tag values, so later when the model makes a prediction, we can turn it into something readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict that maps labels to integers, and the reverse\n",
    "labels_map = {labels[i]:i for i in range(len(labels))}\n",
    "inv_labels_map = {i:labels[i] for i in range(len(labels))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tie all of this together into a convenience function called create_tag_mapping() that will take the loaded DataFrame containing the train_v2.csv data and return a mapping and inverse mapping dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# create a mapping of tags to integers given the loaded mapping file\n",
    "#####################################################################################\n",
    "def create_tag_mapping(mapping_csv):\n",
    "    # create a set of all known tags\n",
    "    labels = set()\n",
    "    for i in range(len(mapping_csv)):\n",
    "        # convert spaced separated tags into an array of tags\n",
    "        tags = mapping_csv['tags'][i].split(' ')\n",
    "        # add tags to the set of known labels\n",
    "        labels.update(tags)\n",
    "    # convert set of labels to a list to list\n",
    "    labels = list(labels)\n",
    "    # order set alphabetically\n",
    "    labels.sort()\n",
    "    # dict that maps labels to integers, and the reverse\n",
    "    labels_map = {labels[i]:i for i in range(len(labels))}\n",
    "    inv_labels_map = {i:labels[i] for i in range(len(labels))}\n",
    "    return labels_map, inv_labels_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a mapping of training set filenames to the tags for the image.\n",
    "\n",
    "This is a simple dictionary with the filename of the image as the key and the list of tags as the value.\n",
    "\n",
    "The create_file_mapping() below implements this, also taking the loaded DataFrame as an argument and returning the mapping with the tag value for each filename stored as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# create a mapping of filename to tags\n",
    "#####################################################################################\n",
    "def create_file_mapping(mapping_csv):\n",
    "    mapping = dict()\n",
    "    for i in range(len(mapping_csv)):\n",
    "        name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i]\n",
    "        mapping[name] = tags.split(' ')\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# create a one hot encoding for one list of tags\n",
    "#####################################################################################\n",
    "def one_hot_encode(tags, mapping):\n",
    "    # create empty vector\n",
    "    encoding = zeros(len(mapping), dtype='uint8')\n",
    "    # mark 1 for each tag in the vector\n",
    "    for tag in tags:\n",
    "        encoding[mapping[tag]] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create In-Memory Dataset\n",
    "\n",
    "We need to be able to load the JPEG images into memory.\n",
    "\n",
    "This can be achieved by enumerating all files in the train-jpg/ folder. Keras provides a simple API to load an image from file via the load_img() function and to cover it to a NumPy array via the img_to_array() function.\n",
    "\n",
    "As part of loading an image, we can force the size to be smaller to save memory and speed up training. In this case, we will halve the size of the image from 256×256 to 128×128. We will also store the pixel values as an unsigned 8-bit integer (e.g. values between 0 and 255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# keras convenience routine to load images\n",
    "#####################################################################################\n",
    "def load_dataset(path, file_mapping, tag_mapping):\n",
    "    photos, targets = list(), list()\n",
    "    # enumerate files in the directory\n",
    "    for filename in listdir(folder):\n",
    "        # load image, resize to more memory conservative approach\n",
    "        photo = load_img(path + filename, target_size=(128,128))\n",
    "        # convert to numpy array\n",
    "        photo = img_to_array(photo, dtype='uint8')\n",
    "        # get tags\n",
    "        tags = file_mapping[filename[:-4]]\n",
    "        # one hot encode tags\n",
    "        target = one_hot_encode(tags, tag_mapping)\n",
    "        # store\n",
    "        photos.append(photo)\n",
    "        targets.append(target)\n",
    "    X = asarray(photos, dtype='uint8')\n",
    "    y = asarray(targets, dtype='uint8')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save both arrays to one file in compressed format\n",
    "#savez_compressed('planet_data.npz', X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the function after re-reading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Use Pandas to read the csv\n",
    "#####################################################################################\n",
    "filename = root_location+os.sep+'train_v2.csv' + os.sep + 'train_v2.csv'\n",
    "mapping_csv = read_csv(filename)\n",
    "\n",
    "#####################################################################################\n",
    "# create a mapping of tags to integers\n",
    "#####################################################################################\n",
    "tag_mapping, _ = create_tag_mapping(mapping_csv)\n",
    "\n",
    "#####################################################################################\n",
    "# create a mapping of filenames to tag lists\n",
    "#####################################################################################\n",
    "file_mapping = create_file_mapping(mapping_csv)\n",
    "\n",
    "#####################################################################################\n",
    "# load the jpeg images\n",
    "#####################################################################################\n",
    "folder = root_location+os.sep+'train-jpg/'\n",
    "X, y = load_dataset(folder, file_mapping, tag_mapping)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "#####################################################################################\n",
    "# save both arrays to one file in compressed format\n",
    "#####################################################################################\n",
    "savez_compressed('planet_data.npz', X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Measure\n",
    "\n",
    "Before we start modeling, we must select a performance metric.\n",
    "\n",
    "Classification accuracy is often appropriate for binary classification tasks with a balanced number of examples in each class.\n",
    "\n",
    "In this case, we are working neither with a binary or multi-class classification task; instead, it is a multi-label classification task and the number of labels are not balanced, with some used more heavily than others.\n",
    "\n",
    "As such, the Kaggle competition organizes chose the F-beta metric, specifically the F2 score. This is a metric that is related to the F1 score (also called F-measure).\n",
    "\n",
    "The F1 score calculates the average of the recall and the precision. You may remember that the precision and recall are calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-a392e680a6dd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-a392e680a6dd>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    precision = true positives / (true positives + false positives)\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "precision = true positives / (true positives + false positives)\n",
    "recall = true positives / (true positives + false negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision describes how good a model is at predicting the positive class. Recall describes how good the model is at predicting the positive class when the actual outcome is positive.\n",
    "\n",
    "The F1 is the mean of these two scores, specifically the harmonic mean instead of the arithmetic mean because the values are proportions. F1 is preferred over accuracy when evaluating the performance of a model on an imbalanced dataset, with a value between 0 and 1 for worst and best possible scores.\n",
    "\n",
    "The F-beta metric is a generalization of F1 that allows a term called beta to be introduced that weights how important recall is compared to precision when calculating the mean.\n",
    "\n",
    "A common value of beta is two, and this was the value used in the competition, where recall valued twice as highly as precision. This is often referred to as the F2 score.\n",
    "\n",
    "The idea of a positive and negative class only makes sense for a binary classification problem. As we are predicting multiple classes, the idea of positive and negative and related terms are calculated for each class in a one vs. rest manner, then averaged across each class.\n",
    "\n",
    "The scikit-learn library provides an implementation of F-beta via the fbeta_score() function. We can call this function to evaluate a set of predictions and specify a beta value of 2 and the “average” argument set to “samples“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = 2 x (precision x recall) / (precision + recall)\n",
    "\n",
    "F-Beta = (1 + Beta^2) x (precision x recall) / (Beta^2 x precision + recall)\n",
    "\n",
    "score = fbeta_score(y_true, y_pred, 2, average='samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can test this on our prepared dataset.\n",
    "\n",
    "We can split our loaded dataset into separate train and test datasets that we can use to train and evaluate models on this problem. This can be achieved using the train_test_split() and specifying a ‘random_state‘ argument so that the same data split is given each time the code is run.\n",
    "\n",
    "We will use 70% for the training set and 30% for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load_dataset() function below implements this by loading the saved dataset, splitting it into train and test components, and returning them ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Load prebuilt/prepped/saved dataset from original Kaggle dataset and split into test/train sets\n",
    "#####################################################################################\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    data = load('planet_data.npz')\n",
    "    X, y = data['arr_0'], data['arr_1']\n",
    "    # separate into train and test datasets\n",
    "    trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then make a prediction of all classes or all 1 values in the one hot encoded vectors.\n",
    "\n",
    "The predictions can then be evaluated using the scikit-learn fbeta_score() function with the true values in the train and test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "trainX, trainY, testX, testY = load_dataset()\n",
    "\n",
    "# make all one predictions\n",
    "train_yhat = asarray([ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
    "test_yhat = asarray([ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
    "\n",
    "# evaluate predictions\n",
    "train_score = fbeta_score(trainY, train_yhat, 2, average='samples')\n",
    "test_score = fbeta_score(testY, test_yhat, 2, average='samples')\n",
    "debug.msg_debug(str('All Ones: train=%.3f, test=%.3f' % (train_score, test_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# calculate fbeta score for multi-class/label classification\n",
    "#####################################################################################\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    # clip predictions\n",
    "    y_pred = backend.clip(y_pred, 0, 1)\n",
    "    \n",
    "    # calculate elements\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "    \n",
    "    # calculate precision\n",
    "    p = tp / (tp + fp + backend.epsilon())\n",
    "    \n",
    "    # calculate recall\n",
    "    r = tp / (tp + fn + backend.epsilon())\n",
    "    \n",
    "    # calculate fbeta, averaged across each class\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "trainX, trainY, testX, testY = load_dataset()\n",
    "\n",
    "# make all one predictions\n",
    "train_yhat = asarray([ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
    "test_yhat = asarray([ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
    "\n",
    "# evaluate predictions with sklearn\n",
    "train_score = fbeta_score(trainY, train_yhat, 2, average='samples')\n",
    "test_score = fbeta_score(testY, test_yhat, 2, average='samples')\n",
    "\n",
    "debug.msg_debug(str('All Ones (sklearn): train=%.3f, test=%.3f' % (train_score, test_score)))\n",
    "\n",
    "# evaluate predictions with keras\n",
    "train_score = fbeta(backend.variable(trainY), backend.variable(train_yhat))\n",
    "test_score = fbeta(backend.variable(testY), backend.variable(test_yhat))\n",
    "debug.msg_debug(str('All Ones (keras): train=%.3f, test=%.3f' % (train_score, test_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the final pooling layer will be flattened and fed to a fully connected layer for interpretation then finally to an output layer for prediction.\n",
    "\n",
    "The model must produce a 17-element vector with a prediction between 0 and 1 for each output class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Evaluate a Baseline Model\n",
    "\n",
    "We are now ready to develop and evaluate a baseline convolutional neural network model for the prepared planet dataset.\n",
    "\n",
    "We will design a baseline model with a VGG-type structure. That is blocks of convolutional layers with small 3×3 filters followed by a max pooling layer, with this pattern repeating with a doubling in the number of filters with each block added.\n",
    "\n",
    "Specifically, each block will have two convolutional layers with 3×3 filters, ReLU activation and He weight initialization with same padding, ensuring the output feature maps have the same width and height. These will be followed by a max pooling layer with a 3×3 kernel. Three of these blocks will be used with 32, 64 and 128 filters respectively.\n",
    "\n",
    "The output of the final pooling layer will be flattened and fed to a fully connected layer for interpretation then finally to an output layer for prediction.\n",
    "\n",
    "The model must produce a 17-element vector with a prediction between 0 and 1 for each output class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this were a multi-class classification problem, we would use a softmax activation function and the categorical cross entropy loss function. This would not be appropriate for multi-label classification, as we expect the model to output multiple 1 values, not a single 1 value. In this case, we will use the sigmoid activation function in the output layer and optimize the binary cross entropy loss function.\n",
    "\n",
    "The model will be optimized with mini-batch stochastic gradient descent with a conservative learning rate of 0.01 and a momentum of 0.9, and the model will keep track of the “fbeta” metric during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model(in_shape=(128, 128, 3), out_shape=17):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(out_shape, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values will be normalized before fitting the model. We will achieve this by defining an ImageDataGenerator instance and specify the rescale argument as 1.0/255.0. This will normalize pixel values per batch to 32-bit floating point values, which might be more memory efficient than rescaling all of the pixel values at once in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create iterators from this data generator for both the train and test sets, and in this case, we will use the relatively large batch size of 128 images to accelerate learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare iterators\n",
    "train_it = datagen.flow(trainX, trainY, batch_size=128)\n",
    "test_it = datagen.flow(testX, testY, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defined model can then be fit using the train iterator, and the test iterator can be used to evaluate the test dataset at the end of each epoch. The model will be fit for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "    validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fit, we can calculate the final loss and F-beta scores on the test dataset to estimate the skill of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
    "print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit_generator() function called to fit the model returns a dictionary containing the loss and F-beta scores recorded each epoch on the train and test dataset. We can create a plot of these traces that can provide insight into the learning dynamics of the model.\n",
    "\n",
    "The summarize_diagnostics() function will create a figure from this recorded history data with one plot showing loss and another the F-beta scores for the model at the end of each training epoch on the train dataset (blue lines) and test dataset (orange lines).\n",
    "\n",
    "The created figure is saved to a PNG file with the same filename as the script with a “_plot.png” extension. This allows the same test harness to be used with multiple different script files for different model configurations, saving the learning curves in separate files along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Fbeta')\n",
    "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
    "\t# save plot to file\n",
    "\tfilename = sys.argv[0].split('/')[-1]\n",
    "\tpyplot.savefig(filename + '_plot.png')\n",
    "\tpyplot.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tie this together and define a function run_test_harness() to drive the test harness, including the loading and preparation of the data as well as definition, fit, and evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\ttrainX, trainY, testX, testY = load_dataset()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\t# prepare iterators\n",
    "\ttrain_it = datagen.flow(trainX, trainY, batch_size=128)\n",
    "\ttest_it = datagen.flow(testX, testY, batch_size=128)\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# fit model\n",
    "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0)\n",
    "\t# evaluate model\n",
    "\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
    "\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Improve Model Performance\n",
    "\n",
    "In the previous section, we defined a baseline model that can be used as the basis for improvement on the planet dataset.\n",
    "\n",
    "The model achieved a reasonable F-beta score, although the learning curves suggested that the model had overfit the training dataset. Two common approaches to explore to address overfitting are dropout regularization and data augmentation. Both have the effect of disrupting and slowing down the learning process, specifically the rate that the model improves over training epochs.\n",
    "\n",
    "We will explore both of these methods in this section. Given that we expect the rate of learning to be slowed, we give the model more time to learn by increasing the number of training epochs from 50 to 200.\n",
    "\n",
    "##Dropout Regularization\n",
    "\n",
    "Dropout regularization is a computationally cheap way to regularize a deep neural network.\n",
    "\n",
    "Dropout works by probabilistically removing, or “dropping out,” inputs to a layer, which may be input variables in the data sample or activations from a previous layer. It has the effect of simulating a large number of networks with very different network structure and, in turn, making nodes in the network generally more robust to the inputs.\n",
    "\n",
    "For more information on dropout, see the post:\n",
    "\n",
    "    How to Reduce Overfitting With Dropout Regularization in Keras\n",
    "\n",
    "Typically, a small amount of dropout can be applied after each VGG block, with more dropout applied to the fully connected layers near the output layer of the model.\n",
    "\n",
    "Below is the define_model() function for an updated version of the baseline model with the addition of Dropout. In this case, a dropout of 20% is applied after each VGG block, with a larger dropout rate of 50% applied after the fully connected layer in the classifier part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full example with drop-out defined\n",
    "# baseline model with dropout on the planet dataset\n",
    "import sys\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\tdata = load('planet_data.npz')\n",
    "\tX, y = data['arr_0'], data['arr_1']\n",
    "\t# separate into train and test datasets\n",
    "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "\treturn trainX, trainY, testX, testY\n",
    "\n",
    "# calculate fbeta score for multi-class/label classification\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "\t# clip predictions\n",
    "\ty_pred = backend.clip(y_pred, 0, 1)\n",
    "\t# calculate elements\n",
    "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "\t# calculate precision\n",
    "\tp = tp / (tp + fp + backend.epsilon())\n",
    "\t# calculate recall\n",
    "\tr = tp / (tp + fn + backend.epsilon())\n",
    "\t# calculate fbeta, averaged across each class\n",
    "\tbb = beta ** 2\n",
    "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "\treturn fbeta_score\n",
    "\n",
    "# define cnn model\n",
    "def define_model(in_shape=(128, 128, 3), out_shape=17):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(out_shape, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
    "\treturn model\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Fbeta')\n",
    "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
    "\t# save plot to file\n",
    "\tfilename = sys.argv[0].split('/')[-1]\n",
    "\tpyplot.savefig(filename + '_plot.png')\n",
    "\tpyplot.close()\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\ttrainX, trainY, testX, testY = load_dataset()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\t# prepare iterators\n",
    "\ttrain_it = datagen.flow(trainX, trainY, batch_size=128)\n",
    "\ttest_it = datagen.flow(testX, testY, batch_size=128)\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# fit model\n",
    "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=200, verbose=0)\n",
    "\t# evaluate model\n",
    "\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
    "\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data Augmentation\n",
    "\n",
    "Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n",
    "\n",
    "Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\n",
    "\n",
    "Data augmentation can also act as a regularization technique, adding noise to the training data and encouraging the model to learn the same features, invariant to their position in the input.\n",
    "\n",
    "Small changes to the input photos of the satellite photos might be useful for this problem, such as horizontal flips, vertical flips, rotations, zooms, and perhaps more. These augmentations can be specified as arguments to the ImageDataGenerator instance, used for the training dataset. The augmentations should not be used for the test dataset, as we wish to evaluate the performance of the model on the unmodified photographs.\n",
    "\n",
    "This requires that we have a separate ImageDataGenerator instance for the train and test dataset, then iterators for the train and test sets created from the respective data generators. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# prepare iterators\n",
    "train_it = train_datagen.flow(trainX, trainY, batch_size=128)\n",
    "test_it = test_datagen.flow(testX, testY, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, photos in the training dataset will be augmented with random horizontal and vertical flips as well as random rotations of up to 90 degrees. Photos in both the train and test steps will have their pixel values scaled in the same way as we did for the baseline model.\n",
    "\n",
    "The full code listing of the baseline model with training data augmentation for the planet dataset is listed below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete example with image generators included\n",
    "# baseline model with data augmentation for the planet dataset\n",
    "import sys\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\tdata = load('planet_data.npz')\n",
    "\tX, y = data['arr_0'], data['arr_1']\n",
    "\t# separate into train and test datasets\n",
    "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "\treturn trainX, trainY, testX, testY\n",
    "\n",
    "# calculate fbeta score for multi-class/label classification\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "\t# clip predictions\n",
    "\ty_pred = backend.clip(y_pred, 0, 1)\n",
    "\t# calculate elements\n",
    "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "\t# calculate precision\n",
    "\tp = tp / (tp + fp + backend.epsilon())\n",
    "\t# calculate recall\n",
    "\tr = tp / (tp + fn + backend.epsilon())\n",
    "\t# calculate fbeta, averaged across each class\n",
    "\tbb = beta ** 2\n",
    "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "\treturn fbeta_score\n",
    "\n",
    "# define cnn model\n",
    "def define_model(in_shape=(128, 128, 3), out_shape=17):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(out_shape, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
    "\treturn model\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Fbeta')\n",
    "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
    "\t# save plot to file\n",
    "\tfilename = sys.argv[0].split('/')[-1]\n",
    "\tpyplot.savefig(filename + '_plot.png')\n",
    "\tpyplot.close()\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\ttrainX, trainY, testX, testY = load_dataset()\n",
    "\t# create data generator\n",
    "\ttrain_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "\ttest_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\t# prepare iterators\n",
    "\ttrain_it = train_datagen.flow(trainX, trainY, batch_size=128)\n",
    "\ttest_it = test_datagen.flow(testX, testY, batch_size=128)\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# fit model\n",
    "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=200, verbose=0)\n",
    "\t# evaluate model\n",
    "\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
    "\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "We have explored two different improvements to the baseline model.\n",
    "\n",
    "The results can be summarized below, although we must assume some variance in these results given the stochastic nature of the algorithm:\n",
    "\n",
    "    Baseline + Dropout Regularization: 0.859\n",
    "    Baseline + Data Augmentation: 0.882\n",
    "\n",
    "As suspected, the addition of regularization techniques slows the progression of the learning algorithms and reduces overfitting, resulting in improved performance on the holdout dataset. It is likely that the combination of both approaches with a further increase in the number of training epochs will result in further improvements. That is, the combination of both dropout with data augmentation.\n",
    "\n",
    "This is just the beginning of the types of improvements that can be explored on this dataset. In addition to tweaks to the regularization methods described, other regularization methods could be explored such as weight decay and early stopping.\n",
    "\n",
    "It may be worth exploring changes to the learning algorithm, such as changes to the learning rate, use of a learning rate schedule, or an adaptive learning rate such as Adam.\n",
    "\n",
    "Alternate model architectures may also be worth exploring. The chosen baseline model is expected to offer more capacity than may be required for this problem and a smaller model may faster to train and in turn could result in better performance.\n",
    "\n",
    "## How to Use Transfer Learning\n",
    "\n",
    "Transfer learning involves using all or parts of a model trained on a related task.\n",
    "\n",
    "Keras provides a range of pre-trained models that can be loaded and used wholly or partially via the Keras Applications API.\n",
    "\n",
    "A useful model for transfer learning is one of the VGG models, such as VGG-16 with 16 layers that, at the time it was developed, achieved top results on the ImageNet photo classification challenge.\n",
    "\n",
    "The model is comprised of two main parts: the feature extractor part of the model that is made up of VGG blocks, and the classifier part of the model that is made up of fully connected layers and the output layer.\n",
    "\n",
    "We can use the feature extraction part of the model and add a new classifier part of the model that is tailored to the planets dataset. Specifically, we can hold the weights of all of the convolutional layers fixed during training and only train new fully connected layers that will learn to interpret the features extracted from the model and make a suite of binary classifications.\n",
    "\n",
    "This can be achieved by loading the VGG-16 model, removing the fully connected layers from the output-end of the model, then adding the new fully connected layers to interpret the model output and make a prediction. The classifier part of the model can be removed automatically by setting the “include_top” argument to “False“, which also requires that the shape of the input be specified for the model, in this case (128, 128, 3). This means that the loaded model ends at the last max pooling layer, after which we can manually add a Flatten layer and the new classifier fully-connected layers.\n",
    "\n",
    "See: https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/, section on Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that Keras has a number of pre-trained models to work with:\n",
    "\n",
    "https://keras.io/api/applications/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
