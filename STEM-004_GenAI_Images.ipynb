{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg3iJooMQjWA"
      },
      "source": [
        "# Artificial Intelligence\n",
        "## Image Generator\n",
        "\n",
        "OpenAI Dall-E (and others) are text-to-image models developed by OpenAI using deep learning methodologies to generate digital images from natural language descriptions, called \"prompts\".\n",
        "\n",
        "Reference:\n",
        "\n",
        "+ https://gemini.google.com/app\n",
        "+ https://openai.com/index/dall-e-2/\n",
        "+ https://cloud.google.com/vertex-ai/generative-ai/docs/image/img-gen-prompt-guide\n",
        "+ https://tech.co/news/use-google-bard-ai-image-generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "jljx58WFBpXo"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME     = \"ai-training-2024-08-09-bucket\"\n",
        "PROJECT_ID      = \"ai-training-2024-08-09\"\n",
        "LOCATION        = \"us-central1\"\n",
        "secret_name     = \"ai-training-key-secret\"\n",
        "secret_version  = \"latest\"\n",
        "project_id      = \"usfs-tf-admin\"\n",
        "resource_name   = f\"projects/{project_id}/secrets/{secret_name}/versions/{secret_version}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zramkw-P93C-"
      },
      "source": [
        "## Environment Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shY7a4DVQjWB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#- Google Colab Check\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib.util\n",
        "\n",
        "RunningInCOLAB = False\n",
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if RunningInCOLAB:\n",
        "    print(\"You are running this notebook in Google Colab.\")\n",
        "else:\n",
        "    print(\"You are running this notebook with Jupyter iPython runtime.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCP Gemini Setup"
      ],
      "metadata": {
        "id": "r5kZDFBsXcoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Google Vextex/AI Libraries\n",
        "subprocess.run([\"pip\", \"install\" , \"--upgrade\", \"google-cloud-aiplatform\", \"--quiet\"])\n",
        "\n",
        "\n",
        "libraries=[\"google-generativeai\", \"google-cloud-secret-manager\"]\n",
        "\n",
        "for library in libraries:\n",
        "    spec = importlib.util.find_spec(library)\n",
        "    if spec is None:\n",
        "      print(\"Installing library \" + library)\n",
        "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"])\n",
        "    else:\n",
        "      print(\"Library \" + library + \" already installed.\")\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "import vertexai.preview\n",
        "from google.cloud import secretmanager\n",
        "import vertexai\n",
        "import openai\n",
        "from google.auth import default, transport"
      ],
      "metadata": {
        "id": "jsDkV2olXfGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO_Hq5eq9joH"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJuXEPlkSo9p",
        "tags": []
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "# INCLUDES\n",
        "############################################\n",
        "#libraries specific to this example\n",
        "## Imports\n",
        "#import tensorflow as tf\n",
        "#from tensorflow import keras\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "#a set of libraries that perhaps should always be in Python source\n",
        "import os\n",
        "import datetime\n",
        "import sys\n",
        "import gc\n",
        "import getopt\n",
        "import inspect\n",
        "import math\n",
        "import warnings\n",
        "import textwrap\n",
        "import random\n",
        "import glob\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "#images\n",
        "import imageio\n",
        "import matplotlib as matplt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#data science\n",
        "import numpy as np\n",
        "\n",
        "#a darn useful library for creating paths and one I recommend you load to your environment\n",
        "from pathlib import Path\n",
        "\n",
        "from pydoc import help                          # can type in the python console `help(name of function)` to get the documentation\n",
        "\n",
        "warnings.filterwarnings('ignore')               # don't print out warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9gpU3zJ9l9H"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoQDWB9s9n7H",
        "tags": []
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "# GLOBAL VARIABLES\n",
        "############################################\n",
        "DEBUG = 1\n",
        "DEBUG_DATA = 0\n",
        "\n",
        "# CODE CONSTRAINTS\n",
        "VERSION_NAME    = \"MLGENIMG\"\n",
        "VERSION_MAJOR   = 0\n",
        "VERSION_MINOR   = 0\n",
        "VERSION_RELEASE = 1\n",
        "\n",
        "#used for values outside standard ASCII, just do it, you'll need it\n",
        "ENCODING  =\"utf-8\"\n",
        "\n",
        "############################################\n",
        "# GLOBAL CONSTANTS\n",
        "############################################\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "TEXT_WIDTH=77\n",
        "############################################\n",
        "# APPLICATION VARIABLES\n",
        "############################################\n",
        "start = \"\\033[1m\"\n",
        "end = \"\\033[0;0m\"\n",
        "\n",
        "############################################\n",
        "# GLOBAL CONFIGURATION\n",
        "############################################\n",
        "os.environ['PYTHONIOENCODING']=ENCODING\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FUa8QJT9tw_"
      },
      "source": [
        "## Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_CqUVLZ98Mz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Outputs library version history of effort.\n",
        "#\n",
        "def lib_diagnostics() -> None:\n",
        "\n",
        "    import pkg_resources\n",
        "\n",
        "    package_name_length=40\n",
        "    package_version_length=20\n",
        "\n",
        "    # Get installed packages\n",
        "    the_packages=[\"nltk\", \"numpy\", \"os\", \"pandas\"]\n",
        "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
        "    for package_idx, package_name in enumerate(installed):\n",
        "         if package_name in the_packages:\n",
        "             installed_version = installed[package_name]\n",
        "             print(f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"{'OpenAI version':<40}#: {str(openai.__version__):<20}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "\n",
        "    try:\n",
        "        print(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
        "        print(f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\")\n",
        "        print(f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        print(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
        "        print(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
        "        print(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
        "        print(f\"{'     current':<40}#: {torch.cuda.current_device()}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "      print(f\"{'GCP AI Platform version':<40}#: {str(aiplatform.__version__):<20}\")\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      print(f\"{'GCP Vertex version':<40}#: {str(vertexai.__version__):<20}\")\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      print(f\"{'Secret Manager version':<40}#: {str(secretmanager.__version__):<20}\")\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG0mFzUX-DV1"
      },
      "source": [
        "## Function Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSOOEwn8-FKg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "lib_diagnostics()\n",
        "wrapper = textwrap.TextWrapper(width=TEXT_WIDTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JMSTY2QjWD"
      },
      "source": [
        "## Variables and Model Parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################\n",
        "#- API Parameters for things like WordCloud\n",
        "#- Variables help hold information for later use\n",
        "#- The \"constants\" represent variables that we don't anticipate changing over the course of the program.\n",
        "###########################################\n",
        "#model parameters\n",
        "#changing the model can influence the type of response you get at the end.\n",
        "\n",
        "#AVAILABLE MODELS - https://firebase.google.com/docs/vertex-ai/gemini-models\n",
        "#Gemini 1.5 Flash\tgoogle/gemini-1.5-flash-001\n",
        "#Gemini 1.5 Prov\tgoogle/gemini-1.5-pro-001\n",
        "#Gemini 1.0 Prov\tgoogle/gemini-1.0-pro-002\n",
        "#                   google/gemini-1.0-pro-001\n",
        "#                   google/gemini-1.0-pro\n",
        "# select ai model type\n",
        "AI_MODEL_TYPE = \"imagegeneration@006\"\n",
        "\n",
        "model_temperature=0.7                      #start at 1 and decrease for more imaginitive responses\n",
        "model_max_tokens=8000                      #Gemini 1.5 ~ 1M, Gemini 1.0 ~ 16k\n",
        "model_max_token_response=2048              #Gemini 1.5 ~ 8K, Gemini 1.0 ~ 2048\n",
        "\n",
        "model_top_p=1                              #Top P specifies the cumulative probability score threshold that the tokens must reach.\n",
        "                                           # For example, if you set Top P to 0.6, then only the first two tokens, for and to, are sampled\n",
        "                                           # because their probabilities (0.4 and 0.25) add up to 0.65.\n",
        "\n",
        "model_top_k=1                              #Top-k sampling samples tokens with the highest probabilities until the specified number of\n",
        "                                           # tokens is reached. Top-p sampling samples tokens with the highest probability scores until\n",
        "                                           # the sum of the scores reaches the specified threshold value. (Top-p sampling is also called nucleus sampling.)\n",
        "\n",
        "summary_token_max=150\n",
        "\n"
      ],
      "metadata": {
        "id": "NtRDeZQ_Ysyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Filters"
      ],
      "metadata": {
        "id": "0ADGObWZZN3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the required libraries\n",
        "import vertexai\n",
        "from vertexai.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        ")\n",
        "\n",
        "# safety settings\n",
        "\n",
        "safety = [\n",
        "    SafetySetting(\n",
        "        category = HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category = HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category = HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category = HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "mYX18Vt9ZPrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvlYNerhGp7f"
      },
      "source": [
        "## Large Language Model (LLM) ~ Gemini Visual Model (Google)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVSfexHQG4pk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# initialize vertexai\n",
        "vertexai.init(project = PROJECT_ID, location = LOCATION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7J5-zMedkVL"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.vision_models import ImageGenerationModel\n",
        "\n",
        "# Create the model\n",
        "# See https://ai.google.dev/api/python/google/generativeai/GenerativeModel\n",
        "\n",
        "model = ImageGenerationModel.from_pretrained(AI_MODEL_TYPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pI0FSnAjNU3"
      },
      "source": [
        "## Create Your Picture Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMxl-xK1jAZn"
      },
      "outputs": [],
      "source": [
        "#options for creating an image\n",
        "style=[\"normal\", \"abstract\", \"surrealism\", \"cubism\", \"impressionism\", \"3d\", \"expressionist painting\", \"manga\", \"pop art\", \"pencil sketch\", \"watercolor\"]\n",
        "detail=[\"normal\", \"realistic\", \"ultrarealistic\", \"photorealistic\", \"photorealistic\", \"photorealistic\", \"photorealistic\", \"photorealistic\", \"photorealistic\", \"intricate\", \"highly detailed\"]\n",
        "position=[\"normal\", \"on the wall\", \"in the background\", \"in the foreground\", \"in the middle\", \"in the left\", \"in the right\"]\n",
        "mood=[\"normal\", \"stunning\", \"elegant\", \"radiant\", \"delicate\", \"cute\", \"striking\" \"glamorous\"]\n",
        "lights=[\"normal\", \"warm lighting\", \"natural lighting\", \"cold lighting\", \"dark aesthetic\", \"red tones\", \"blue tones\"]\n",
        "viewpoint=[\"portrait\", \"landscape\", \"close-up\", \"headshot\", \"mid-range\", \"3/4 shot\", \"full body shot\", \"wide shot\", \"low angle shot\", \"high angle shot\"]\n",
        "subject=[\"penguin\", \"bear\", \"tree\", \"dog\", \"cat\"]\n",
        "verb=[\"running\", \"surfing\", \"on the computer\", \"sleeping\", \"eating\", \"playing\", \"dancing\", \"sitting\", \"standing\", \"walking\"]\n",
        "\n",
        "\n",
        "\n",
        "#generate a picture from random choices above\n",
        "picture_prompt=f\"{style[random.randint(0, len(style)-1)]} \\\n",
        " {detail[random.randint(0, len(detail)-1)]} \\\n",
        " {position[random.randint(0, len(position)-1)]} \\\n",
        " {mood[random.randint(0, len(mood)-1)]} \\\n",
        " {lights[random.randint(0, len(lights)-1)]} \\\n",
        " {viewpoint[random.randint(0, len(viewpoint)-1)]}\\\n",
        " {subject[random.randint(0, len(subject)-1)]} \\\n",
        " {verb[random.randint(0, len(verb)-1)]}\"\n",
        "\n",
        "#examples\n",
        "#picture_prompt=\"magazine style, 4k, photorealistic, modern red armchair, natural\"\n",
        "#picture_prompt=\"oil painting, 4k, happy bear surfing\"\n",
        "\n",
        "\n",
        "#format the output to make it legible\n",
        "#string = wrapper.fill(text=picture_prompt)\n",
        "print(f\"Random text created: {picture_prompt}\")\n",
        "\n",
        "#############################################################\n",
        "# REPLACE picture_prompt with your own\n",
        "# input if you want the image created to change\n",
        "#############################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3_zbPFkjUHy"
      },
      "source": [
        "## Execute the Image Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLguZitVjWAu"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "try:\n",
        "  images = model.generate_images(\n",
        "      prompt=picture_prompt,\n",
        "\n",
        "      # Optional parameters\n",
        "      number_of_images=1,\n",
        "      language=\"en\",\n",
        "\n",
        "      # You can't use a seed value and watermark at the same time.\n",
        "      # add_watermark=False,\n",
        "      # seed=100,\n",
        "      aspect_ratio=\"1:1\",\n",
        "      safety_filter_level=\"block_some\",\n",
        "      person_generation=\"allow_adult\",\n",
        "  )\n",
        "\n",
        "  try:\n",
        "    output_file=\"./folderOnColab/output.png\"\n",
        "    images[0].save(location=output_file, include_generation_parameters=False)\n",
        "    print(f\"Your save location is {output_file}.\")\n",
        "    print(f\"For prompt: {picture_prompt}\")\n",
        "  except Exception as e:\n",
        "    print(\"There was a problem saving your image.\")\n",
        "    print(f\"{start}See exception:{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "    print(f\"Your save location was {output_file}.\")\n",
        "\n",
        "  # Optional. View the generated image in a notebook.\n",
        "  try:\n",
        "    #images[0].show(). #without modification to size (which is huge)\n",
        "    image = Image.open(output_file)\n",
        "    scale = 0.3\n",
        "    display(image.resize(( int(image.width * scale), int(image.height * scale))))\n",
        "\n",
        "    print(f\"Created output image using {len(images[0]._image_bytes)} bytes\")\n",
        "  except Exception as e:\n",
        "    print(\"There was a problem displaying your image.\")\n",
        "    print(f\"{start}See exception:{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"There was a problem creating your image.\")\n",
        "  print(\"\\n\")\n",
        "  print(f\"{start}See exception:{end} \")\n",
        "  string = wrapper.fill(text=str(e))\n",
        "  print(string)\n",
        "  print(\"\\n\")\n",
        "  print(f\"{start}Your prompt was:{end} \")\n",
        "  string = wrapper.fill(text=picture_prompt)\n",
        "  print(string)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Do you understand the stochastic nature of AI?\n",
        "\n",
        "Definition of stochastic: randomly determined; having a random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely.\n",
        "\n",
        "*** If this same prompt runs multiple times, do you expect to see the same picture every time....??? ***"
      ],
      "metadata": {
        "id": "tyh-6i-abGNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_times=3\n",
        "for index, value in enumerate(range(1,5)):\n",
        "\n",
        "  try:\n",
        "    images = model.generate_images(\n",
        "        prompt=picture_prompt,\n",
        "\n",
        "        # Optional parameters\n",
        "        number_of_images=1,\n",
        "        language=\"en\",\n",
        "\n",
        "        # You can't use a seed value and watermark at the same time.\n",
        "        # add_watermark=False,\n",
        "        # seed=100,\n",
        "        aspect_ratio=\"1:1\",\n",
        "        safety_filter_level=\"block_some\",\n",
        "        person_generation=\"allow_adult\",\n",
        "    )\n",
        "\n",
        "    try:\n",
        "      output_file=f\"./folderOnColab/output_{str(index)}.png\"\n",
        "      images[0].save(location=output_file, include_generation_parameters=False)\n",
        "      print(f\"Image Number: {str(index)}\")\n",
        "      print(f\"Your save location is {output_file}.\")\n",
        "      print(f\"For prompt: {picture_prompt}\")\n",
        "    except Exception as e:\n",
        "      print(\"There was a problem saving your image.\")\n",
        "      print(f\"{start}See exception:{end} \")\n",
        "      string = wrapper.fill(text=str(e))\n",
        "      print(string)\n",
        "      print(f\"Your save location was {output_file}.\")\n",
        "\n",
        "    # Optional. View the generated image in a notebook.\n",
        "    try:\n",
        "      #images[0].show(). #without modification to size (which is huge)\n",
        "      image = Image.open(output_file)\n",
        "      scale = 0.3\n",
        "      display(image.resize(( int(image.width * scale), int(image.height * scale))))\n",
        "\n",
        "      print(f\"Created output image using {len(images[0]._image_bytes)} bytes\")\n",
        "      print(\"\\n\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"There was a problem displaying your image.\")\n",
        "      print(f\"{start}See exception:{end} \")\n",
        "      string = wrapper.fill(text=str(e))\n",
        "      print(string)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"There was a problem creating your image.\")\n",
        "    print(\"\\n\")\n",
        "    print(f\"{start}See exception:{end} \")\n",
        "    string = wrapper.fill(text=str(e))\n",
        "    print(string)\n",
        "    print(\"\\n\")\n",
        "    print(f\"{start}Your prompt was:{end} \")\n",
        "    string = wrapper.fill(text=picture_prompt)\n",
        "    print(string)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dg7gDHTvbAf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A2gRtyRZb7-n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "STEM-004_GenAI_Images.ipynb",
      "provenance": [
        {
          "file_id": "https://github.com/christophergarthwood/jbooks/blob/main/STEM-001_WordClouds.ipynb",
          "timestamp": 1716214402332
        }
      ]
    },
    "environment": {
      "kernel": "conda-env-tensorflow-tensorflow",
      "name": "workbench-notebooks.m123",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
    },
    "kernelspec": {
      "display_name": "TensorFlow 2-11 (Local)",
      "language": "python",
      "name": "conda-env-tensorflow-tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}