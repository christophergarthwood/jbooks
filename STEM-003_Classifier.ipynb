{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg3iJooMQjWA"
      },
      "source": [
        "# Artificial Intelligence\n",
        "## Classifier\n",
        "### Clothing\n",
        "\n",
        "Basic classification: Classify images of clothing.\n",
        "\n",
        "Classification is the process of predicting the class of given data points. Classes are sometimes called as targets/ labels or categories. Classification predictive modeling is the task of approximating a mapping function (f) from input variables (X) to discrete output variables (y).\n",
        "\n",
        "Said another way...\n",
        "\n",
        "A classifier in machine learning is an algorithm that automatically orders or categorizes data into one or more of a set of “classes.” The process of categorizing or classifying information based on certain characteristics is known as classification.\n",
        "\n",
        "For example, spam detection in email service providers can be identified as a classification problem. This is s binary classification since there are only 2 classes as spam and not spam. A classifier utilizes some training data to understand how given input variables relate to the class. In this case, known spam and non-spam emails have to be used as the training data. When the classifier is trained accurately, it can be used to detect an unknown email.\n",
        "\n",
        "Classification belongs to the category of supervised learning where the targets also provided with the input data. There are many applications in classification in many domains such as in credit approval, medical diagnosis, target marketing etc.\n",
        "\n",
        "\n",
        "Types of Classifiers:\n",
        "+ Binary Classifiers: These are used when there are only two possible classes. For example, an email classifier might be designed to detect spam and non-spam emails.\n",
        "+ Multiclass Classifiers: These handle situations where there are more than two classes. For instance, a classifier that categorizes news articles into topics like sports, politics, and technology.\n",
        "+ Multilabel Classifiers: These can assign multiple labels to each instance. For example, a movie could be classified into multiple genres like comedy,drama, and action simultaneously.\n",
        "\n",
        "\n",
        "\n",
        "### Reference:\n",
        "+ https://www.tensorflow.org/tutorials/keras/classification\n",
        "+ https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "EvWPDwom_wCc"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME=\"ai-training-2024-08-09-bucket\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zramkw-P93C-"
      },
      "source": [
        "## Environment Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shY7a4DVQjWB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#- Google Colab Check\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "RunningInCOLAB = False\n",
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if RunningInCOLAB:\n",
        "    print(\"You are running this notebook in Google Colab.\")\n",
        "else:\n",
        "    print(\"You are running this notebook with Jupyter iPython runtime.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO_Hq5eq9joH"
      },
      "source": [
        "## Library Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJuXEPlkSo9p",
        "tags": []
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "# INCLUDES\n",
        "############################################\n",
        "#libraries specific to this example\n",
        "## Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib as matplt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "#a set of libraries that perhaps should always be in Python source\n",
        "import os\n",
        "import datetime\n",
        "import sys\n",
        "import gc\n",
        "import getopt\n",
        "import inspect\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "#a darn useful library for creating paths and one I recommend you load to your environment\n",
        "from pathlib import Path\n",
        "\n",
        "from pydoc import help                          # can type in the python console `help(name of function)` to get the documentation\n",
        "\n",
        "warnings.filterwarnings('ignore')               # don't print out warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9gpU3zJ9l9H"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoQDWB9s9n7H",
        "tags": []
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "# GLOBAL VARIABLES\n",
        "############################################\n",
        "DEBUG = 1\n",
        "DEBUG_DATA = 0\n",
        "\n",
        "# CODE CONSTRAINTS\n",
        "VERSION_NAME    = \"MLCLASSIFIER\"\n",
        "VERSION_MAJOR   = 0\n",
        "VERSION_MINOR   = 0\n",
        "VERSION_RELEASE = 1\n",
        "\n",
        "#used for values outside standard ASCII, just do it, you'll need it\n",
        "ENCODING  =\"utf-8\"\n",
        "\n",
        "############################################\n",
        "# GLOBAL CONSTANTS\n",
        "############################################\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "############################################\n",
        "# APPLICATION VARIABLES\n",
        "############################################\n",
        "\n",
        "############################################\n",
        "# GLOBAL CONFIGURATION\n",
        "############################################\n",
        "os.environ['PYTHONIOENCODING']=ENCODING\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FUa8QJT9tw_"
      },
      "source": [
        "## Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_CqUVLZ98Mz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Outputs library version history of effort.\n",
        "#\n",
        "def lib_diagnostics() -> None:\n",
        "\n",
        "    import pkg_resources\n",
        "\n",
        "    package_name_length=40\n",
        "    package_version_length=20\n",
        "\n",
        "    # Get installed packages\n",
        "    the_packages=[\"nltk\", \"numpy\", \"os\", \"pandas\"]\n",
        "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
        "    for package_idx, package_name in enumerate(installed):\n",
        "         if package_name in the_packages:\n",
        "             installed_version = installed[package_name]\n",
        "             print(f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
        "        print(f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\")\n",
        "        print(f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        print(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
        "        print(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
        "        print(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
        "        print(f\"{'     current':<40}#: {torch.cuda.current_device()}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "\n",
        "    try:\n",
        "      print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
        "    except Exception as e:\n",
        "      pass\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG0mFzUX-DV1"
      },
      "source": [
        "## Function Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSOOEwn8-FKg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "lib_diagnostics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JMSTY2QjWD"
      },
      "source": [
        "## Input Sources\n",
        "\n",
        "### Load and prepare the dataset\n",
        "\n",
        "You will use the MNIST dataset to obtain data.\n",
        "\n",
        "*Note*:\n",
        "MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.  Reference: https://en.wikipedia.org/wiki/MNIST_database\n",
        "\n",
        "This example uses the Fashion MNIST: https://github.com/zalandoresearch/fashion-mnist\n",
        "\n",
        "Fashion MNIST is intended as a drop-in replacement for the classic MNIST dataset—often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc.) in a format identical to that of the articles of clothing you'll use here.\n",
        "\n",
        "This guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n",
        "\n",
        "Here, 60,000 images are used to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVDRXgc1-UZo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#create reference to dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "#load the data and split into train/test datasets\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooY_0D7N-Yuo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Train variable details\n",
        "print(\"Your training data:\")\n",
        "print(\"{} shape is {}.\".format(\"Train images \", train_images.shape) )\n",
        "print(\"{} shape is {}.\".format(\"Train labels \", train_labels.shape) )\n",
        "print(\"{} sample structure is {}\".format(\"Train labels\", train_labels))\n",
        "print(\"These images and thier corresponding labels represent trusted data to train the model on.\")\n",
        "print(\"\")\n",
        "\n",
        "#Test variable details\n",
        "print(\"Your test data:\")\n",
        "print(\"{} shape is {}.\".format(\"Test images \", test_images.shape) )\n",
        "print(\"{} shape is {}.\".format(\"Test labels \", test_labels.shape) )\n",
        "print(\"{} sample structure is {}\".format(\"Test labels\", test_labels))\n",
        "print(\"These images and thier corresponding labels represent trusted data to test the model on.  How does the model know if it's performing well?\")\n",
        "print(\"By using the test data per iteration to check itself.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjHVk42F-lVq"
      },
      "source": [
        "### Dataset Definition\n",
        "\n",
        "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The labels are an array of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n",
        "\n",
        "Label \tClass\n",
        "\n",
        "+ 0 \tT-shirt/top\n",
        "+ 1 \tTrouser\n",
        "+ 2 \tPullover\n",
        "+ 3 \tDress\n",
        "+ 4 \tCoat\n",
        "+ 5 \tSandal\n",
        "+ 6 \tShirt\n",
        "+ 7 \tSneaker\n",
        "+ 8 \tBag\n",
        "+ 9 \tAnkle boot\n",
        "\n",
        "Each image is mapped to a single label. Since the class names are not included with the dataset, store them here to use later when plotting the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jTKLszX-o8A",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#associate \"human legible\" labels to the actual dataset\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaILuoePQNmj"
      },
      "source": [
        "## Example Image from Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCTjK9jR_F93",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#show the first image from the training dataset\n",
        "plt.figure()\n",
        "plt.imshow(train_images[1])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPMjfihG_CnA"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "The data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxuYBu_T_Mnp"
      },
      "source": [
        "Scale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It's important that the training set and the testing set be preprocessed in the same way.  Ultimately your inference / prediction of data will follow the same method of scaling.\n",
        "\n",
        "If you don't make your inference inputs align to the same method(s) you used during training, you cannot expect the same level of quality result obtained during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdBgQHkF_Roe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#scale the images between 0 - 1\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa3V9P3H_ZMH"
      },
      "source": [
        "## Show Sample Data\n",
        "Show the first 25 images from the training dataset and display the classification name with each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ogHRMro_cqp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#sneak peek of the data\n",
        "#Did you notice a difference in the output of the data?  What's the difference between the first image seen and these images?\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spqo9lXN_hKo"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Building the neural network requires configuring the layers of the model, then compiling the model.\n",
        "\n",
        "### Set up the layers\n",
        "\n",
        "The basic building block of a neural network is the layer. Layers extract representations from the data fed into them.\n",
        "\n",
        "Most of deep learning consists of chaining together simple layers. Most layers, such as tf.keras.layers.Dense, have parameters that are learned during training.\n",
        "\n",
        "Rectified linear unit (ReLU), is an activation function which means, this is how the neuron determines if it's going to fire (weighting of probabilities). See: https://en.wikipedia.org/wiki/Rectifier_(neural_networks) The short version is that ReLU is considered an excellent tool and using a vanishing gradient to determine probabilities always making sure any value less than zero is set to 0.\n",
        "\n",
        "Your output layer must equal the number of questions you're seeking to ask.  In this case we have ten (10) categories of clothing so we need 10 neurons to represent those different potential answers.\n",
        "\n",
        "Notice that the \"shape\" of the first layer corresponds to the shape of the input image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y61jBamv_kQu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#capture the function calls return into a variable, this encapsulate a complex object that defines the entire neural model\n",
        "model = keras.Sequential([\n",
        "    #one-D array are preferred for all processing\n",
        "    #The first layer in this network, tf.keras.layers.Flatten, transforms the format of the images from a two-dimensional array\n",
        "    #(of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels\n",
        "    #in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    #first layer is 128 neurons and a ReLU activation function.\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "\n",
        "    #output layer defined\n",
        "    keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWtT5s9z_oer"
      },
      "source": [
        "## Compile the model\n",
        "\n",
        "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
        "\n",
        "+ Loss function —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
        "\n",
        "+ Optimizer —This is how the model is updated based on the data it sees and its loss function.\n",
        "\n",
        "+ Metrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6gLEOLW_r8L",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              #v2.1 accepts this activation method but previous versions DO NOT\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3K60Ov0_w-C"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Training the neural network model requires the following steps:\n",
        "\n",
        "    1. Feed the training data to the model. In this example, the training data is in the train_images and train_labels arrays.\n",
        "    2. The model learns to associate images and labels.\n",
        "    3. You ask the model to make predictions about a test set—in this example, the test_images array.\n",
        "    4. Verify that the predictions match the labels from the test_labels array.\n",
        "\n",
        "## Train the model\n",
        "\n",
        "### Training the neural network model requires the following steps:\n",
        "\n",
        "    1. Feed the training data to the model. In this example, the training data is in the train_images and train_labels arrays.\n",
        "    2. The model learns to associate images and labels.\n",
        "    3. You ask the model to make predictions about a test set—in this example, the test_images array.\n",
        "    4. Verify that the predictions match the labels from the test_labels array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvRy77TQ_5rE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#given the images and labels begin learning from the train_* dataset for 10 \"iterations\".\n",
        "#note that you are ACTIVELY training a neural layer (AI) solution now.\n",
        "model.fit(train_images, train_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvscSmDkAARI"
      },
      "source": [
        "## Epochs\n",
        "\n",
        "Notice each epoc completion demonstrates an accuracy improvement.  You can add a *patience* function to drop out when your training reaches an optimal level to avoid overfitting.  Additional *drop out* functions are available as well.\n",
        "\n",
        "***Ask the instructor what other drop out functions are available.***\n",
        "\n",
        "## Evaluate accuracy\n",
        "\n",
        "Now we see how well the model performs.  The `evaluate()` function performs a prediction by taking the test_* inputs and and performing a diff between actual data and predicted data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpjzICjlABCl",
        "tags": []
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teG2v-OjAR-g"
      },
      "source": [
        "As previously discussed overfitting is a concern.  Notes from the demonstration code:\n",
        "\n",
        "It turns out that the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy represents overfitting. Overfitting happens when a machine learning model performs worse on new, previously unseen inputs than it does on the training data. An overfitted model \"memorizes\" the noise and details in the training dataset to a point where it negatively impacts the performance of the model on the new data. For more information, see the following:\n",
        "\n",
        "1.  Demonstration of overfitting - https://www.tensorflow.org/tutorials/keras/overfit_and_underfit#demonstrate_overfitting\n",
        "2.  Strategies to prevent overfitting - https://www.tensorflow.org/tutorials/keras/overfit_and_underfit#strategies_to_prevent_overfitting\n",
        "\n",
        "## Make Predictions\n",
        "\n",
        "With the model trained, you can use it to make predictions about some images. The model's linear outputs, logits. Attach a softmax layer to convert the logits to probabilities, which are easier to interpret.\n",
        "\n",
        "### Logits\n",
        "\n",
        "The vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function. If the model is solving a multi-class classification problem, logits typically become an input to the softmax function. The softmax function then generates a vector of (normalized) probabilities with one value for each possible class.\n",
        "\n",
        "In addition, logits sometimes refer to the element-wise inverse of the sigmoid function. For more information, see tf.nn.sigmoid_cross_entropy_with_logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1qIb9l0AS58",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#define another model designed to help identify classes\n",
        "probability_model = tf.keras.Sequential([model,\n",
        "                                         tf.keras.layers.Softmax()])\n",
        "\n",
        "#given the test images what labels have precipitated out?\n",
        "predictions = probability_model.predict(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmr5ALYlAYb5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#let's see what the predictions resulting data structure looks like on the inside.\n",
        "predictions\n",
        "\n",
        "#the resultant is an array of arrays [][]\n",
        "# each row represents each image predicted against\n",
        "#   each column in each row are the probabilities for each \"class\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tnwTd4AAfxh"
      },
      "source": [
        "#let's see what the predictions resulting data structure looks like on the inside.\n",
        "\n",
        "Predictions for the first input (image) submitted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL4lkSKPAggd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFIkYri6AiIK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#numpy magic, obtain the highest value of the array\n",
        "the_answer=np.argmax(predictions[0])\n",
        "\n",
        "#so for the first test image the most likely candidate for a class is element 9 in the array (remember, arrays go from 0..9)\n",
        "#what does element 9 in the classes array defined earlier represent?  what class?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Yle-wTgAoxg"
      },
      "source": [
        "So, the model is most confident that this image is an ankle boot, or class_names[9]. Examining the test label shows that this classification is correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IC1_sl3AmvW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"Label {} relates to a {}.\".format(the_answer,class_names[np.argmax(predictions[0])]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0Bj-6h7A41S"
      },
      "source": [
        "Graph this to look at the full set of 10 class predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MsE2JLBAt3r",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#functions defined to show the item and relevance of the prediction\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array, true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFx6OGloA-E6"
      },
      "source": [
        "Let's plot several images with their predictions. Note that the model can be wrong even when very confident."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnDI7RqwA7YR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Plot the first X test images, their predicted labels, and the true labels.\n",
        "# Color correct predictions in blue and incorrect predictions in red.\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, predictions[i], test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, predictions[i], test_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZPQ22zsBETp"
      },
      "source": [
        "## Use the trained model\n",
        "\n",
        "Use the trained model to make a prediction about a single image.  We will generate a random number selected out of the domain of values.\n",
        "\n",
        "### *Note*\n",
        "tf.keras models are optimized to make predictions on a batch, or collection, of examples at once. Accordingly, even though you're using a single image, you need to add it to a list:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAwXPO2XBA70",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# seed the pseudorandom number generator\n",
        "from random import seed\n",
        "from random import random\n",
        "\n",
        "# seed random number generator\n",
        "seed(1)\n",
        "\n",
        "#lowest array position\n",
        "min=0\n",
        "#highest array position (note that the returned shape is 1000x28x28)\n",
        "max=test_images.shape[0]-1\n",
        "\n",
        "#generate a random value\n",
        "value = random()\n",
        "\n",
        "#scale that value to the range we have available\n",
        "scaled_value = min + (value * (max - min))\n",
        "scaled_value = int(scaled_value)\n",
        "\n",
        "\n",
        "#take the an image from the test dataset, normally a 28x28 matrix\n",
        "img = test_images[scaled_value]\n",
        "\n",
        "# Add the image to a batch where it's the only member, now a 1x28x28 matrix.\n",
        "img = (np.expand_dims(img,0))\n",
        "\n",
        "#predict the correct label for this image\n",
        "predictions_single = probability_model.predict(img)\n",
        "\n",
        "print(predictions_single)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP78ehnQBGo4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "plot_value_array(1, predictions_single[0], test_labels)\n",
        "_ = plt.xticks(range(10), class_names, rotation=45)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-Qt8MuiBNOp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#keras.Model.predict returns a list of lists—one list for each image in the batch of data. Grab the predictions for our (only) image in the batch\n",
        "np.argmax(predictions_single[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhWaj6PqBRZu"
      },
      "source": [
        "### Which means..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj4t4EXaBPcc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"Prediction is label {} which is a {}.\".format(np.argmax(predictions_single[0]),class_names[np.argmax(predictions_single[0])]))\n",
        "print(\"Reality suggests label {} which is a {}.\".format(test_labels[scaled_value],class_names[test_labels[scaled_value]] ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJm3RPsLBURs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(1,1))\n",
        "plot_image(scaled_value,np.argmax(predictions_single[0]), test_labels, test_images)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHrSY6KLBVvM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#let's try another one\n",
        "#generate a random value\n",
        "value = random()\n",
        "\n",
        "#scale that value to the range we have available\n",
        "scaled_value = min + (value * (max - min))\n",
        "scaled_value = int(scaled_value)\n",
        "#take the an image from the test dataset, normally a 28x28 matrix\n",
        "img = test_images[scaled_value]\n",
        "\n",
        "# Add the image to a batch where it's the only member, now a 1x28x28 matrix.\n",
        "img = (np.expand_dims(img,0))\n",
        "#predict the correct label for this image\n",
        "predictions_single = probability_model.predict(img)\n",
        "\n",
        "print(predictions_single)\n",
        "plot_value_array(1, predictions_single[0], test_labels)\n",
        "_ = plt.xticks(range(10), class_names, rotation=45)\n",
        "np.argmax(predictions_single[0])\n",
        "print(\"Prediction is label {} which is a {}.\".format(np.argmax(predictions_single[0]),class_names[np.argmax(predictions_single[0])]))\n",
        "print(\"Reality suggests label {} which is a {}.\".format(test_labels[scaled_value],class_names[test_labels[scaled_value]] ))\n",
        "plt.figure(figsize=(1,1))\n",
        "plot_image(scaled_value,np.argmax(predictions_single[0]), test_labels, test_images)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "STEM-003_Classifier.ipynb",
      "provenance": [
        {
          "file_id": "https://github.com/christophergarthwood/jbooks/blob/main/STEM-001_WordClouds.ipynb",
          "timestamp": 1716214402332
        }
      ]
    },
    "environment": {
      "kernel": "conda-env-tensorflow-tensorflow",
      "name": "workbench-notebooks.m123",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
    },
    "kernelspec": {
      "display_name": "TensorFlow 2-11 (Local)",
      "language": "python",
      "name": "conda-env-tensorflow-tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}