{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jg3iJooMQjWA"
   },
   "source": [
    "# Artificial Intelligence Computer Vision & Classifier\n",
    "## Generative AI (GenAI) - 004\n",
    "### Harmful Species Identification (Spotted Lantern Fly vs. Asian Longhorn Beetle)\n",
    "\n",
    "<center>\n",
    "<table align=\"center\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/christophergarthwood/jbooks/blob/main/STEM-004_ComputerVision.ipynb\">\n",
    "      <img src=\"./img/GoogleColab-logo.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/notebooks?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Link to Colab Enterprise\n",
    "    </a>\n",
    "  </td>   \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/christophergarthwood/jbooks/blob/main/STEM-004_ComputerVision.ipynb\">\n",
    "      <img src=\"./img/GitHub-logo.jpg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Link to Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "</center>\n",
    "</br></br></br>\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Christopher G Wood](https://github.com/christophergarthwood)  |\n",
    "\n",
    "# Overview\n",
    "\n",
    "Using a Convolutional Neural Network (CNN) architecture we will read in images for two invasive species and train a model on that data to produce a saved neural layer / model.  We will reload that model and make inferences (predictions) against test data and measure accuracy with various methods.\n",
    "\n",
    "## What is a convolutional neural network (CNN)?\n",
    "\n",
    "A convolutional neural network (CNN) is a category of machine learning model, namely a type of deep learning algorithm well suited to analyzing visual data. CNNs -- sometimes referred to as convnets -- use principles from linear algebra, particularly convolution operations, to extract features and identify patterns within images. Although CNNs are predominantly used to process images, they can also be adapted to work with audio and other signal data.\n",
    "\n",
    "CNN architecture is inspired by the connectivity patterns of the human brain -- in particular, the visual cortex, which plays an essential role in perceiving and processing visual stimuli. The artificial neurons in a CNN are arranged to efficiently interpret visual information, enabling these models to process entire images. Because CNNs are so effective at identifying objects, they are frequently used for computer vision tasks such as image recognition and object detection, with common use cases including self-driving cars, facial recognition and medical image analysis.\n",
    "\n",
    "Unlike CNNs, older forms of neural networks often needed to process visual data in a piecemeal manner, using segmented or lower-resolution input images. A CNN's comprehensive approach to image recognition lets it outperform a traditional neural network on a range of image-related tasks and, to a lesser extent, speech and audio processing.\n",
    "\n",
    "### How do convolutional neural networks work?\n",
    "CNNs use a series of layers, each of which detects different features of an input image. Depending on the complexity of its intended purpose, a CNN can contain dozens, hundreds or even thousands of layers, each building on the outputs of previous layers to recognize detailed patterns.\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "\n",
    "+ https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats\n",
    "+ https://pythonawesome.com/an-animated-and-smart-progress-bar-for-python/\n",
    "+ https://www.tensorflow.org/tutorials/images/cnn\n",
    "+ https://www.geeksforgeeks.org/computer-vision/\n",
    "+ https://www.machinelearningnuggets.com/cnn-tensorflow/\n",
    "+ https://www.datacamp.com/tutorial/cnn-tensorflow-python\n",
    "+ https://pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/\n",
    "+ https://www.baeldung.com/cs/ml-loss-accuracy\n",
    "+ https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew\n",
    "+https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1737665372909,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "M3qlCehNBu-_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's define some variables (information holders) for our project overall\n",
    "\n",
    "global PROJECT_ID, BUCKET_NAME, LOCATION\n",
    "BUCKET_NAME =\"jbooks_ai_ml_public\"\n",
    "PROJECT_ID  =\"testproject-366516\"\n",
    "LOCATION    = \"us-central1\"\n",
    "\n",
    "BOLD_START=\"\\033[1m\"\n",
    "BOLD_END=\"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508,
     "referenced_widgets": [
      "1015b1ce380943d48c7a6386444c768a",
      "f5405303114146a28663cb1c46db7beb",
      "bdc74a6d20a041819b6fc6d1c3f85a25",
      "033cb5a8c21c4ca5a13b5d03e8b78fa0",
      "2b447186de7a4571821ca59295744ce3",
      "cf363ab273bf4eeba7fbea4d79ed3a64",
      "479233081fbc44c8afe9a044ebb95faf",
      "202c8798fdda4778bf09a2124c37a6c7",
      "ad3b9f4855d541e49303c03fb9f07db7",
      "0d8988d435644a928829ef71435a3e83",
      "ebb9ca01cf52473699f21f15c5ec7d34",
      "689fe090492947608518efc9f3bd6d5d",
      "dd3de71c792f4762af3b280ac9b7f5e6",
      "fc3fc48bdebe4501998819fbda36152c",
      "f196621910824a00a97d06e5433cb294",
      "31b012d7d41a4d06933aa38e8e3bd74a"
     ]
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1737665373417,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "3TrA1A5nIeCV",
    "outputId": "65ad0051-dd12-4511-e88f-62bf0f35300e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now create a means of enforcing project id selection\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def wait_for_button_press():\n",
    "\n",
    "    button_pressed = False\n",
    "\n",
    "    # Create widgets\n",
    "    html_widget = widgets.HTML(\n",
    "\n",
    "    value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "\n",
    "        <table><tr><td>\n",
    "            <span style=\"font-family: Tahoma;font-size: 18\">\n",
    "              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n",
    "              Please verify that you are in the appropriate project and that the:</br>\n",
    "              <center><code><b>PROJECT_ID</b></code> </br></center>\n",
    "              aligns with the Project Id in the upper left corner of this browser and that the location:\n",
    "              <center><code><b>LOCATION</b></code> </br></center>\n",
    "              aligns with the instructions provided.\n",
    "            </span>\n",
    "          </td></tr></table></br></br>\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    project_list=[\"ai-bootcamp\", \"ai-advanced-training\", \"I will setup my own\"]\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=project_list,\n",
    "        value=project_list[0],\n",
    "        description='Set Your Project:',\n",
    "    )\n",
    "\n",
    "    html_widget2 = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "          \"\"\")\n",
    "\n",
    "    button = widgets.Button(description=\"Accept\")\n",
    "\n",
    "    # Function to handle the selection change\n",
    "    def on_change(change):\n",
    "        global PROJECT_ID\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            #print(\"Selected option:\", change['new'])\n",
    "            PROJECT_ID=change['new']\n",
    "\n",
    "    # Observe the dropdown for changes\n",
    "    dropdown.observe(on_change)\n",
    "\n",
    "    def on_button_click(b):\n",
    "        nonlocal button_pressed\n",
    "        global PROJECT_ID\n",
    "        button_pressed = True\n",
    "        #button.disabled = True\n",
    "        button.close()  # Remove the button from display\n",
    "        with output:\n",
    "          #print(f\"Button pressed...continuing\")\n",
    "          #print(f\"Selected option: {dropdown.value}\")\n",
    "          PROJECT_ID=dropdown.value\n",
    "\n",
    "    button.on_click(on_button_click)\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Create centered layout\n",
    "    centered_layout = widgets.VBox([\n",
    "                                    html_widget,\n",
    "                                    widgets.HBox([dropdown, button]),\n",
    "                                    html_widget2,\n",
    "    ], layout=widgets.Layout(\n",
    "                              display='flex',\n",
    "                              flex_flow='column',\n",
    "                              align_items='center',\n",
    "                              width='100%'\n",
    "    ))\n",
    "    # Display the layout\n",
    "    display(centered_layout)\n",
    "\n",
    "\n",
    "wait_for_button_press()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zramkw-P93C-"
   },
   "source": [
    "## Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1737666212893,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "shY7a4DVQjWB",
    "outputId": "606fe1d2-f3de-47cf-f457-bb449cca5b52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#- Google Colab Check\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "import datetime\n",
    "\n",
    "RunningInCOLAB = False\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "current_time   = datetime.datetime.now()\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    print(f\"You are running this notebook in Google Colab at {current_time} in the {PROJECT_ID} lab.\")\n",
    "else:\n",
    "    print(f\"You are likely running this notebook with Jupyter iPython runtime at {current_time} in the {PROJECT_ID} lab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZVkISRuLURi"
   },
   "source": [
    "# Library Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1737665373426,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "logDyNfnLURj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import key libraries necessary to support dynamic installation of additional libraries\n",
    "import sys\n",
    "# Use subprocess to support running operating system commands from the program, using the \"bang\" (!)\n",
    "# symbology is supported, however that does not translate to an actual python script, this is a more\n",
    "# agnostic approach.\n",
    "import subprocess\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74882,
     "status": "ok",
     "timestamp": 1737665448288,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "ldXG-5fhsV1e",
    "outputId": "5941b5ee-93f4-41df-ccc2-8915e0e73ed7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the libraries you'd like to add to this Runtime environment.\n",
    "libraries=[\"backoff\", \"pathlib\", \"numpy\", \"Pillow\", \"pandas\",\n",
    "           \"python-dotenv\", \"seaborn\", \"rich\", \"rich[jupyter]\", \"piexif\", \"unidecode\",\n",
    "           \"watermark\", \"watermark[GPU]\",\"keras\", \"imageio\", \"pydot\",\n",
    "           ]\n",
    "\n",
    "# Loop through each library and test for existence, if not present install quietly\n",
    "for library in libraries:\n",
    "    if library == \"Pillow\":\n",
    "      spec = importlib.util.find_spec(\"PIL\")\n",
    "    else:\n",
    "      spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "      print(\"Installing library \" + library)\n",
    "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"], check=True)\n",
    "    else:\n",
    "      print(\"Library \" + library + \" already installed.\")\n",
    "      print(\"...attempting to upgrade if appropriate.\")\n",
    "      try:\n",
    "          subprocess.run([\"pip\", \"install\" , \"--upgrade\",  library,], check=True)\n",
    "      except Exception as e:\n",
    "        print(f\"ERROR: Problem detected while trying to upgrade {library}, ignoring.\")\n",
    "        print(f\"....{e}\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO_Hq5eq9joH"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5041,
     "status": "ok",
     "timestamp": 1737665453315,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "PJuXEPlkSo9p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#- Import additional libraries that add value to the project related to NLP\n",
    "\n",
    "# Beautiful Soup (BS4) is used to parse HTML documents.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Word cloud building library\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#- Set of libraries that perhaps should always be in Python source\n",
    "import backoff\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "import gc\n",
    "import getopt\n",
    "import glob\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "import socket\n",
    "import sys\n",
    "import textwrap\n",
    "import tqdm\n",
    "import traceback\n",
    "import warnings\n",
    "import time\n",
    "from time import perf_counter\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.traceback import install\n",
    "import locale\n",
    "\n",
    "#- Displays system info\n",
    "from watermark import watermark as the_watermark\n",
    "from py3nvml import py3nvml\n",
    "\n",
    "#- Additional libraries for this work\n",
    "import math\n",
    "from base64 import b64decode\n",
    "from IPython.display import Image, Markdown\n",
    "import pandas, IPython.display as display, io, jinja2, base64\n",
    "from IPython.display import clear_output #used to support real-time plotting\n",
    "import requests\n",
    "import unidecode\n",
    "import pydot\n",
    "\n",
    "#- Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#- Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.cbook import get_sample_data\n",
    "from matplotlib.offsetbox import (AnnotationBbox, DrawingArea, OffsetImage,\n",
    "                                  TextArea)\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.patches import Circle\n",
    "from PIL import Image as PIL_Image\n",
    "import PIL.ImageOps\n",
    "import matplotlib.image as mpimg\n",
    "from imageio import imread\n",
    "\n",
    "#- Image meta-data for Section 508 compliance\n",
    "import piexif\n",
    "from piexif.helper import UserComment\n",
    "\n",
    "#- Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tensorflow and related AI libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import keras\n",
    "#import tensorrt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# existing pre-trained model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#from keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import load_img\n",
    "#from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# progress bar\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Setup some basic timers material\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9gpU3zJ9l9H"
   },
   "source": [
    "## Application Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1737665453699,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "DoQDWB9s9n7H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Semantic Versioning\n",
    "VERSION_NAME    = \"MLCNN\"\n",
    "VERSION_MAJOR   = 0\n",
    "VERSION_MINOR   = 0\n",
    "VERSION_RELEASE = 1\n",
    "\n",
    "# API Parameters for things like WordCloud, variables help hold information for later use\n",
    "# The \"constants\" represent variables that we don't anticipate changing over the course of the program.\n",
    "IMG_BACKGROUND=\"black\"     #options are black, white, another color or None\n",
    "IMG_FONT_SIZE_MIN=10\n",
    "IMG_WIDTH=1024\n",
    "IMG_HEIGHT=768\n",
    "IMG_INTERP=\"bilinear\"\n",
    "IMG_ALPHA=0.8\n",
    "IMG_ASPECT=\"equal\"\n",
    "FIGURE_WIDTH=11\n",
    "FIGURE_HEIGHT=8.5\n",
    "WORD_FREQ=10\n",
    "\n",
    "# specify how image formats will be saved\n",
    "IMG_EXT=\".jpg\"\n",
    "\n",
    "# used to fully display the error stack, set to 1 if you want to see a ridiculous amount of debugging information\n",
    "DEBUG_STACKTRACE=0\n",
    "\n",
    "# location of our working files\n",
    "#WORKING_FOLDER=\"/content/folderOnColab\"\n",
    "WORKING_FOLDER=\"./folderOnColab\"\n",
    "\n",
    "# Notebook Author details\n",
    "AUTHOR_NAME=\"Christopher G Wood\"\n",
    "GITHUB_USERNAME=\"christophergarthwood\"\n",
    "AUTHOR_EMAIL=\"christopher.g.wood@gmail.com\"\n",
    "\n",
    "# GenAI\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "TEXT_WIDTH=77\n",
    "IMG_SCALE=0.75\n",
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Encoding\n",
    "ENCODING  =\"utf-8\"\n",
    "os.environ['PYTHONIOENCODING']=ENCODING\n",
    "\n",
    "\n",
    "# Artificial Intelligence Variables\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_DATA_DIR=f\"{WORKING_FOLDER}/ENTOMOLOGY/train\"\n",
    "VALIDATION_DATA_DIR=f\"{WORKING_FOLDER}/ENTOMOLOGY/validation\"\n",
    "VALIDATION_SPLIT=0.2\n",
    "IMG_HEIGHT=224\n",
    "IMG_WIDTH=224\n",
    "EPOCHS=50\n",
    "BOLD_START = \"\\033[1m\"\n",
    "BOLD_END = \"\\033[0;0m\"\n",
    "\n",
    "METRICS = metrics=['accuracy',\n",
    "                  \ttf.keras.metrics.Precision(name='precision'),\n",
    "                  \ttf.keras.metrics.Recall(name='recall')]\n",
    "\n",
    "#You can also adjust the verbosity by changing the value of TF_CPP_MIN_LOG_LEVEL:\n",
    "#\n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed\n",
    "TF_CPP_MIN_LOG_LEVEL_SETTING=0\n",
    "\n",
    "TEXT_WIDTH=77\n",
    "\n",
    "# Model variables\n",
    "categories=[\"ASIAN_LONGHORN_BEETLE\", \"SPOTTED_LANTERN_FLY\"]\n",
    "categories_short_name=[\"alb\", \"slf\"]\n",
    "range_max=5\n",
    "plot_max=330\n",
    "start = \"\\033[1m\"\n",
    "end = \"\\033[0;0m\"\n",
    "THE_DEVICE_NAME=\"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
    "\n",
    "# Set the Seed for the experiment (ask me why?)\n",
    "# seed the pseudorandom number generator\n",
    "# THIS IS ESSENTIAL FOR CONSISTENT MODEL OUTPUT, remember these are random in nature.\n",
    "SEED_INIT=7\n",
    "random.seed(SEED_INIT)\n",
    "tf.random.set_seed(SEED_INIT)\n",
    "np.random.seed(SEED_INIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FUa8QJT9tw_"
   },
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1737665453699,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "v_CqUVLZ98Mz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lib_diagnostics() -> None:\n",
    "\n",
    "    import pkg_resources\n",
    "\n",
    "    package_name_length=20\n",
    "    package_version_length=10\n",
    "\n",
    "    # Show notebook details\n",
    "    #%watermark?\n",
    "    #%watermark --github_username christophergwood --email christopher.g.wood@gmail.com --date --time --iso8601 --updated --python --conda --hostname --machine --githash --gitrepo --gitbranch --iversions --gpu\n",
    "    # Watermark\n",
    "    rprint(the_watermark(author=f\"{AUTHOR_NAME}\", github_username=f\"GITHUB_USERNAME\", email=f\"{AUTHOR_EMAIL}\",iso8601=True, datename=True, current_time=True, python=True, updated=True, hostname=True, machine=True, gitrepo=True, gitbranch=True, githash=True))\n",
    "\n",
    "\n",
    "    print(f\"{BOLD_START}Packages:{BOLD_END}\")\n",
    "    print(\"\")\n",
    "    # Get installed packages\n",
    "    the_packages=[\"nltk\", \"numpy\", \"os\", \"pandas\", \"keras\", \"seaborn\"]# Functions are like legos that do one thing, this function outputs library version history of effort.\n",
    "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "    for package_idx, package_name in enumerate(installed):\n",
    "         if package_name in the_packages:\n",
    "             installed_version = installed[package_name]\n",
    "             rprint(f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\")\n",
    "\n",
    "    try:\n",
    "        rprint(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
    "        rprint(f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\")\n",
    "        rprint(f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        rprint(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
    "        rprint(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
    "        rprint(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
    "        rprint(f\"{'     current':<40}#: {torch.cuda.current_device()}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "      print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
    "    except Exception as e:\n",
    "      pass\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1737665453948,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "Nl_kxpFUKKD5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Routines designed to support adding ALT text to an image generated through Matplotlib.\n",
    "\n",
    "def capture(figure):\n",
    "   buffer = io.BytesIO()\n",
    "   figure.savefig(buffer)\n",
    "   #return F\"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "   return F\"data:image/jpg;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "\n",
    "def make_accessible(figure, template, **kwargs):\n",
    "   return display.Markdown(F\"\"\"![]({capture(figure)} \"{template.render(**globals(), **kwargs)}\")\"\"\")\n",
    "\n",
    "\n",
    "# requires JPG's or TIFFs\n",
    "def add_alt_text(image_path, alt_text):\n",
    "    try:\n",
    "        if os.path.isfile(image_path):\n",
    "          img = PIL_Image.open(image_path)\n",
    "          if \"exif\" in img.info:\n",
    "              exif_dict = piexif.load(img.info[\"exif\"])\n",
    "          else:\n",
    "              exif_dict={}\n",
    "\n",
    "          w, h = img.size\n",
    "          if \"0th\" not in exif_dict:\n",
    "            exif_dict[\"0th\"]={}\n",
    "          exif_dict[\"0th\"][piexif.ImageIFD.XResolution] = (w, 1)\n",
    "          exif_dict[\"0th\"][piexif.ImageIFD.YResolution] = (h, 1)\n",
    "\n",
    "          software_version=\" \".join([\"STEM-001 with Python v\", str(sys.version).split(\" \")[0]])\n",
    "          exif_dict[\"0th\"][piexif.ImageIFD.Software]=software_version.encode(\"utf-8\")\n",
    "\n",
    "          if \"Exif\" not in exif_dict:\n",
    "            exif_dict[\"Exif\"]={}\n",
    "          exif_dict[\"Exif\"][piexif.ExifIFD.UserComment] = UserComment.dump(alt_text, encoding=\"unicode\")\n",
    "\n",
    "          exif_bytes = piexif.dump(exif_dict)\n",
    "          img.save(image_path, \"jpeg\", exif=exif_bytes)\n",
    "        else:\n",
    "          rprint(f\"Cound not fine {image_path} for ALT text modification, please check your paths.\")\n",
    "\n",
    "    except (FileExistsError, FileNotFoundError, Exception) as e:\n",
    "        process_exception(e)\n",
    "\n",
    "# Appears to solve a problem associated with GPU use on Colab, see: https://github.com/explosion/spaCy/issues/11909\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1737665453948,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "X8FdPtZzKMyw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this function displays the stack trace on errors from a central location making adjustments to the display on an error easier to manage\n",
    "# functions perform useful solutions for highly repetitive code\n",
    "def process_exception(inc_exception: Exception) -> None:\n",
    "  if DEBUG_STACKTRACE==1:\n",
    "    traceback.print_exc()\n",
    "    console.print_exception(show_locals=True)\n",
    "  else:\n",
    "    rprint(repr(inc_exception))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmbRXk62KTpe"
   },
   "source": [
    "## Setup Instances of Variables for System Configuration and Library Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1737665453949,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "cQ7g7aUTKXcs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the rich print console for future use\n",
    "if DEBUG_STACKTRACE==1:\n",
    "  console = Console()\n",
    "\n",
    "# Use the 'Agg' backend for non-interactive environments\n",
    "#matplotlib.use('Agg')\n",
    "\n",
    "# Ensure UTF-8 Encoding is set\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG0mFzUX-DV1"
   },
   "source": [
    "## Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1737665454279,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "SSOOEwn8-FKg",
    "outputId": "f1d7c56b-5717-480a-f2f8-93d95eeecfa6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib_diagnostics()\n",
    "#setup the text wrapper\n",
    "wrapper = textwrap.TextWrapper(width=TEXT_WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTrnAspo3CWR"
   },
   "source": [
    "## Check your resources from a CPU/GPU perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1737665454280,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "hmGUvC7M3B0H",
    "outputId": "638ecead-d943-42f2-a2eb-99790d7fe3bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{BOLD_START}List Devices{BOLD_END} #########################################\")\n",
    "try:\n",
    "  from tensorflow.python.client import device_lib\n",
    "  rprint(device_lib.list_local_devices())\n",
    "  print(\"\")\n",
    "except RuntimeError as e:\n",
    "  # Visible devices must be set before GPUs have been initialized\n",
    "  rprint(str(repr(e)))\n",
    "\n",
    "print(f\"{BOLD_START}Devices Counts{BOLD_END} ########################################\")\n",
    "try:\n",
    "  rprint(f\"Num GPUs Available: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\" )\n",
    "  rprint(f\"Num CPUs Available: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\" )\n",
    "  print(\"\")\n",
    "except RuntimeError as e:\n",
    "  # Visible devices must be set before GPUs have been initialized\n",
    "  rprint(str(repr(e)))\n",
    "\n",
    "print(f\"{BOLD_START}Optional Enablement{BOLD_END} ####################################\")\n",
    "try:\n",
    "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "except RuntimeError as e:\n",
    "  # Visible devices must be set before GPUs have been initialized\n",
    "  rprint(str(repr(e)))\n",
    "\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    rprint( str( str(len(gpus)) + \" Physical GPUs,\" + str(len(logical_gpus)) + \" Logical GPU\") )\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    rprint(str(repr(e)))\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNG1Rq0bKjl6"
   },
   "source": [
    "## Lookup for the following for TensorBoard use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1737665454281,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "tpk0bQejxXW_",
    "outputId": "a066c671-a95f-4eeb-d2d0-5ff8335fe9df",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%load_ext tensorboard\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46JMSTY2QjWD"
   },
   "source": [
    "## Input Sources\n",
    "### Copy a repository of images to the local Google Colab instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1737665454282,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "pu8f4w5XK6i8",
    "outputId": "2aa1c04c-7c5a-4348-93a1-67ea14f79c83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the folder that will hold our content.\n",
    "target_folder=WORKING_FOLDER\n",
    "rprint(f\"Creating a folder ({target_folder}) to store project data.\")\n",
    "\n",
    "try:\n",
    "  if os.path.isfile(target_folder):\n",
    "    raise OSError(\"Cannot create your folder a file of the same name already exists there, work with your instructor or remove it yourself.\")\n",
    "  elif os.path.isdir(target_folder):\n",
    "    print(f\"The folder named ({target_folder}) {BOLD_START}already exists{BOLD_END}, we won't try to create a new folder.\")\n",
    "  else:\n",
    "    subprocess.run([\"mkdir\", \"-p\" , target_folder], check=True)\n",
    "except (subprocess.CalledProcessError, Exception) as e:\n",
    "  process_exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66496,
     "status": "ok",
     "timestamp": 1737665520749,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "AZcbq467sgrc",
    "outputId": "0a82f71c-4bcc-45bb-8681-8cce7803c49a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_folder=WORKING_FOLDER\n",
    "#!rm -rf ./folderOnColab && echo \"Ok, removed.\" || { echo \"No folder to remove.\"; exit 1; }\n",
    "\n",
    "#BEARS\n",
    "#!mkdir -p ./folderOnColab && echo \"Folder created.\" || { echo \"Failed to create folder, perhaps it already exists.\";   }\n",
    "#!gsutil -m cp -r gs://usfs-gcp-rand-test-data-usc1/public_source/computervision/bear ./folderOnColab\n",
    "\n",
    "#ENTOMOLOGY\n",
    "#!mkdir -p ./folderOnColab && echo \"Folder created.\" || { echo \"Failed to create folder, perhaps it already exists.\";   }\n",
    "#!gsutil -m cp -r gs://usfs-gcp-rand-test-data-usc1/public_source/computervision/ENTOMOLOGY ./folderOnColab\n",
    "\n",
    "if os.path.isdir(target_folder):\n",
    "    print(f\"Copying file to target folder: {target_folder}\")\n",
    "    try:\n",
    "      subprocess.run([\"gsutil\", \"-m\" , \"cp\", \"-r\", f\"gs://{BUCKET_NAME}/training-data/computervision/ENTOMOLOGY\",  target_folder], check=True)\n",
    "    except (subprocess.CalledProcessError, Exception) as e:\n",
    "      process_exception(e)\n",
    "else:\n",
    "    print(\"ERROR: Local folder not found/created.  Check the output to ensure your folder is created.\")\n",
    "    print(f\"...target folder: {target_folder}\")\n",
    "    print(\"...if you can't find the problem contact the instructor.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDLoEWmVR9Sk"
   },
   "source": [
    "### List the files copied, are they there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1737665520751,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "IZrJ8FhpR78m",
    "outputId": "111a3aec-6ad8-4889-c3ed-7f8e9ed4e737",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rprint(f\"Display of files in target folder: {target_folder}/ENTOMOLOGY\")\n",
    "\n",
    "from pathlib import Path\n",
    "source = Path(f\"{target_folder}/ENTOMOLOGY\")\n",
    "the_files=[x.name for x in source.iterdir()]\n",
    "for file in the_files:\n",
    "  rprint(f\"./{file:50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flrUnfF9SRp-"
   },
   "source": [
    "### Show some pictures\n",
    "\n",
    "Let's look at the first set of images in the dataset.\n",
    "\n",
    "What happens if you change \"range_max\" to another number, like 12?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 828
    },
    "executionInfo": {
     "elapsed": 1833,
     "status": "ok",
     "timestamp": 1737665522572,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "Rqs3zKR6STGu",
    "outputId": "6e17b064-dd35-4d50-9887-c799c9c77b12",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#- Show pictures of the image dataset\n",
    "\n",
    "num_images=range_max\n",
    "\n",
    "caption_start=f\"Image from the test dataset representing an image of the actual training data data:\"\n",
    "caption_end = \" as seen from top to bottom, left to right.\"\n",
    "the_labels=[]\n",
    "\n",
    "for idx, category in enumerate(categories_short_name):\n",
    "  print(f\"{BOLD_START}{category.upper()}{BOLD_END}\")\n",
    "  # define location of dataset\n",
    "  folder = TRAIN_DATA_DIR + os.sep + category\n",
    "\n",
    "  # plot first few images\n",
    "  for i in range(num_images):\n",
    "    idx=i+1\n",
    "    plt.subplot(plot_max + 1 + i)\n",
    "    filename = folder + os.sep + f'00{idx}{IMG_EXT}'\n",
    "    actual_filename=os.path.basename(filename)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    the_labels.append(f\"{category}-{idx}\")\n",
    "    plt.xlabel(f\"{the_labels[-1]} - {actual_filename}\")\n",
    "    image = imread(filename)\n",
    "    plt.imshow(image)\n",
    "  plt.show()\n",
    "\n",
    "caption_labels=\", \".join(the_labels)\n",
    "caption_text = \" \".join([caption_start, caption_labels, caption_end])\n",
    "rprint(caption_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-CaUT8B3T8V"
   },
   "source": [
    "## OS Configuration Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1737665522573,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "dwPl2ODJ3Tic",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(TF_CPP_MIN_LOG_LEVEL_SETTING)\n",
    "\n",
    "#To disable GPU access to the current runtime enable -1 for \"no GPUs\"\n",
    "#confirmed to work on the Command Line Interface (CLI) of all systems but this one...\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM7Z5aaQbOX9"
   },
   "source": [
    "## Compiling the model\n",
    "To compile the neural network the gradient descent is applied. This is the optimization strategy that reduces the errors as the network is learning. There are various optimization strategies but sgd and adam are common approachs. In the compile stage, we also define the loss function and the metrics. We use sparse categorical cross-entropy because the labels are integers. The categorical cross-entropy is used when the labels are one-hot encoded.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "Machine learning algorithms have hyperparameters that can be configured to tailor the algorithm to a specific dataset.\n",
    "\n",
    "Although the dynamics of many hyperparameters are known, the specific effect they will have on the performance of the resulting model on a given dataset is not known. As such, it is a standard practice to test a suite of values for key algorithm hyperparameters for a chosen machine learning algorithm.\n",
    "\n",
    "This is called hyperparameter tuning or hyperparameter optimization.\n",
    "\n",
    "It is common to use a naive optimization algorithm for this purpose, such as a random search algorithm or a grid search algorithm.\n",
    "\n",
    "Hyperparameter Tuning: Function inputs are algorithm hyperparameters, optimization problems that require an iterative global search algorithm.\n",
    "For more on this topic, see the tutorial:\n",
    "\n",
    "Hyperparameter Optimization With Random Search and Grid Search\n",
    "\n",
    "Nevertheless, it is becoming increasingly common to use an iterative global search algorithm for this optimization problem. A popular choice is a Bayesian optimization algorithm that is capable of simultaneously approximating the target function that is being optimized (using a surrogate function) while optimizing it.\n",
    "\n",
    "This is desirable as evaluating a single combination of model hyperparameters is expensive, requiring fitting the model on the entire training dataset one or many times, depending on the choice of model evaluation procedure (e.g. repeated k-fold cross-validation).\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "An \"activation function\" in a neural network is a mathematical function that transforms the weighted sum of inputs received by a neuron into an output signal, essentially deciding whether a neuron should be \"activated\" or not, allowing the network to learn complex patterns in data by introducing non-linearity to the model; it plays a crucial role in determining the output of a neuron based on its input and is a key component of artificial neural networks.\n",
    "\n",
    "The choice of activation function can significantly influence the performance of a neural network.\n",
    "\n",
    "Common examples of activation functions:\n",
    "\n",
    "+ Sigmoid: Outputs a value between 0 and 1, often used in older neural networks.\n",
    "\n",
    "+ ReLU (Rectified Linear Unit): Outputs the input if positive, otherwise 0; widely used due to its computational efficiency.\n",
    "\n",
    "+ Tanh (Hyperbolic Tangent): Outputs a value between -1 and 1.\n",
    "\n",
    "+ Softmax: Used primarily in the output layer for multi-class classification problems, where it outputs a probability distribution over all classes.\n",
    "\n",
    "## Optimization Algorithms\n",
    "\n",
    "Optimization algorithms are the backbone of machine learning models as they enable the modeling process to learn from a given data set. These algorithms are used in order to find the minimum or maximum of an objective function which in machine learning context stands for error or loss.\n",
    "\n",
    "### Adam\n",
    "Adam, or Adaptive Moment Estimation, is an optimization algorithm that's used to train deep neural networks in machine learning. It's a combination of the ideas from two other optimization techniques, RMSprop and momentum, and is considered the default algorithm for deep learning.\n",
    "\n",
    "\n",
    "## Over/Under-fitting\n",
    "\n",
    "**Overfitting**\n",
    "\n",
    "In machine learning, overfitting is when a model is too closely trained to a specific set of data, causing it to perform poorly on new data. This can happen when:\n",
    "+ The model trains for too long on the training data\n",
    "+ The model is too complex\n",
    "+ The model memorizes irrelevant information in the training data\n",
    "\n",
    "An overfitted model is similar to an invention that works well in a lab but not in the real world. It can't make accurate predictions or conclusions from any data other than the training data.\n",
    "\n",
    "**Underfitting**\n",
    "\n",
    "Underfitting in machine learning occurs when a model is unable to accurately capture the relationship between input and output data. This can lead to poor model performance and unreliable predictions.\n",
    "\n",
    "Here are some characteristics of underfitting:\n",
    "+ High bias\n",
    "  + Underfit models produce inaccurate results for both the training data and test set.\n",
    "+ Low variance\n",
    "  + Underfitting is characterized by low variance.\n",
    "+ Too simple model\n",
    "  + Underfitting can occur when a model is too simple, such as a linear regression model trained on a dataset with a polynomial relationship.\n",
    "+ Not enough training data\n",
    "  + Underfitting can occur if the model hasn't been trained for long enough or on enough data points.\n",
    "\n",
    "## References:\n",
    "+ https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "+ https://machinelearningmastery.com/why-optimization-is-important-in-machine-learning/\n",
    "+ https://towardsdatascience.com/understanding-optimization-algorithms-in-machine-learning-edfdb4df766b\n",
    "+ https://geeksforgeeks.org/optimization-algorithms-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaZUjXEMoE12"
   },
   "source": [
    "## Develop a Baseline CNN Model\n",
    "\n",
    "A baseline model will establish a minimum model performance to which all of our other models can be compared, as well as a model architecture that we can use as the basis of study and improvement.\n",
    "\n",
    "A good starting point is the general architectural principles of the VGG models.\n",
    "\n",
    "### What is the Conv2D Layer?\n",
    "\n",
    "A \"Conv2D\" layer in a CNN (Convolutional Neural Network) refers to a 2-dimensional convolutional layer, which applies a set of filters (also called kernels) that slide across the input data (like an image) to extract features by performing element-wise multiplication and summation, essentially creating a new feature map that captures local patterns within the image; it is the primary building block of a CNN for processing 2D data like images, where the convolution operation happens across both the height and width dimensions of the input data.\n",
    "\n",
    "### Max Pooling?\n",
    "Max pooling is a technique that reduces the size of an image while keeping the most important information. It's commonly used in convolutional neural networks (CNNs).\n",
    "\n",
    "### Why Flatten?\n",
    "\n",
    "The flatten layer typically appears after the convolutional and pooling layers in convolutional neural network (CNN) architectures. It acts as a bridge between the convolutional/pooling layers, which extract spatial features, and the fully connected layers, which perform classification or regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 1442,
     "status": "ok",
     "timestamp": 1737665524007,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "6fx40WcUMkzy",
    "outputId": "ec15805d-d525-4ced-939e-8e239084c9e4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define cnn model with a single layer\n",
    "def define_model_1Block():\n",
    "\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
    "\t\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\t\tmodel.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\t\t# not used, demonstration of additional parameters only\n",
    "\t\topt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "\t\t# compile model\n",
    "\t\tmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=METRICS)\n",
    "\t\tmodel.summary()\n",
    "\n",
    "\t\treturn model\n",
    "\n",
    "start_t=perf_counter()\n",
    "\n",
    "try:\n",
    "\t#notice that we're loading the model on an explicit device.\n",
    "  with tf.device(THE_DEVICE_NAME):\n",
    "    model1 = define_model_1Block()\n",
    "except RuntimeError as e:\n",
    "  print(str(repr(e)))\n",
    "\n",
    "end_t=perf_counter()\n",
    "print(f\"Model setup time: {end_t - start_t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1737665524007,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "MTJJMK8nMg0j",
    "outputId": "3c69969c-fcca-41d0-f68c-7fa00155ed40",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define cnn model with two layers\n",
    "def define_model_2Block():\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
    "\t\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', ))\n",
    "\t\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\t\tmodel.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    #Not used, just another example\n",
    "\t\t#opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "\t\t#model.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
    "\n",
    "\t\t# compile model\n",
    "\t\tmodel.compile(optimizer=\"adam\", loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
    "\t\tmodel.summary()\n",
    "\t\treturn model\n",
    "\n",
    "start_t=perf_counter()\n",
    "\n",
    "try:\n",
    "  with tf.device(THE_DEVICE_NAME):\n",
    "    model2 = define_model_2Block()\n",
    "except RuntimeError as e:\n",
    "  print(str(repr(e)))\n",
    "\n",
    "end_t=perf_counter()\n",
    "print(f\"Model setup time: {end_t - start_t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1737665524283,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "YLZzN-o3a8fc",
    "outputId": "ffa56f0e-fe3f-4901-e9c5-02713c2d8849",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define cnn model with 3 layers\n",
    "# Adding dropout regularization is a computationally cheap way to regularize a deep neural network.\n",
    "# Dropout works by probabilistically removing, or dropping out, inputs to a layer, which may be input variables\n",
    "# in the data sample or activations from a previous layer. It has the effect of simulating a large number of networks\n",
    "# with very different network structures and, in turn, making nodes in the network generally more robust to the inputs.\n",
    "# https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c\n",
    "\n",
    "def define_model_3Block():\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "  model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dropout(0.24))\n",
    "  model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "  # compile model\n",
    "  model.compile(optimizer=\"adam\", loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
    "\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "start_t=perf_counter()\n",
    "\n",
    "try:\n",
    "  with tf.device(THE_DEVICE_NAME):\n",
    "    model3 = define_model_3Block()\n",
    "except RuntimeError as e:\n",
    "  print(str(repr(e)))\n",
    "\n",
    "end_t=perf_counter()\n",
    "print(f\"Model setup time: {end_t - start_t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1737665524972,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "VNT8whktNCZb",
    "outputId": "da378ae4-c54e-4539-aea3-294cba768a2b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define a new model using VGG16 with Transfer Learning\n",
    "def define_model_TransferLearning():\n",
    "  #transfer learning (VGG16)\n",
    "  model = VGG16(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "  # mark loaded layers as un-trainable, keep the original weights\n",
    "  for layer in model.layers:\n",
    "      layer.trainable = False\n",
    "\n",
    "  # add new classifier layers\n",
    "  flat1 = Flatten()(model.layers[-1].output)\n",
    "  class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\n",
    "  #this layer defines the classification, outputs much match total labels\n",
    "  output = Dense(2, activation='softmax')(class1)\n",
    "\n",
    "  # define new model\n",
    "  model = Model(inputs=model.inputs, outputs=output)\n",
    "\n",
    "  # optimization\n",
    "  opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "  # compile model\n",
    "  #model.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
    "  model.compile(optimizer=\"adam\", loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
    "\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "\n",
    "start_t=perf_counter()\n",
    "\n",
    "try:\n",
    "  with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "    modelTransfer = define_model_TransferLearning()\n",
    "except RuntimeError as e:\n",
    "  print(str(repr(e)))\n",
    "\n",
    "end_t=perf_counter()\n",
    "print(f\"Transfer learning model setup time: {end_t - start_t}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "no7ztdkrRcG8"
   },
   "source": [
    "## Data Marshalling and Augmentation\n",
    "\n",
    "Reference: https://keras.io/examples/vision/image_classification_from_scratch/\n",
    "\n",
    "Note that embedding the image augmentation within the model build/fit also allows the image manipulations (only available during training, not permanently stored to disk) to benefit from GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "executionInfo": {
     "elapsed": 1893,
     "status": "ok",
     "timestamp": 1737665526860,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "yxMkhxpXoc3O",
    "outputId": "3df6af17-2ef9-4e76-df04-7da37287c4a9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{BOLD_START}Preparing Training Datasets{BOLD_END}\")\n",
    "print('\\n')\n",
    "\n",
    "try:\n",
    "  with tf.device(THE_DEVICE_NAME):\n",
    "\n",
    "      # Example code of past Keras 2.0 use.  API upgrades can have drastic impact on existing code.\n",
    "      \"\"\"\n",
    "      #deprecated, use tf.data methods to replace this functionality, see: https://www.tensorflow.org/tutorials/load_data/images\n",
    "      train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "      #data augmentation is a way to improve results, uncomment this line, replacing train_datagen and re-run your experiment\n",
    "      train_datagen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                                        height_shift_range=0.1,\n",
    "                                        rescale=1./255,\n",
    "                                        shear_range=0.2,\n",
    "                                        zoom_range=0.2,\n",
    "                                        rotation_range=45,\n",
    "                                        horizontal_flip=True,\n",
    "                                        vertical_flip=True,\n",
    "                                        validation_split = .2)\n",
    "      \"\"\"\n",
    "\n",
    "      #see: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory\n",
    "      train_it = keras.utils.image_dataset_from_directory(\n",
    "          directory=TRAIN_DATA_DIR,\n",
    "          labels=\"inferred\",\n",
    "          image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          #class_mode='binary',\n",
    "          label_mode='categorical',\n",
    "          #verbose=True,\n",
    "          #pad_to_aspect_ratio=False,\n",
    "          #crop_to_aspect_ratio=False,\n",
    "          #seed=SEED_INIT,\n",
    "          #shuffle=True,\n",
    "          #validation_split=VALIDATION_SPLIT,\n",
    "          #subset=\"training\",\n",
    "          )\n",
    "\n",
    "      #deprecated\n",
    "      #test_it = train_datagen.flow_from_directory(\n",
    "      test_it = keras.utils.image_dataset_from_directory(\n",
    "          directory=VALIDATION_DATA_DIR, # same directory as training data\n",
    "          labels=\"inferred\",\n",
    "          image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          #class_mode='binary',\n",
    "          label_mode='categorical',\n",
    "          #verbose=True,\n",
    "          #pad_to_aspect_ratio=False,\n",
    "          #crop_to_aspect_ratio=False,\n",
    "          #seed=SEED_INIT,\n",
    "          #shuffle=True,\n",
    "          #validation_split=VALIDATION_SPLIT,\n",
    "          #subset=\"validation\",\n",
    "          )\n",
    "      #batch size is a hyper-parameter and has influence over training as well.  Larger batch sizes get done faster but take more memory, smaller batch sizes take longer but consume less memory.\n",
    "\n",
    "except (Exception, RuntimeError) as e:\n",
    "    print(f\"{start}ERROR (Exception encountered):{end} \")\n",
    "    string = wrapper.fill(text=str(e))\n",
    "    print(string)\n",
    "\n",
    "print(\"\")\n",
    "class_names = test_it.class_names\n",
    "print(f\"{BOLD_START} Class Names (labels){BOLD_END} ##################################\")\n",
    "rprint(class_names)\n",
    "print(\"\")\n",
    "\n",
    "# it's a good practice to artificially introduce sample diversity by applying\n",
    "#random yet realistic transformations to the training images, such as random horizontal\n",
    "#flipping or small random rotations. This helps expose the model to different aspects of\n",
    "#the training data while slowing down overfitting.\n",
    "# Apply `data_augmentation` to the training images, see: https://www.tensorflow.org/tutorials/images/data_augmentation\n",
    "# Note that this is a CPU intensive operation, see the reference above if you\n",
    "# want to add this transformation to the GPU instead.\n",
    "\n",
    "print(\"Augmenting data...\")\n",
    "data_augmentation_layers = [\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomContrast(0.1, seed=SEED_INIT,),\n",
    "    layers.RandomCrop(IMG_HEIGHT, IMG_WIDTH, seed=SEED_INIT,),\n",
    "    layers.RandomZoom(0.1,seed=SEED_INIT,)\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images\n",
    "\n",
    "\n",
    "train_it = train_it.map(\n",
    "                        lambda img, label: (data_augmentation(img), label),\n",
    "                        num_parallel_calls=tf_data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "#prepare the dataset for maximum performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "train_it= train_it.prefetch(tf_data.AUTOTUNE)\n",
    "test_it = test_it.prefetch(tf_data.AUTOTUNE)\n",
    "\n",
    "end_t=perf_counter()\n",
    "print(f\"Data read setup time: {end_t - start_t}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTtJhQ2hnL0V"
   },
   "source": [
    "## Epochs\n",
    "\n",
    "Notice each epoc completion demonstrates an accuracy improvement.  You can add a *patience* function to drop out when your training reaches an optimal level to avoid overfitting.  Additional *drop out* functions are available as well.\n",
    "\n",
    "In artificial intelligence, an \"epoch\" refers to a single complete pass of the entire training dataset through a machine learning model, where the model processes every data point once and updates its internal parameters based on that pass, essentially representing one full cycle of learning from the training data; it's a key metric used to measure how many times the model has seen the entire dataset during training.\n",
    "\n",
    "## Key points about epochs:\n",
    "\n",
    "+ Full data pass:\n",
    "An epoch is completed when the model has processed every data point in the training set once.\n",
    "\n",
    "+ Hyperparameter:\n",
    "The number of epochs to run is considered a hyperparameter that needs to be tuned for optimal model performance.\n",
    "\n",
    "+ Batching:\n",
    "While processing the data, the dataset is often split into smaller batches, and the model updates its parameters after each batch, but one full pass through all batches constitutes an epoch.\n",
    "\n",
    "### Why epochs matter:\n",
    "\n",
    "+ Learning progression:\n",
    "By running multiple epochs, the model can gradually learn more complex patterns from the data.\n",
    "\n",
    "+ Overfitting risk:\n",
    "Training for too many epochs can lead to overfitting, where the model becomes too specialized to the training data and performs poorly on new data.\n",
    "\n",
    "+ Validation monitoring:\n",
    "To avoid overfitting, it's crucial to monitor the model's performance on a separate validation set after each epoch and stop training when validation accuracy starts to decline.\n",
    "\n",
    "## Batch-Size\n",
    "\n",
    "The choice of batch size can have a significant impact on the learning process. A smaller batch size can lead to faster convergence and can help the model escape from local minima. However, it also introduces more noise into the gradient estimate, which can lead to instability in the learning process.\n",
    "\n",
    "A larger batch size, on the other hand, can provide a more stable learning process and a more accurate gradient estimate. However, it also requires more computational resources and may lead to slower convergence. Furthermore, it may increase the risk of the model getting stuck in local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op_8ukuEO2IA"
   },
   "source": [
    "## Create a Custom Class for Visualization during Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1737665526860,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "T81dBVbyzcxR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# custom code designed to plot results real-time as the model trains.\n",
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "\n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "\n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        #plot metrics, if accuracy show percent else show sliding scale\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2),\n",
    "                        self.metrics[metric],\n",
    "                        label=metric,\n",
    "                        )\n",
    "            if metric==\"accuracy\":\n",
    "              axs[i].set_ylim([0.0, 1.0])\n",
    "              axs[i].set_ylabel(\"Percent (0.0 - 1.0)\")\n",
    "\n",
    "\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2),\n",
    "                            self.metrics['val_' + metric],\n",
    "                            label='val_' + metric)\n",
    "                if metric==\"val_\" + metric:\n",
    "                  current_ax = plt.gca()\n",
    "                  axs[i].set_ylim([0.0, 1.0])\n",
    "                  axs[i].set_ylabel(\"Percent (0.0 - 1.0)\")\n",
    "\n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-sMQ4DVw1mn"
   },
   "source": [
    "## What is Loss and Accruacy?\n",
    "\n",
    "The lower the loss, the better a model (unless the model has over-fitted to the training data). The loss is calculated on training and validation and its interperation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets.\n",
    "\n",
    "In the case of neural networks, the loss is usually negative log-likelihood and residual sum of squares for classification and regression respectively. Then naturally, the main objective in a learning model is to reduce (minimize) the loss function's value with respect to the model's parameters by changing the weight vector values through different optimization methods, such as backpropagation in neural networks.\n",
    "\n",
    "Loss value implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of loss after each, or several, iteration(s).\n",
    "\n",
    "The accuracy of a model is usually determined after the model parameters are learned and fixed and no learning is taking place. Then the test samples are fed to the model and the number of mistakes (zero-one loss) the model makes are recorded, after comparison to the true targets. Then the percentage of misclassification is calculated.\n",
    "\n",
    "For example, if the number of test samples is 1000 and model classifies 952 of those correctly, then the model's accuracy is 95.2%.\n",
    "\n",
    "Precision and recall are metrics used in machine learning to evaluate how well a model performs at identifying positive class samples:\n",
    "\n",
    "+ **Precision**: The proportion of positive class predictions that are correct.\n",
    "+ **Recall**: The proportion of actual positive class samples that are identified by the model.\n",
    "\n",
    "Precision and recall are useful for identifying knowledge gaps in a model, especially when the dataset is imbalanced. However, there is a trade-off between the two metrics, so increasing one will usually decrease the other. For example, if a model has 100% recall, but only 50% precision, then it means that the model correctly identified all positive samples, but also incorrectly classified some other observations as positive.\n",
    "When choosing between precision and recall, it's important to consider the class balance and the cost of different errors. A common metric that combines both precision and recall is the F-score, which is the harmonic average of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psNDTVBEy07O"
   },
   "source": [
    "### Define an Early Stopper to prevent Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1737665526861,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "oYjB8RtPyxTj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a call back.  A callback is a function that is passed as an argument to another function and\n",
    "# is executed at a later point in the program\n",
    "\n",
    "# Simple callback to show results real-time\n",
    "#callbacks_list = [PlotLearning()]\n",
    "\n",
    "# Watch the learning after execution\n",
    "#call this on the CLI after execution: tensorboard --logdir=./folderOnColab/logs\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./folderOnColab/logs\")\n",
    "\n",
    "# Another way to increase performance is to stop early, try commenting out the previous callback list\n",
    "# and uncommenting this one.  See the EPOCHS go from full execution to a shortened cycle.\n",
    "es_callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "# How to you persist your model, how to you deal with overfitting?\n",
    "mc_callback = ModelCheckpoint('best_model1.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOC03czMO0CK"
   },
   "source": [
    "## Train 1 Block Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "executionInfo": {
     "elapsed": 190435,
     "status": "ok",
     "timestamp": 1737665717279,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "8wsDSMW3oqsB",
    "outputId": "94f3c459-15ee-4843-db2a-5a1d13756ea1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure callbacks (performed per epoch) for this particular model\n",
    "callbacks_list = [PlotLearning(), es_callback, mc_callback, tensorboard_callback]\n",
    "\n",
    "try:\n",
    "  # Ensure you specify the execution hardward\n",
    "  with tf.device(THE_DEVICE_NAME):\n",
    "    # fit model, actual training against data, see: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "    history1 = model1.fit(train_it,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          epochs=EPOCHS,\n",
    "                          callbacks=callbacks_list,\n",
    "                          validation_data=test_it,\n",
    "                          validation_split=VALIDATION_SPLIT,\n",
    "                          class_weight=None,             #you would be a set of weights per category if you had an unbalanced classification problem\n",
    "                          verbose=2,                     #0 silent, 1 more, 2 max\n",
    "                          )\n",
    "\n",
    "except (Exception, RuntimeError) as e:\n",
    "    process_exception(e)\n",
    "\n",
    "\n",
    "end_t=perf_counter()\n",
    "rprint(f\"Model fit time: {end_t - start_t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rh__nOOPKm1"
   },
   "source": [
    "## Train Model 2 Block (2 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "executionInfo": {
     "elapsed": 167845,
     "status": "ok",
     "timestamp": 1737665885113,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "SQ_5Sy27PPFo",
    "outputId": "1329cb16-40e2-4d8e-93f3-cea2fe961a9b"
   },
   "outputs": [],
   "source": [
    "# Configure call backs to include real-time plotting, early stopping, saving models as you go and tensor log saves.\n",
    "mc_callback = ModelCheckpoint('best_model2.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "callbacks_list = [PlotLearning(), es_callback, mc_callback, tensorboard_callback]\n",
    "\n",
    "try:\n",
    "  with tf.device(THE_DEVICE_NAME):\n",
    "    # fit model, actual training against data, see: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "    history2 = model2.fit(train_it,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          epochs=EPOCHS,\n",
    "                          callbacks=callbacks_list,\n",
    "                          validation_data=test_it,\n",
    "                          validation_split=VALIDATION_SPLIT,\n",
    "                          class_weight=None,             #you would be a set of weights per category if you had an unbalanced classification problem\n",
    "                          verbose=2,                     #0 silent, 1 more, 2 max\n",
    "                          )\n",
    "\n",
    "except (Exception, RuntimeError) as e:\n",
    "    process_exception(e)\n",
    "\n",
    "\n",
    "end_t=perf_counter()\n",
    "rprint(f\"Model fit time: {end_t - start_t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJMB2zTuPSGk"
   },
   "source": [
    "## Train Model 3 Block (3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "executionInfo": {
     "elapsed": 186275,
     "status": "ok",
     "timestamp": 1737666071366,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "cAsmzk_WPRXC",
    "outputId": "f22aaa46-94dd-42b6-93f8-0c2090b672a9"
   },
   "outputs": [],
   "source": [
    "mc_callback = ModelCheckpoint('best_model3.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "callbacks_list = [PlotLearning(), es_callback, mc_callback, tensorboard_callback]\n",
    "\n",
    "try:\n",
    "  with tf.device(THE_DEVICE_NAME):\n",
    "    # fit model, actual training against data, see: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "    history3 = model3.fit(train_it,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          epochs=EPOCHS,\n",
    "                          callbacks=callbacks_list,\n",
    "                          validation_data=test_it,\n",
    "                          validation_split=VALIDATION_SPLIT,\n",
    "                          class_weight=None,             #you would be a set of weights per category if you had an unbalanced classification problem\n",
    "                          verbose=2,                     #0 silent, 1 more, 2 max\n",
    "                          )\n",
    "\n",
    "except (Exception, RuntimeError) as e:\n",
    "    process_exception(e)\n",
    "\n",
    "\n",
    "end_t=perf_counter()\n",
    "rprint(f\"Model fit time: {end_t - start_t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWsTR8ZfPWM2"
   },
   "source": [
    "## Tranfer Learning Model (VGG 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "executionInfo": {
     "elapsed": 114089,
     "status": "ok",
     "timestamp": 1737666185439,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "ddvLxqNDPZOJ",
    "outputId": "3e6f9c59-2120-4fc5-f5f1-90afa0def2a5"
   },
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc_callback = ModelCheckpoint('best_modelVGG16.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks_list = [PlotLearning(), es_callback, mc_callback, tensorboard_callback]\n",
    "\n",
    "try:\n",
    "  with tf.device(THE_DEVICE_NAME):\n",
    "    # fit model, actual training against data, see: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "    historyTransfer = modelTransfer.fit(train_it,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        epochs=EPOCHS,\n",
    "                                        callbacks=callbacks_list,\n",
    "                                        validation_data=test_it,\n",
    "                                        validation_split=VALIDATION_SPLIT,\n",
    "                                        class_weight=None,             #you would be a set of weights per category if you had an unbalanced classification problem\n",
    "                                        verbose=2,                     #0 silent, 1 more, 2 max\n",
    "                                       )\n",
    "\n",
    "except (Exception, RuntimeError) as e:\n",
    "  process_exception(e)\n",
    "\n",
    "end_t=perf_counter()\n",
    "rprint(f\"Model fit time: {end_t - start_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1737666185439,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "_AdW3Tx-o2hS"
   },
   "outputs": [],
   "source": [
    "# Plot diagnostic learning curves, additional plotting routines\n",
    "\n",
    "def summarize_diagnostics_seaborn(history, title):\n",
    "  # Create pandas DataFrame\n",
    "  df_history = pd.DataFrame(history.history)\n",
    "  #print(df_history)\n",
    "\n",
    "  #turn values into real percents\n",
    "  df_history['Training_Accuracy'] = df_history[\"accuracy\"] * 100.0\n",
    "  df_history['Validation_Accuracy'] = df_history[\"val_accuracy\"] * 100.0\n",
    "\n",
    "  #color palette\n",
    "  palette = ['r','b','g']\n",
    "\n",
    "  # Plot using Seaborn\n",
    "  sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "  sns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\n",
    "  fig = plt.figure(figsize=[FIGURE_WIDTH, FIGURE_HEIGHT])\n",
    "  ax = plt.subplot(111)\n",
    "  my_plot = sns.lineplot(data=df_history[[\"Training_Accuracy\",\"Validation_Accuracy\"]],\n",
    "                         markers=True, dashes=False,palette=palette)\n",
    "\n",
    "  my_plot.set_xlabel('Epochs')\n",
    "  my_plot.set_ylim(0,100)\n",
    "  my_plot.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter())\n",
    "  my_plot.set_ylabel('Accuracy')\n",
    "\n",
    "  plt.title('Training and Validation Loss \\n' + title)\n",
    "  ttl = ax.title\n",
    "  ttl.set_weight('bold')\n",
    "\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1737666185440,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "L0_mPjkfdeJs"
   },
   "outputs": [],
   "source": [
    "def show_performance_curve(training_result, metric, metric_label):\n",
    "\n",
    "\ttrain_perf = training_result.history[str(metric)]\n",
    "\tvalidation_perf = training_result.history['val_'+str(metric)]\n",
    "\tintersection_idx = np.argwhere(np.isclose(train_perf,\n",
    "                                            \tvalidation_perf, atol=1e-2)).flatten()[0]\n",
    "\tintersection_value = train_perf[intersection_idx]\n",
    "\n",
    "\tplt.plot(train_perf, label=metric_label)\n",
    "\tplt.plot(validation_perf, label = 'val_'+str(metric))\n",
    "\tplt.axvline(x=intersection_idx, color='r', linestyle='--', label='Intersection')\n",
    "\n",
    "\tplt.annotate(f'Optimal Value: {intersection_value:.4f}',\n",
    "         \txy=(intersection_idx, intersection_value),\n",
    "         \txycoords='data',\n",
    "         \tfontsize=IMG_FONT_SIZE_MIN,\n",
    "         \tcolor='green')\n",
    "\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel(metric_label)\n",
    "\tplt.legend(loc='lower right')\n",
    "\tplt.title(metric)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRZ-3hKmJO5v"
   },
   "source": [
    "## Demonstrate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944
    },
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1737666186429,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "VZoK-f5bo5T0",
    "outputId": "43240944-8f72-45a5-a947-2c1699012c61"
   },
   "outputs": [],
   "source": [
    "# model 1\n",
    "try:\n",
    "    print(f'  {BOLD_START}1 Layer Model{BOLD_END}')\n",
    "    loss, accuracy, precision, recall = model1.evaluate(test_it, verbose=2)\n",
    "\n",
    "    rprint(f'  Loss > {loss: .2f}')\n",
    "    rprint(f'  Accuracy > {accuracy: .2%}')\n",
    "    print(\"\\n\\n\")\n",
    "    summarize_diagnostics_seaborn(history1, \"One Layer\")\n",
    "except Exception as e:\n",
    "  process_exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "executionInfo": {
     "elapsed": 1108,
     "status": "ok",
     "timestamp": 1737666187529,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "-_aMxcRJIwf9",
    "outputId": "c3a84622-5586-4dc3-8adc-5c8ebdab6adc"
   },
   "outputs": [],
   "source": [
    "# Model 2 Layer Configuration Results\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f'  {BOLD_START}2 Layer Model{BOLD_END}')\n",
    "loss, accuracy, precision, recall = model2.evaluate(test_it, steps=len(test_it), verbose=0)\n",
    "rprint(f'  Loss > {loss: .2f}')\n",
    "rprint(f'  Accuracy > {accuracy: .2%}')\n",
    "print(\"\\n\\n\")\n",
    "summarize_diagnostics_seaborn(history2, \"Two Layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1737666188331,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "aXocrLceI0yl",
    "outputId": "b864ef68-8dba-4227-acd6-7ff5eb0b0cb5"
   },
   "outputs": [],
   "source": [
    "# Model 3 Layer Configuration Results\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f'  {BOLD_START}3 Layer Model{BOLD_END}')\n",
    "loss, accuracy, precision, recall = model3.evaluate(test_it, steps=len(test_it), verbose=0)\n",
    "rprint(f'  Loss > {loss: .2f}')\n",
    "rprint(f'  Accuracy > {accuracy: .2%}')\n",
    "print(\"\\n\\n\")\n",
    "summarize_diagnostics_seaborn(history3, \"Three Layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "executionInfo": {
     "elapsed": 1182,
     "status": "ok",
     "timestamp": 1737666189508,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "dwHyjZLgI56W",
    "outputId": "c07ede7a-3d72-4ec6-94b6-79882eb77001"
   },
   "outputs": [],
   "source": [
    "# Transfer Learning Configuration Results\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f'  {BOLD_START}VGG16 Model{BOLD_END}')\n",
    "loss, accuracy, precision, recall = modelTransfer.evaluate(test_it, steps=len(test_it), verbose=0)\n",
    "rprint(f'  Loss > {loss: .2f}')\n",
    "rprint(f'  Accuracy > {accuracy: .2%}')\n",
    "print(\"\\n\\n\")\n",
    "summarize_diagnostics_seaborn(historyTransfer, \"VGG16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA9qFriF7bnh"
   },
   "source": [
    "## Inference\n",
    "\n",
    "### Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4910,
     "status": "ok",
     "timestamp": 1737666194409,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "pfEyJlYgCRiT",
    "outputId": "9a4d4481-6e56-41a9-ea3e-ebbd419e5b4d"
   },
   "outputs": [],
   "source": [
    "print(f\"{BOLD_START}Loading saved model definitions and weights.{BOLD_END}\")\n",
    "try:\n",
    "    best_model1 = load_model('./best_model1.keras')\n",
    "    best_model2 = load_model('./best_model2.keras')\n",
    "    best_model3 = load_model('./best_model3.keras')\n",
    "    best_modelVGG16 = load_model('./best_modelVGG16.keras')\n",
    "except Exception as e:\n",
    "    process_exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0-mE5CGrVUr"
   },
   "source": [
    "### Execute your first Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "executionInfo": {
     "elapsed": 3551,
     "status": "ok",
     "timestamp": 1737666197938,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "sJPdM40r7dA1",
    "outputId": "8bcf50f9-1737-4a45-da29-1428ce0c2c07"
   },
   "outputs": [],
   "source": [
    "# load and prepare the input image for the newly trained models, simulates your input in the field\n",
    "def load_image(img):\n",
    "\n",
    "  #resize the image\n",
    "  newsize = (IMG_HEIGHT, IMG_WIDTH)\n",
    "  img1 = img.resize(newsize)\n",
    "\n",
    "  # convert to array\n",
    "  img2 = img_to_array(img1)\n",
    "\n",
    "  # reshape into a single sample with 3 channels\n",
    "  img = img2.reshape(1, IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "\n",
    "  # ensure datatype\n",
    "  img = img.astype('float32')\n",
    "\n",
    "  # scale image, we did this to the training data, you must scale the inference input!!!\n",
    "  img = img / 255.0\n",
    "  return img\n",
    "\n",
    "# load an image and predict the class\n",
    "def run_example(inc_url, inc_model):\n",
    "\n",
    "  # load the image\n",
    "  img = load_image(inc_url)\n",
    "\n",
    "  # predict the class\n",
    "  result = inc_model.predict(img)\n",
    "  return result\n",
    "\n",
    "rprint(\"Load a sample image.\")\n",
    "filename=\"./folderOnColab/ENTOMOLOGY/validation/alb/001.jpg\"\n",
    "#filename=\"./folderOnColab/ENTOMOLOGY/validation/slf/001.jpg\"\n",
    "img = load_img(filename, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "\n",
    "rprint(\"Obtain an inference from each model.\")\n",
    "try:\n",
    "    results1 = run_example(img, best_model1)\n",
    "    results2 = run_example(img, best_model2)\n",
    "    results3 = run_example(img, best_model3)\n",
    "    resultsVGG16 = run_example(img, best_modelVGG16)\n",
    "except Exception as e:\n",
    "    process_exception(e)\n",
    "\n",
    "the_logo = PIL_Image.open(filename)\n",
    "\n",
    "#turn the image into a displayed graphic\n",
    "plt.figure()\n",
    "plt.title(\"Image of first inference test\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.axis('off')\n",
    "plt.imshow(the_logo)\n",
    "plt.show()\n",
    "\n",
    "print(\"\")\n",
    "rprint(\"What's in the image?  Share with the class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1737666197938,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "QZiWxJsWFQWB",
    "outputId": "03524c90-e45c-40f8-8fc5-5675c4138d2e"
   },
   "outputs": [],
   "source": [
    "start=BOLD_START\n",
    "end=BOLD_END\n",
    "\n",
    "print(f\"{start}Evaluate the Models{end}\")\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#model 1\n",
    "try:\n",
    "  class_names = categories\n",
    "  predicted_class_index = np.argmax(results1)\n",
    "  predicted_class = class_names[predicted_class_index]\n",
    "  print(f'  {start}1 Layer Model{end}')\n",
    "  print(f'{start}{predicted_class}{end}> {float(results1[0][predicted_class_index]):.2%}')\n",
    "  print(f'{start}{predicted_class}{end}> {results1}')\n",
    "except Exception as e:\n",
    "    process_exception(e)\n",
    "print('\\n')\n",
    "\n",
    "#model 2\n",
    "try:\n",
    "  class_names = categories\n",
    "  predicted_class_index = np.argmax(results2)\n",
    "  predicted_class = class_names[predicted_class_index]\n",
    "  print(\"\\n\")\n",
    "  print(f'  {start}2 Layer Model{end}')\n",
    "  print(f'{start}{predicted_class}{end}> {float(results2[0][predicted_class_index]):.2%}')\n",
    "  print(f'{start}{predicted_class}{end}> {results2}')\n",
    "except Exception as e:\n",
    "    process_exception(e)\n",
    "print('\\n')\n",
    "\n",
    "#model 3\n",
    "try:\n",
    "  class_names = categories\n",
    "  predicted_class_index = np.argmax(results3)\n",
    "  predicted_class = class_names[predicted_class_index]\n",
    "  print(\"\\n\")\n",
    "  print(f'  {start}3 Layer Model{end}')\n",
    "  print(f'{start}{predicted_class}{end}> {float(results3[0][predicted_class_index]):.2%}')\n",
    "  print(f'{start}{predicted_class}{end}> {results3}')\n",
    "except Exception as e:\n",
    "  process_exception(e)\n",
    "print('\\n')\n",
    "\n",
    "#VGG16\n",
    "try:\n",
    "  class_names = categories\n",
    "  predicted_class_index = np.argmax(resultsVGG16)\n",
    "  predicted_class = class_names[predicted_class_index]\n",
    "  print(\"\\n\")\n",
    "  print(f'  {start}VGG16 Model{end}')\n",
    "  print(f'{start}{predicted_class}{end}> {float(resultsVGG16[0][predicted_class_index]):.2%}')\n",
    "  print(f'{start}{predicted_class}{end}> {resultsVGG16}')\n",
    "except Exception as e:\n",
    "  process_exception(e)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ino8BmE7fdhJ"
   },
   "source": [
    "## Inference through Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1737666197939,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "Xm1t3aZtbKY9",
    "outputId": "62011ed5-248f-402b-c764-4bf786f09eca"
   },
   "outputs": [],
   "source": [
    "# get a list of all validation files\n",
    "import os\n",
    "import random\n",
    "\n",
    "alb_files=os.listdir(f\"{WORKING_FOLDER}/ENTOMOLOGY/validation/alb\")\n",
    "slf_files=os.listdir(f\"{WORKING_FOLDER}/ENTOMOLOGY/validation/slf\")\n",
    "\n",
    "# divisor, cut down just how many images we show\n",
    "divisor=3\n",
    "\n",
    "# determine how many you have\n",
    "alb_num=len(alb_files)\n",
    "slf_num=len(slf_files)\n",
    "rprint(f\"There are {alb_num} files in the Asian Longhorn Bettle domain.\")\n",
    "rprint(f\"There are {slf_num} files in the Spotted Lantern Fly domain.\")\n",
    "\n",
    "minimal_count = int(min([alb_num, slf_num])/divisor)  #let's not make too many images\n",
    "rprint(f\"There are {minimal_count} total files you can work with in either folder.\")\n",
    "rprint(f\"Creating random list of files to choose from.\")\n",
    "\n",
    "# build an array to hold the randomly created names that represent our data we're going to inference\n",
    "y_true_filenames=[]\n",
    "y_true = []\n",
    "y_trueNum=[]\n",
    "for idx in range(0,minimal_count):\n",
    "  category_idx=int(random.randrange(0, 2))\n",
    "  file_idx=int(random.randrange(0,minimal_count))\n",
    "  if category_idx==0:\n",
    "    category=\"alb\"\n",
    "  else:\n",
    "    category=\"slf\"\n",
    "  if file_idx < 10:\n",
    "    filename=\"00\"\n",
    "  else:\n",
    "    filename=\"0\"\n",
    "  the_filename=f\"{WORKING_FOLDER}/ENTOMOLOGY/validation/{category}/{filename}{file_idx}.jpg\"\n",
    "  y_true_filenames.append(the_filename)\n",
    "\n",
    "  y_true.append(class_names[category_idx])\n",
    "  y_trueNum.append(category_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8189,
     "status": "ok",
     "timestamp": 1737666206119,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "VoOka4yfWmYs",
    "outputId": "1d366614-d8ad-45bb-9e9d-01c710d1a0ed"
   },
   "outputs": [],
   "source": [
    "# create arrays to hold the results, think of arrays as a carton is to eggs\n",
    "y_pred_Model1 = []\n",
    "y_pred_Model2 = []\n",
    "y_pred_Model3 = []\n",
    "y_pred_VGG16 = []\n",
    "\n",
    "y_predNum_Model1 = []\n",
    "y_predNum_Model2 = []\n",
    "y_predNum_Model3 = []\n",
    "y_predNum_VGG16 = []\n",
    "\n",
    "class_names = categories\n",
    "\n",
    "#now iterate through each image, mark the identification, gather the results and produce a confusion matrix to show true quality of the image model\n",
    "for idx, target_filename, in enumerate(y_true_filenames):\n",
    "    img = load_img(target_filename, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    try:\n",
    "        results1=run_example(img, best_model1)\n",
    "    except Exception as e:\n",
    "        print(f\"Model 1 had issues with inference, see: {str(repr(e))}\")\n",
    "        process_exception(e)\n",
    "\n",
    "    try:\n",
    "        results2=run_example(img, best_model2)\n",
    "    except Exception as e:\n",
    "        print(f\"Model 2 had issues with inference, see: {str(repr(e))}\")\n",
    "        process_exception(e)\n",
    "\n",
    "    try:\n",
    "        results3=run_example(img, best_model3)\n",
    "    except Exception as e:\n",
    "        print(f\"Model 3 had issues with inference, see: {str(repr(e))}\")\n",
    "        process_exception(e)\n",
    "\n",
    "    try:\n",
    "        resultsVGG16 = run_example(img, best_modelVGG16)\n",
    "    except Exception as e:\n",
    "        print(f\"Model VGG16 had issues with inference, see: {str(repr(e))}\")\n",
    "        process_exception(e)\n",
    "\n",
    "    rprint(f\"TRUTH == {y_true[idx]}\")\n",
    "\n",
    "    try:\n",
    "        predicted_class_index = np.argmax(results1)\n",
    "        y_predNum_Model1.append( predicted_class_index )\n",
    "        predicted_class = class_names[predicted_class_index]\n",
    "        y_pred_Model1.append( predicted_class )\n",
    "        print(f\"Iteration[{idx}] = Model#1 {start}Predicted class:{end}: {predicted_class}[{predicted_class_index}], {resultsVGG16[0][predicted_class_index]:.1%} \")\n",
    "    except Exception as e:\n",
    "        print(f\"Model 1 had issues, see: {str(repr(e))}\")\n",
    "        process_exception(e)\n",
    "\n",
    "\n",
    "    try:\n",
    "        predicted_class_index = np.argmax(results2)\n",
    "        y_predNum_Model2.append( predicted_class_index )\n",
    "        predicted_class = class_names[predicted_class_index]\n",
    "        y_pred_Model2.append( predicted_class )\n",
    "        print(f\"Iteration[{idx}] = Model#1 {start}Predicted class:{end}: {predicted_class}[{predicted_class_index}], {results2[0][predicted_class_index]:.1%} \")\n",
    "    except Exception as e:\n",
    "        print(f\"Model 2 had issues, see: {str(repr(e))}\")\n",
    "        process_exception(e)\n",
    "\n",
    "    try:\n",
    "        predicted_class_index = np.argmax(results3)\n",
    "        y_predNum_Model3.append( predicted_class_index )\n",
    "        predicted_class = class_names[predicted_class_index]\n",
    "        y_pred_Model3.append( predicted_class )\n",
    "        print(f\"Iteration[{idx}] = Model#1 {start}Predicted class:{end}: {predicted_class}[{predicted_class_index}], {results3[0][predicted_class_index]:.1%} \")\n",
    "    except Exception as e:\n",
    "        print(f\"Model 3 had issues, see: {str(repr(e))}\")\n",
    "        process_exception(e)\n",
    "\n",
    "    try:\n",
    "        predicted_class_index = np.argmax(resultsVGG16)\n",
    "        y_predNum_VGG16.append( predicted_class_index )\n",
    "        predicted_class = class_names[predicted_class_index]\n",
    "        y_pred_VGG16.append( predicted_class )\n",
    "        print(f\"Iteration[{idx}] = VGG16 {start}Predicted class:{end}: {predicted_class}[{predicted_class_index}], {resultsVGG16[0][predicted_class_index]:.1%} \")\n",
    "    except Exception as e:\n",
    "        print(f\"Model VGG16 had issues, see: {str(repr(e))}\")\n",
    "        process_exception(e)\n",
    "\n",
    "    the_logo = PIL_Image.open(target_filename)\n",
    "    #turn the image into a displayed graphic\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(the_logo)\n",
    "    plt.show()\n",
    "    print(\"#######################################################################################\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "the_model_names=['Model 1', 'Model 2', 'Model 3', 'VGG16']\n",
    "the_models=[]\n",
    "the_models.append(y_pred_Model1)\n",
    "the_models.append(y_pred_Model2)\n",
    "the_models.append(y_pred_Model3)\n",
    "the_models.append(y_pred_VGG16)\n",
    "the_num_models=[]\n",
    "the_num_models.append(y_predNum_Model1)\n",
    "the_num_models.append(y_predNum_Model2)\n",
    "the_num_models.append(y_predNum_Model3)\n",
    "the_num_models.append(y_predNum_VGG16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1aeHbpkWao9"
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix is a 2 dimensional array comparing predicted category labels to the true label. For binary classification, these are the True Positive, True Negative, False Positive and False Negative categories.\n",
    "\n",
    "*The probability score is not reality, or ground truth.* There are four possible outcomes for each output from a binary classifier. For an email spam classifier example, if you lay out the ground truth as columns and the model's prediction as rows:\n",
    "\n",
    "+ **True positive (TP):** A spam email correctly classified as a spam email. These are the spam messages automatically sent to the spam folder.\n",
    "\n",
    "+ **False positive (FP):** A not-spam email misclassified as spam. These are the legitimate emails that wind up in the spam folder.\n",
    "\n",
    "+ **False negative (FN):** A spam email misclassified as not-spam. These are spam emails that aren't caught by the spam filter and make their way into the inbox.\n",
    "\n",
    "+ **True negative (TN):** A not-spam email correctly classified as not-spam. These are the legitimate emails that are sent directly to the inbox.\n",
    "\n",
    "#### Choice of metric and tradeoffs\n",
    "\n",
    "The metric(s) you choose to prioritize when evaluating the model and choosing a threshold depend on the costs, benefits, and risks of the specific problem.\n",
    "\n",
    "Metric Guidance\n",
    "\n",
    "**Accuracy**\n",
    "\n",
    "+ Use as a rough indicator of model training progress/convergence for balanced datasets.\n",
    "\n",
    "+ For model performance, use only in combination with other metrics.\n",
    "\n",
    "+ Avoid for imbalanced datasets. Consider using another metric.\n",
    "\n",
    "**Recall**\n",
    "\n",
    "+ (True positive rate) - Use when false negatives are more expensive than false positives.\n",
    "\n",
    "+ False positive rate - Use when false positives are more expensive than false negatives.\n",
    "\n",
    "**Precision**\n",
    "\n",
    "+ Use when it's very important for positive predictions to be accurate.\n",
    "\n",
    "\n",
    "#### References:\n",
    "\n",
    "+ https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall\n",
    "+ https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1737666206120,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "Qs3aNFNmrVUs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_confusion_classification_output(inc_cm,\n",
    "                                         inc_cr : str,\n",
    "                                         inc_model_name : str,\n",
    "                                         inc_labels : []\n",
    "                                        ) -> None:\n",
    "\n",
    "    cf=inc_cm\n",
    "    plt.rcParams['figure.figsize'] = (FIGURE_WIDTH/1.5, FIGURE_HEIGHT/1.5)\n",
    "    plt.xticks(fontsize=IMG_FONT_SIZE_MIN * 0.75)\n",
    "    plt.yticks(fontsize=IMG_FONT_SIZE_MIN * 0.75)\n",
    "    plt.title(f'Confusion Matrix for Invasive Species - {inc_model_name}', fontsize=IMG_FONT_SIZE_MIN * 1.25)\n",
    "\n",
    "    group_names= inc_labels\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cf.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf,annot=labels,fmt=\"\",cmap='Blues',cbar=True,xticklabels=categories,yticklabels=categories,annot_kws={\"size\": IMG_FONT_SIZE_MIN})\n",
    "    caption_text=f\"\"\"Confusion Matrix, for: {inc_model_name} with {minimal_count} random samples from the validation dataset with the following metrics: \\n{inc_cr}\"\"\"\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # Saving plot\n",
    "    target_filename=f\"\"\"{WORKING_FOLDER}/STEM-004_ConfusionMatrix_{inc_model_name}{IMG_EXT}\"\"\"\n",
    "    plt.savefig(target_filename, transparent=False, dpi=150);\n",
    "    add_alt_text(target_filename, caption_text);\n",
    "    plt.close()\n",
    "\n",
    "    #Now show the image\n",
    "    image = mpimg.imread(target_filename)\n",
    "    # Display the image\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    #make_accessible(plt, jinja2.Template(caption_text), len=len)\n",
    "    rprint(caption_text)\n",
    "    print(\"################################################################################\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2172,
     "status": "ok",
     "timestamp": 1737666208276,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "aFmxY0g7fnuQ",
    "outputId": "29c9ea4e-aa9c-4b4c-d497-bd9904a06166"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve\n",
    "\n",
    "#build the confusion matrices\n",
    "cms={}\n",
    "cr={}\n",
    "group_names = [\"True Pos\",\"False Pos\",\"False Neg\",\"True Neg\"]\n",
    "\n",
    "for idx, the_model in enumerate(the_models):\n",
    "    rprint(f\"Processing {the_model_names[idx]}:\")\n",
    "    try:\n",
    "        rprint(\"...calculating confusion matrix.\")\n",
    "        cms[the_model_names[idx]] = confusion_matrix(y_true,the_model, labels=class_names)\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "\n",
    "    try:\n",
    "        rprint(\"...calculating classification report.\" )\n",
    "        cr[the_model_names[idx]]=classification_report(y_true, the_model)\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "\n",
    "    rprint(\"...creating confusion matrix graph.\")\n",
    "\n",
    "    make_confusion_classification_output(cms[the_model_names[idx]], cr[the_model_names[idx]], the_model_names[idx], group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClwYZuSPrVUs"
   },
   "source": [
    "### Receiver Operating Characteristic (ROC) / Area Under the Curve (AUC)\n",
    "\n",
    "ROC stands for Receiver Operating Characteristic, and it's a curve that shows how well a machine learning model performs at different classification thresholds. ROC curves are a key tool in machine learning and predictive analytics.\n",
    "\n",
    "In general, an AUC of 0.5 suggests no discrimination (i.e., ability to diagnose patients with and without the disease or condition based on the test), 0.7 to 0.8 is considered acceptable, 0.8 to 0.9 is considered excellent, and more than 0.9 is considered outstanding.\n",
    "\n",
    "How ROC curves work\n",
    "\n",
    "+ True positive rate (TPR) - The proportion of actual positives that the model correctly identifies. TPR is also known as sensitivity or recall.\n",
    "+ False positive rate (FPR) - The proportion of actual negatives that the model incorrectly identifies as positives. FPR is also known as the fall-out or probability of false alarm.\n",
    "\n",
    "Area under the curve (AUC) is the overall performance of the classifier, which is calculated by measuring the area under the ROC curve. A larger AUC indicates a better classifier. The area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative.  AUC is a number between 0.0 and 1.0 representing a binary classification model's ability to separate positive classes from negative classes. The closer the AUC is to 1.0, the better the model's ability to separate classes from each other.\n",
    "\n",
    "\n",
    "Why ROC curves are important\n",
    "\n",
    "+ ROC curves are useful for comparing the performance of different classifiers.\n",
    "+ The initial slope of the ROC curve shows how quickly the performance degrades.\n",
    "+ ROC curves are a fundamental tool for evaluating the quality of classifier output.\n",
    "\n",
    "\n",
    "\n",
    "### References:\n",
    "\n",
    "+ https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "+ https://medium.com/@shaileydash/understanding-the-roc-and-auc-intuitively-31ca96445c02\n",
    "+ https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1956,
     "status": "ok",
     "timestamp": 1737666210221,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "FzwUCfBrrVUs",
    "outputId": "b945a066-8d6c-471b-9aef-8aa6303e47d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics, model_selection, svm\n",
    "\n",
    "for idx, the_model in enumerate(the_num_models):\n",
    "    rprint(f\"Processing {the_model_names[idx]}:\")\n",
    "    try:\n",
    "        rprint(\"...calculating ROC/AUC.\")\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_trueNum, the_model,)\n",
    "        roc_auc=metrics.auc(fpr, tpr)\n",
    "        display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=the_model_names[idx], )\n",
    "        display.plot()\n",
    "        plt.rcParams['figure.figsize'] = (FIGURE_WIDTH/1.5, FIGURE_HEIGHT/1.5)\n",
    "        plt.xticks(fontsize=IMG_FONT_SIZE_MIN * 0.5)\n",
    "        plt.yticks(fontsize=IMG_FONT_SIZE_MIN * 0.5)\n",
    "        plt.title(f'ROC - {the_model_names[idx]}', fontsize=IMG_FONT_SIZE_MIN * 1.25)\n",
    "        caption_text=f\"\"\"ROC, for: {the_model_names[idx]} with {minimal_count} random samples from the validation dataset.\"\"\"\n",
    "\n",
    "        # Saving plot\n",
    "        target_filename=f\"\"\"{WORKING_FOLDER}/STEM-004_ROC_{the_model_names[idx]}{IMG_EXT}\"\"\"\n",
    "        plt.savefig(target_filename, transparent=False, dpi=150);\n",
    "        add_alt_text(target_filename, caption_text);\n",
    "        plt.close()\n",
    "\n",
    "        #Now show the image\n",
    "        image = mpimg.imread(target_filename)\n",
    "        # Display the image\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        rprint(caption_text)\n",
    "        print(\"################################################################################\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2401,
     "status": "ok",
     "timestamp": 1737666212617,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "luR_YlDqRVaW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    #best_model1, best_model2, best_model3, best_modelTransfer\n",
    "    target_model='best_model1'\n",
    "    target_filename=f'{WORKING_FOLDER}/STEM-005_{target_model}{IMG_EXT}'\n",
    "    tf.keras.utils.plot_model(\n",
    "        load_model(target_model+'.keras'),\n",
    "        to_file=target_filename,\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        rankdir=\"TB\",\n",
    "        expand_nested=True,\n",
    "        dpi=96,\n",
    "    )\n",
    "except Exception as e:\n",
    "    process_exception(e)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  img=PIL_Image.open(target_filename)\n",
    "  img.show()\n",
    "except Exception as e:\n",
    "    process_exception(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8r0DsMDrVUs"
   },
   "source": [
    "# Advanced Class\n",
    "\n",
    "Exercise the following requirements:\n",
    "\n",
    "1. Utilize a configuration management schema to hold your hyper-parameters.\n",
    "2. Utilize a hyper-parameter search for a more optimally performing model.\n",
    "3. Updating the dataset is another acceptable mechanism for model improvement.\n",
    "\n",
    "\n",
    "## Tips for Hyperparameter Optimization\n",
    "\n",
    "This section lists some handy tips to consider when tuning hyperparameters of your neural network.\n",
    "\n",
    "+ k-fold Cross Validation. You can see that the results from the examples in this post show some variance. A default cross-validation of 3 was used, but perhaps k=5 or k=10 would be more stable. Carefully choose your cross validation configuration to ensure your results are stable.\n",
    "\n",
    "+ Review the Whole Grid. Do not just focus on the best result, review the whole grid of results and look for trends to support configuration decisions.\n",
    "\n",
    "+ Parallelize. Use all your cores if you can, neural networks are slow to train and we often want to try a lot of different parameters. Consider spinning up a lot of AWS instances.\n",
    "\n",
    "+ Use a Sample of Your Dataset. Because networks are slow to train, try training them on a smaller sample of your training dataset, just to get an idea of general directions of parameters rather than optimal configurations.\n",
    "\n",
    "+ Start with Coarse Grids. Start with coarse-grained grids and zoom into finer grained grids once you can narrow the scope.\n",
    "\n",
    "+ Do not Transfer Results. Results are generally problem specific. Try to avoid favorite configurations on each new problem that you see. It is unlikely that optimal results you discover on one problem will transfer to your next project. Instead look for broader trends like number of layers or relationships between parameters.\n",
    "\n",
    "+ Reproducibility is a Problem. Although we set the seed for the random number generator in NumPy, the results are not 100% reproducible. There is more to reproducibility when grid searching wrapped Keras models than is presented in this post.\n",
    "\n",
    "Why do most CNN models not apply the cross-validation technique?\n",
    "\n",
    "K-fold cross-validation is often used for simple models with few parameters, models with more basic hyperparameters and the models are easy to optimize. Some examples of simple models are: linear regression, logistic regression, small neural networks, support vector machines, etc... For a CNN, due to its many parameters ther are too many possible changes in the architecture. Normally you experiment with the learning rate, batch size, dropout (amount and position) and batch normalization (position). Training a CNN with a sufficiently large dataset can take quite a long time.\n",
    "\n",
    "#### Reference:\n",
    "\n",
    "+ https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "+ https://medium.com/@AIandInsights/hyperparameter-tuning-with-keras-and-gridsearchcv-a-comprehensive-guide-46214cc0d999\n",
    "+ https://medium.com/@sengupta.joy4u/how-to-decide-the-hyperparameters-in-cnn-bfa37b608046\n",
    "+ https://keras.io/keras_tuner/api/tuners/grid/\n",
    "+ https://www.kaggle.com/code/pramodiasuka/cnn-model-with-kfold-cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1737666212618,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "W-bdIyO-rVUs",
    "outputId": "f58bffa6-9422-4da4-9805-a247176f1000",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ugly attempt a parameterization of hyper-parameters using looping structure\n",
    "# Should use Keras Classification Search but data will have to be transformed into numpy arrays\n",
    "# Results aren't being saved either so... who did the best???\n",
    "\"\"\"\n",
    "# Define model function\n",
    "def create_model(inc_optimizer='adam', inc_activation='relu'):\n",
    "\n",
    " #transfer learning (VGG16)\n",
    "  model = VGG16(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "  # mark loaded layers as un-trainable, keep the original weights\n",
    "  for layer in model.layers:\n",
    "      layer.trainable = False\n",
    "\n",
    "  # add new classifier layers\n",
    "  flat1 = Flatten()(model.layers[-1].output)\n",
    "  class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\n",
    "  #this layer defines the classification, outputs much match total labels\n",
    "  output = Dense(2, activation=inc_activation)(class1)\n",
    "\n",
    "  # define new model\n",
    "  model = Model(inputs=model.inputs, outputs=output)\n",
    "\n",
    "  # compile model\n",
    "  #model.compile(optimizer=\"adam\", loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
    "  model.compile(optimizer=inc_optimizer, loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'activation': ['relu', 'sigmoid', 'tanh'],\n",
    "    'batch_size': [8, 16, 32, 54, 128],\n",
    "}\n",
    "\n",
    "# Create KerasClassifier\n",
    "\n",
    "try:\n",
    "  with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "      for idx, optimizer in enumerate(param_grid['optimizer']):\n",
    "            for idx2, activation in enumerate(param_grid['activation']):\n",
    "                for idx2, batch_size in enumerate(param_grid['batch_size']):\n",
    "                    model=create_model(inc_optimizer=optimizer, inc_activation=activation)\n",
    "                    rprint(f\"Optimizer: {optimizer:10} - Activation Function: {activation:10} - Batch Size: {batch_size}\")\n",
    "                    try:\n",
    "                        mc_callback = ModelCheckpoint(f\"best_VGG16_{optimizer}_{activation}_{batch_size}.keras\", monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "                        callbacks_list = [PlotLearning(), es_callback, mc_callback, tensorboard_callback]\n",
    "                        history1 = model1.fit(train_it,\n",
    "                                              batch_size=batch_size,\n",
    "                                              epochs=EPOCHS,\n",
    "                                              callbacks=callbacks_list,\n",
    "                                              validation_data=test_it,\n",
    "                                              validation_split=VALIDATION_SPLIT,\n",
    "                                              verbose=2,                     #0 silent, 1 more, 2 max\n",
    "                                             )\n",
    "                    except Exception as inner_exception:\n",
    "                        process_exception(inner_exception)\n",
    "\n",
    "except Exception as e:\n",
    "    process_exception(e)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "STEM-004_ComputerVision.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "033cb5a8c21c4ca5a13b5d03e8b78fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebb9ca01cf52473699f21f15c5ec7d34",
      "placeholder": "",
      "style": "IPY_MODEL_689fe090492947608518efc9f3bd6d5d",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n          "
     }
    },
    "0d8988d435644a928829ef71435a3e83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1015b1ce380943d48c7a6386444c768a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f5405303114146a28663cb1c46db7beb",
       "IPY_MODEL_bdc74a6d20a041819b6fc6d1c3f85a25",
       "IPY_MODEL_033cb5a8c21c4ca5a13b5d03e8b78fa0"
      ],
      "layout": "IPY_MODEL_2b447186de7a4571821ca59295744ce3"
     }
    },
    "202c8798fdda4778bf09a2124c37a6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "usfs-ai-bootcamp",
       "usfa-ai-advanced-training",
       "I will setup my own"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Set Your Project:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_dd3de71c792f4762af3b280ac9b7f5e6",
      "style": "IPY_MODEL_fc3fc48bdebe4501998819fbda36152c"
     }
    },
    "2b447186de7a4571821ca59295744ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "31b012d7d41a4d06933aa38e8e3bd74a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "479233081fbc44c8afe9a044ebb95faf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "689fe090492947608518efc9f3bd6d5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad3b9f4855d541e49303c03fb9f07db7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Accept",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_f196621910824a00a97d06e5433cb294",
      "style": "IPY_MODEL_31b012d7d41a4d06933aa38e8e3bd74a",
      "tooltip": ""
     }
    },
    "bdc74a6d20a041819b6fc6d1c3f85a25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_202c8798fdda4778bf09a2124c37a6c7",
       "IPY_MODEL_ad3b9f4855d541e49303c03fb9f07db7"
      ],
      "layout": "IPY_MODEL_0d8988d435644a928829ef71435a3e83"
     }
    },
    "cf363ab273bf4eeba7fbea4d79ed3a64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd3de71c792f4762af3b280ac9b7f5e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebb9ca01cf52473699f21f15c5ec7d34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f196621910824a00a97d06e5433cb294": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5405303114146a28663cb1c46db7beb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf363ab273bf4eeba7fbea4d79ed3a64",
      "placeholder": "",
      "style": "IPY_MODEL_479233081fbc44c8afe9a044ebb95faf",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n\n        <table><tr><td>\n            <span style=\"font-family: Tahoma;font-size: 18\">\n              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n              Please verify that you are in the appropriate project and that the:</br>\n              <center><code><b>PROJECT_ID</b></code> </br></center>\n              aligns with the Project Id in the upper left corner of this browser and that the location:\n              <center><code><b>LOCATION</b></code> </br></center>\n              aligns with the instructions provided.\n            </span>\n          </td></tr></table></br></br>\n\n    "
     }
    },
    "fc3fc48bdebe4501998819fbda36152c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
