{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jg3iJooMQjWA"
   },
   "source": [
    "# Artificial Intelligence Prompt Engineering\n",
    "## Generative AI (GenAI) - 003\n",
    "\n",
    "<center>\n",
    "<table align=\"center\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/christophergarthwood/jbooks/blob/main/STEM-003_GenAI_Prompts.ipynb\">\n",
    "      <img src=\"./img/GoogleColab-logo.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/notebooks?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Link to Colab Enterprise\n",
    "    </a>\n",
    "  </td>   \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/christophergarthwood/jbooks/blob/main/STEM-003-GenAI_Prompts.ipynb\">\n",
    "      <img src=\"./img/GitHub-logo.jpg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Link to Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "</center>\n",
    "</br></br></br>\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Christopher G Wood](https://github.com/christophergarthwood)  |\n",
    "\n",
    "# Overview\n",
    "\n",
    "### A Newer Hope? Spotted Lantern Flies?  Asian Longhorn Beetles?\n",
    "\n",
    "**Generative artificial intelligence** (generative AI, GenAI, or GAI) refers to artificial intelligence systems capable of creating original content in various forms, such as text, images, videos, or even software code.\n",
    "\n",
    "+ These systems operate using generative models, which learn patterns and structures from their input training data and then generate new data with similar characteristics. The advancements in transformer-based deep neural networks, particularly large language models (LLMs).\n",
    "+ Prompt engineering is the process of structuring an instruction that can be interpreted and understood by a generative AI model. In other words, a prompt is natural language text describing the task that an AI should perform.\n",
    "+ Understanding how to make a prompt work for you is an important skill.\n",
    "\n",
    "### References:\n",
    "\n",
    "+ https://realpython.com/practical-prompt-engineering/\n",
    "+ https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values\n",
    "+ https://cloud.google.com/vertex-ai/generative-ai/docs/prompt-gallery\n",
    "+ https://python.langchain.com/v0.1/docs/modules/model_io/prompts/partial/\n",
    "+ https://www.promptingguide.ai/risks/adversarial#defense-tactics\n",
    "+ https://developers.google.com/machine-learning/resources/prompt-eng\n",
    "+ https://builtin.com/artificial-intelligence/prompt-engineering\n",
    "\n",
    "### Google References to their LLM\n",
    "+ https://cloud.google.com/vertex-ai/generative-ai/docs/samples/generativeaionvertexai-non-stream-text-basic#generativeaionvertexai_non_stream_text_basic-python\n",
    "+ https://cloud.google.com/vertex-ai/generative-ai/docs/samples/generativeaionvertexai-gemini-pro-config-example\n",
    "+ https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes\n",
    "\n",
    "### Good Resources to Investigate\n",
    "+ https://gandalf.lakera.ai/intro\n",
    "+ https://labs.google\n",
    "+ https://artsandculture.google.com/experiment/say-what-you-see/jwG3m7wQShZngw\n",
    "\n",
    "### Supporting Developers (Special Thanks)\n",
    "+ Andy Staton\n",
    "+ Carlos Ramirez\n",
    "+ Joel Thompson\n",
    "\n",
    "### Reference for Image Generators:\n",
    "\n",
    "+ https://gemini.google.com/app\n",
    "  + https://cloud.google.com/vertex-ai/generative-ai/docs/image/img-gen-prompt-guide\n",
    "  + https://tech.co/news/use-google-bard-ai-image-generator\n",
    "+ https://www.midjourney.com/explore?tab=top\n",
    "+ https://openai.com/index/dall-e-2/\n",
    "+ https://builtin.com/artificial-intelligence/prompt-engineering\n",
    "+ https://www.altexsoft.com/blog/ai-image-generation/\n",
    "+ https://flux-ai.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1754338742989,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "SwR8grWR_qd7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's define some variables (information holders) for our project overall\n",
    "\n",
    "global PROJECT_ID, BUCKET_NAME, LOCATION\n",
    "BUCKET_NAME =\"jbooks_ai_ml_public\"\n",
    "PROJECT_ID  =\"testproject-366516\"\n",
    "LOCATION    = \"us-central1\"\n",
    "\n",
    "BOLD_START=\"\\033[1m\"\n",
    "BOLD_END=\"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508,
     "referenced_widgets": [
      "5a4fddeb7a3348b19664225987e38c9e",
      "ca140534795d4c70ac2f72cd0125c3d3",
      "82db78c8173c42c0a70513c759ee9c10",
      "590a213cabbd476f8ffcb6bdc797e555",
      "9cf4e444dd674263a950603f66e23c2f",
      "69b409c1fe874ad282e85a8771b0d9c9",
      "1b2db77f14174423b81bed435b0499e1",
      "2c6496043f1948b9b6e4b34c08f6e861",
      "013aadf6843a4f4cb0dcca7d4e9d3a04",
      "4f28547cd23c4863b4a24c1c9cb57717",
      "1efe2a6925ee4f159747f6e1f3334622",
      "87738fe8c2054029a7e1853d48910122",
      "33b4f197d405481aa89b326886e74502",
      "40faa4f636be4e96b9feea5ff491a10f",
      "7e3c806857ee4327bbcd5304b007d6c1",
      "0a2b4baae02c4a2591717a5327d0f6fb"
     ]
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1754338245897,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "peVxsWd11YLw",
    "outputId": "ca831490-b243-485a-a069-1b5d7dd9912f"
   },
   "outputs": [],
   "source": [
    "# Now create a means of enforcing project id selection\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def wait_for_button_press():\n",
    "\n",
    "    button_pressed = False\n",
    "\n",
    "    # Create widgets\n",
    "    html_widget = widgets.HTML(\n",
    "\n",
    "    value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "\n",
    "        <table><tr><td>\n",
    "            <span style=\"font-family: Tahoma;font-size: 18\">\n",
    "              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n",
    "              Please verify that you are in the appropriate project and that the:</br>\n",
    "              <center><code><b>PROJECT_ID</b></code> </br></center>\n",
    "              aligns with the Project Id in the upper left corner of this browser and that the location:\n",
    "              <center><code><b>LOCATION</b></code> </br></center>\n",
    "              aligns with the instructions provided.\n",
    "            </span>\n",
    "          </td></tr></table></br></br>\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    project_list=[\"ai-bootcamp\", \"ai-advanced-training\", \"I will setup my own\"]\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=project_list,\n",
    "        value=project_list[0],\n",
    "        description='Set Your Project:',\n",
    "    )\n",
    "\n",
    "    html_widget2 = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "          \"\"\")\n",
    "\n",
    "    button = widgets.Button(description=\"Accept\")\n",
    "\n",
    "    # Function to handle the selection change\n",
    "    def on_change(change):\n",
    "        global PROJECT_ID\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            #print(\"Selected option:\", change['new'])\n",
    "            PROJECT_ID=change['new']\n",
    "\n",
    "    # Observe the dropdown for changes\n",
    "    dropdown.observe(on_change)\n",
    "\n",
    "    def on_button_click(b):\n",
    "        nonlocal button_pressed\n",
    "        global PROJECT_ID\n",
    "        button_pressed = True\n",
    "        #button.disabled = True\n",
    "        button.close()  # Remove the button from display\n",
    "        with output:\n",
    "          #print(f\"Button pressed...continuing\")\n",
    "          #print(f\"Selected option: {dropdown.value}\")\n",
    "          PROJECT_ID=dropdown.value\n",
    "\n",
    "    button.on_click(on_button_click)\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Create centered layout\n",
    "    centered_layout = widgets.VBox([\n",
    "                                    html_widget,\n",
    "                                    widgets.HBox([dropdown, button]),\n",
    "                                    html_widget2,\n",
    "    ], layout=widgets.Layout(\n",
    "                              display='flex',\n",
    "                              flex_flow='column',\n",
    "                              align_items='center',\n",
    "                              width='100%'\n",
    "    ))\n",
    "    # Display the layout\n",
    "    display(centered_layout)\n",
    "\n",
    "\n",
    "wait_for_button_press()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5Sx_RHQF18s"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1754338247820,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "shY7a4DVQjWB",
    "outputId": "60120f93-0232-4c1d-97f2-2edb876696a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#- Google Colab Check\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "import datetime\n",
    "\n",
    "RunningInCOLAB = False\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "current_time   = datetime.datetime.now()\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    print(f\"You are running this notebook in Google Colab at {current_time} in the {PROJECT_ID} lab.\")\n",
    "else:\n",
    "    print(f\"You are likely running this notebook with Jupyter iPython runtime at {current_time} in the {PROJECT_ID} lab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NnS27iEF41P"
   },
   "source": [
    "## Library Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1754338250095,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "qlsjoFw5QjWC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import key libraries necessary to support dynamic installation of additional libraries\n",
    "import sys\n",
    "# Use subprocess to support running operating system commands from the program, using the \"bang\" (!)\n",
    "# symbology is supported, however that does not translate to an actual python script, this is a more\n",
    "# agnostic approach.\n",
    "import subprocess\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8556,
     "status": "ok",
     "timestamp": 1754338259276,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "nUugIaSYhSw6",
    "outputId": "6e1f43b5-f29f-4557-ef5e-2f508fc06642",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the libraries you'd like to add to this Runtime environment.\n",
    "libraries=[\"backoff\", \"nltk\", \"bs4\", \"wordcloud\", \"pathlib\", \"numpy\", \"Pillow\", \"pandas\",\n",
    "           \"python-dotenv\", \"seaborn\", \"rich\", \"rich[jupyter]\", \"piexif\", \"PyMuPDF\",\"unidecode\",\n",
    "           \"spacy\", \"watermark\", \"watermark[GPU]\",]\n",
    "\n",
    "# Loop through each library and test for existence, if not present install quietly\n",
    "for library in libraries:\n",
    "    if library == \"Pillow\":\n",
    "      spec = importlib.util.find_spec(\"PIL\")\n",
    "    else:\n",
    "      spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "      print(\"Installing library \" + library)\n",
    "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"], check=True)\n",
    "    else:\n",
    "      print(\"Library \" + library + \" already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPs1WXr-3VST"
   },
   "source": [
    "## Large Language Model (LLM) ~ Gemini Pro Setup (Google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11682,
     "status": "ok",
     "timestamp": 1754338579284,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "MdHif0Q03cyF",
    "outputId": "0db78742-b1b3-4c90-d32c-9af1dce6f085",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Download Google Vextex/AI Libraries\n",
    "subprocess.run([\"pip\", \"install\" , \"--upgrade\", \"google-cloud-aiplatform\", \"--quiet\"], check=True)\n",
    "\n",
    "\n",
    "libraries=[\"google-generativeai\", \"google-cloud-secret-manager\", \"openai\", \"google-genai\"]\n",
    "\n",
    "for library in libraries:\n",
    "    spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "      print(\"Installing library \" + library)\n",
    "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"], check=True)\n",
    "    else:\n",
    "      print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import vertexai.preview\n",
    "from google.cloud import secretmanager\n",
    "import vertexai\n",
    "import openai\n",
    "from google.auth import default, transport\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH83RHSH1uMV"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1623,
     "status": "ok",
     "timestamp": 1754338385467,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "PJuXEPlkSo9p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#- Import additional libraries that add value to the project related to NLP\n",
    "\n",
    "# Beautiful Soup (BS4) is used to parse HTML documents.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Word cloud building library\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#- Set of libraries that perhaps should always be in Python source\n",
    "import backoff\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "import gc\n",
    "import getopt\n",
    "import glob\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "import socket\n",
    "import sys\n",
    "import textwrap\n",
    "import tqdm\n",
    "import traceback\n",
    "import warnings\n",
    "import time\n",
    "from time import perf_counter\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.traceback import install\n",
    "import locale\n",
    "\n",
    "#- Displays system info\n",
    "from watermark import watermark as the_watermark\n",
    "from py3nvml import py3nvml\n",
    "\n",
    "#- Additional libraries for this work\n",
    "import math\n",
    "from base64 import b64decode\n",
    "from IPython.display import Image, Markdown\n",
    "import pandas, IPython.display as display, io, jinja2, base64\n",
    "import requests\n",
    "import unidecode\n",
    "\n",
    "#- Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#- Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.cbook import get_sample_data\n",
    "from matplotlib.offsetbox import (AnnotationBbox, DrawingArea, OffsetImage,\n",
    "                                  TextArea)\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.patches import Circle\n",
    "from PIL import Image as PIL_Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "#- Image meta-data for Section 508 compliance\n",
    "import piexif\n",
    "from piexif.helper import UserComment\n",
    "\n",
    "\n",
    "#- Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2977,
     "status": "ok",
     "timestamp": 1754338391364,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "1vadEsUuQjWD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#- Natural Language Processing (NLP) specific libs\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer  # A word stemmer based on the Porter stemming algorithm.  Porter, M. \"An algorithm for suffix stripping.\" Program 14.3 (1980): 130-137.\n",
    "from nltk import pos_tag\n",
    "from nltk.tree import tree\n",
    "from nltk import FreqDist\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#from nltk.book import * #<- Large Download, only pull if you want raw material to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6TIY8mWMFdX"
   },
   "source": [
    "## Application Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1754338406118,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "nK8ANVg2MIuu"
   },
   "outputs": [],
   "source": [
    "# API Parameters for things like WordCloud, variables help hold information for later use\n",
    "# The \"constants\" represent variables that we don't anticipate changing over the course of the program.\n",
    "IMG_BACKGROUND=\"black\"     #options are black, white, another color or None\n",
    "IMG_FONT_SIZE_MIN=10\n",
    "IMG_WIDTH=1024\n",
    "IMG_HEIGHT=768\n",
    "IMG_INTERP=\"bilinear\"\n",
    "IMG_ALPHA=0.8\n",
    "IMG_ASPECT=\"equal\"\n",
    "FIGURE_WIDTH=11\n",
    "FIGURE_HEIGHT=8.5\n",
    "WORD_FREQ=10\n",
    "\n",
    "# specify how image formats will be saved\n",
    "IMG_EXT=\".jpg\"\n",
    "\n",
    "# used to fully display the error stack, set to 1 if you want to see a ridiculous amount of debugging information\n",
    "DEBUG_STACKTRACE=0\n",
    "\n",
    "# location of our working files\n",
    "WORKING_FOLDER=\"./content/folderOnColab\"\n",
    "\n",
    "# Notebook Author details\n",
    "AUTHOR_NAME=\"Christopher G Wood\"\n",
    "GITHUB_USERNAME=\"christophergarthwood\"\n",
    "AUTHOR_EMAIL=\"christopher.g.wood@gmail.com\"\n",
    "\n",
    "# GenAI\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "TEXT_WIDTH=77\n",
    "IMG_SCALE=0.75\n",
    "\n",
    "# Encoding\n",
    "ENCODING  =\"utf-8\"\n",
    "os.environ['PYTHONIOENCODING']=ENCODING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T-A9h3ZF95W"
   },
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1754338406118,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "QSdZ6g7rF_QF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions are like legos that do one thing, this function outputs library version history of effort.\n",
    "def lib_diagnostics() -> None:\n",
    "\n",
    "    import pkg_resources\n",
    "\n",
    "    package_name_length=20\n",
    "    package_version_length=10\n",
    "\n",
    "    # Show notebook details\n",
    "    #%watermark?\n",
    "    #%watermark --github_username christophergwood --email christopher.g.wood@gmail.com --date --time --iso8601 --updated --python --conda --hostname --machine --githash --gitrepo --gitbranch --iversions --gpu\n",
    "    # Watermark\n",
    "    rprint(the_watermark(author=f\"{AUTHOR_NAME}\", github_username=f\"GITHUB_USERNAME\", email=f\"{AUTHOR_EMAIL}\",iso8601=True, datename=True, current_time=True, python=True, updated=True, hostname=True, machine=True, gitrepo=True, gitbranch=True, githash=True))\n",
    "\n",
    "\n",
    "    print(f\"{BOLD_START}Packages:{BOLD_END}\")\n",
    "    print(\"\")\n",
    "    # Get installed packages\n",
    "    the_packages=[\"nltk\", \"numpy\", \"os\", \"pandas\", \"seaborn\"]\n",
    "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "    for package_idx, package_name in enumerate(installed):\n",
    "         if package_name in the_packages:\n",
    "             installed_version = installed[package_name]\n",
    "             rprint(f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\")\n",
    "\n",
    "    try:\n",
    "        rprint(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
    "        rprint(f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\")\n",
    "        rprint(f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        rprint(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
    "        rprint(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
    "        rprint(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
    "        rprint(f\"{'     current':<40}#: {torch.cuda.current_device()}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "      print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
    "    except Exception as e:\n",
    "      pass\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1754338407260,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "dBXFSMn52K7J"
   },
   "outputs": [],
   "source": [
    "# Routines designed to support adding ALT text to an image generated through Matplotlib.\n",
    "\n",
    "def capture(figure):\n",
    "   buffer = io.BytesIO()\n",
    "   figure.savefig(buffer)\n",
    "   #return F\"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "   return F\"data:image/jpg;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "\n",
    "def make_accessible(figure, template, **kwargs):\n",
    "   return display.Markdown(F\"\"\"![]({capture(figure)} \"{template.render(**globals(), **kwargs)}\")\"\"\")\n",
    "\n",
    "\n",
    "# requires JPG's or TIFFs\n",
    "def add_alt_text(image_path, alt_text):\n",
    "    try:\n",
    "        if os.path.isfile(image_path):\n",
    "          img = PIL_Image.open(image_path)\n",
    "          if \"exif\" in img.info:\n",
    "              exif_dict = piexif.load(img.info[\"exif\"])\n",
    "          else:\n",
    "              exif_dict={}\n",
    "\n",
    "          w, h = img.size\n",
    "          if \"0th\" not in exif_dict:\n",
    "            exif_dict[\"0th\"]={}\n",
    "          exif_dict[\"0th\"][piexif.ImageIFD.XResolution] = (w, 1)\n",
    "          exif_dict[\"0th\"][piexif.ImageIFD.YResolution] = (h, 1)\n",
    "\n",
    "          software_version=\" \".join([\"STEM-001 with Python v\", str(sys.version).split(\" \")[0]])\n",
    "          exif_dict[\"0th\"][piexif.ImageIFD.Software]=software_version.encode(\"utf-8\")\n",
    "\n",
    "          if \"Exif\" not in exif_dict:\n",
    "            exif_dict[\"Exif\"]={}\n",
    "          exif_dict[\"Exif\"][piexif.ExifIFD.UserComment] = UserComment.dump(alt_text, encoding=\"unicode\")\n",
    "\n",
    "          exif_bytes = piexif.dump(exif_dict)\n",
    "          img.save(image_path, \"jpeg\", exif=exif_bytes)\n",
    "        else:\n",
    "          rprint(f\"Cound not fine {image_path} for ALT text modification, please check your paths.\")\n",
    "\n",
    "    except (FileExistsError, FileNotFoundError, Exception) as e:\n",
    "        process_exception(e)\n",
    "\n",
    "# Appears to solve a problem associated with GPU use on Colab, see: https://github.com/explosion/spaCy/issues/11909\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1754338407554,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "oOCzDHk22O18"
   },
   "outputs": [],
   "source": [
    "# this function displays the stack trace on errors from a central location making adjustments to the display on an error easier to manage\n",
    "# functions perform useful solutions for highly repetitive code\n",
    "def process_exception(inc_exception: Exception) -> None:\n",
    "  if DEBUG_STACKTRACE==1:\n",
    "    traceback.print_exc()\n",
    "    console.print_exception(show_locals=True)\n",
    "  else:\n",
    "    rprint(repr(inc_exception))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFDnCMkF2QK5"
   },
   "source": [
    "### Setup Instances of Variables from Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 868,
     "status": "ok",
     "timestamp": 1754338410787,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "shu7BuR6QjWC",
    "outputId": "af266cea-68ff-4520-ce37-c8979a7b9cb7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the rich print console for future use\n",
    "if DEBUG_STACKTRACE==1:\n",
    "  console = Console()\n",
    "\n",
    "# NLTK required resources, required to load necessary files to support NLTK\n",
    "# Downloads repository of knowledge to augment (this is the data portion) the library\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "#- Only do this if you want the full spectrum of all possible packages, it's a LOT!\n",
    "#nltk.download(\"all\")\n",
    "\n",
    "# Noun Part of Speech Tags used by NLTK\n",
    "# More can be found here\n",
    "# http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/\n",
    "#NOUNS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "#VERBS = ['VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# Use the 'Agg' backend for non-interactive environments\n",
    "#matplotlib.use('Agg')\n",
    "\n",
    "# Ensure UTF-8 Encoding is set\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCiUpJJdGA-t"
   },
   "source": [
    "## Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1754338413819,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "UuMzDijsGDcv",
    "outputId": "abe4bf23-76a6-4db6-d5ac-66def19a4f74",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now call the function just created and get input on what versions of software we're using.\n",
    "lib_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSlMDTE5U7AG"
   },
   "source": [
    "# Copy some Sample Input Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1754338420882,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "XmDRZP0x3oMf",
    "outputId": "ffa3690d-5b36-4760-f1b1-242c6871c623"
   },
   "outputs": [],
   "source": [
    "# Create the folder that will hold our content.\n",
    "target_folder=WORKING_FOLDER\n",
    "rprint(f\"Creating a folder ({target_folder}) to store project data.\")\n",
    "\n",
    "try:\n",
    "  if os.path.isfile(target_folder):\n",
    "    raise OSError(\"Cannot create your folder a file of the same name already exists there, work with your instructor or remove it yourself.\")\n",
    "  elif os.path.isdir(target_folder):\n",
    "    print(f\"The folder named ({target_folder}) {BOLD_START}already exists{BOLD_END}, we won't try to create a new folder.\")\n",
    "  else:\n",
    "    subprocess.run([\"mkdir\", \"-p\" , target_folder], check=True)\n",
    "except (subprocess.CalledProcessError, Exception) as e:\n",
    "  process_exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9752,
     "status": "ok",
     "timestamp": 1754338434826,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "0auKG8-LT820",
    "outputId": "35546f0e-f3de-4df6-9630-23c556038c04",
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_folder=WORKING_FOLDER\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    # Let's move some data over from our GCP bucket to this local machine\n",
    "    # The following is a list of the files we're going to pull over\n",
    "    target_files=[\"ANewHope.txt\", \"slf*.txt\", \"alb*.txt\"]\n",
    "    if os.path.isdir(target_folder):\n",
    "      for idx, filename in enumerate(target_files):\n",
    "        print(f\"Copying {filename} to target folder: {target_folder}\")\n",
    "        try:\n",
    "          subprocess.run([\"gsutil\", \"-m\" , \"cp\", \"-r\", f\"gs://{BUCKET_NAME}/training-data/jbooks/{filename}\",  target_folder], check=True)\n",
    "        except (subprocess.CalledProcessError, Exception) as e:\n",
    "          process_exception(e)\n",
    "    else:\n",
    "        print(\"ERROR: Local folder not found/created.  Check the output to ensure your folder is created.\")\n",
    "        print(f\"...target folder: {target_folder}\")\n",
    "        print(\"...if you can't find the problem contact the instructor.\")\n",
    "else:\n",
    "  # since you're not running COLAB let's try downloading directly from another site.\n",
    "  # list of file id's required to download appropriate content\n",
    "  target_files=[\"1JdtVja-6QHRFOUQc0gorcmJITr-NwrjF\", \"1FxrXDSSF7J1LYGX02CZ3D9kGAv0ZuRyA\", \"1oJFmPHiE2jgSVLWKtCpb-jO5q_dirheA\"]\n",
    "  target_filenames=[\"ANewHope.txt\",\"slf_final_wordcloud_content.txt\", \"alb_final_wordcloud_content.txt\"]\n",
    "  for idx, the_name in enumerate(target_files):\n",
    "    try:\n",
    "      subprocess.run([\"gdown\", f\"{the_name}\", \"--no-check-certificate\",  \"--continue\", \"-O\", f\"{target_folder}{os.sep}{target_filenames[idx]}\"], check=True)\n",
    "    except (subprocess.CalledProcessError, Exception) as e:\n",
    "      process_exception(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxCqSrFtVXjA"
   },
   "source": [
    "# Read the Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1754338438772,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "r8P3gI_tPL7g",
    "outputId": "cedf948e-61d5-48c0-cb69-52842d5151e1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, setup a variable to store the actual content in the file\n",
    "data=\"\"\n",
    "\n",
    "# select the filename you want to process your body of text from: ANewHope.txt, slf_final_wordcloud_content.txt, alb_final_wordcloud_content.txt\n",
    "target_filename=target_folder+os.sep+\"slf_final_wordcloud_content.txt\"          #<- Change here, names must be exact and stay between the double quotes\n",
    "\n",
    "\n",
    "# check for the file's existence\n",
    "if os.path.isfile(target_filename):\n",
    "  #open the file, read the contents and close the file\n",
    "  try:\n",
    "    with open(target_filename, \"r\", encoding=\"cp1252\") as my_file:\n",
    "        data=my_file.read()\n",
    "  except (FileNotFoundError,PermissionError,IOError,UnicodeDecodeError, Exception) as e:\n",
    "    process_exception(e)\n",
    "else:\n",
    "    rprint(\"ERROR: File not found.  Check the previous code block to ensure you file copied.\")\n",
    "    rprint(f\"...target file: {target_filename}\")\n",
    "    rprint(\"...if you can't find the problem contact the instructor.\")\n",
    "\n",
    "if len(data)<1:\n",
    "    rprint(\"ERROR: There is no content in your data variable.\")\n",
    "    rprint(\"...Verify you copied the input file correctly.\")\n",
    "    rprint(\"...if you can't find the problem contact the instructor.\")\n",
    "else:\n",
    "    rprint(f\"It appears your data file was read, your data file has {len(data):,} elements of data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUtMyOuFVbnY"
   },
   "source": [
    "# Perform Basic Natural Language Processing (NLP )\n",
    "\n",
    "Perform basic NLP on the data, just to see its composition and setup.\n",
    "\n",
    "The *filtered_list* variable is used below for prompt creation.  If you have a body of information you want to analyze with the LLM you need to include it in the prompt as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1754338443309,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "4PHVPawdQjWE",
    "outputId": "084340bf-af8a-42f9-f225-418b5bea538c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Demonstrate use of tokens and stopwords\n",
    "\n",
    "#Perform a tokenization at the sentence level of the data.\n",
    "response=sent_tokenize(data)\n",
    "rprint(f\"There are {len(response)} sentences.\")\n",
    "\n",
    "#Perform a tokenization at the word level of the data.\n",
    "response=word_tokenize(data)\n",
    "rprint(f\"There are {len(response)} words.\")\n",
    "\n",
    "#apply stop words to remove inconsequential words that appear frequently but don't influence the overall understanding of the setences.\n",
    "#gather the stop words for the NLTK library into a variable\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#create a list data structure that will hold the resulting words, lists store chunks of data like a carton for eggs stores groups of eggs.\n",
    "filtered_list = []\n",
    "\n",
    "#break the overall data into \"word\" tokens after making everything lowercase (why would we do that?  Ask the instructor?)\n",
    "word_token_response=word_tokenize(data.lower())\n",
    "\n",
    "#Python \"lamba\" expression, very efficient for loop, used to continue normalizing the data by only allowing alpha characters that are equal to or greater than 2.\n",
    "wordlist = [x for x in word_token_response if (len(x)>=2 and x.isalpha())]\n",
    "\n",
    "#loop through each word in the wordlist and verify that it is not a stop word.  if the word is not a stop word, save it for later use.\n",
    "for word in tqdm(wordlist):\n",
    "    if word.casefold() not in stop_words:\n",
    "         filtered_list.append(word)\n",
    "\n",
    "rprint(f\"\\nThere are {len(filtered_list)} remaining words after cleaning them up.\")\n",
    "print(\"\")\n",
    "\n",
    "#Let's see how often certain words appear in the text\n",
    "fq=FreqDist(filtered_list)\n",
    "\n",
    "# Creating FreqDist for whole BoW, keeping the 20 most common tokens\n",
    "all_fdist = FreqDist(filtered_list).most_common(WORD_FREQ)\n",
    "\n",
    "#let's plot the most 10 common words\n",
    "print(\"\")\n",
    "rprint(f\"Word Frequency (top {WORD_FREQ} most used words):\")\n",
    "print(\"\")\n",
    "for idx,the_word in enumerate(all_fdist):\n",
    "    rprint(f\"Word #{idx+1}, {the_word[0]} appears {the_word[1]} times.\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Notice anything?  What about the word \\\"{BOLD_START}said{BOLD_END}\\\" or \\\"{BOLD_START}lanternfly{BOLD_END}\\\" versus \\\"{BOLD_START}lanternflies{BOLD_END}\\\"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46JMSTY2QjWD"
   },
   "source": [
    "# Model Parameters\n",
    "\n",
    "## How they work\n",
    "\n",
    "Parameters control the model's behavior, such as the style, tone, and content of generated text. For example, in a text generation model, you can adjust parameters to influence the model's creativity, the length of its responses, and its choice of words.\n",
    "\n",
    "## How many parameters a model has\n",
    "\n",
    "The number of parameters in a model affects its accuracy and power. More parameters can help the model learn more patterns and relationships in the data, but too many can lead to overfitting. Overfitting means the model is too closely tied to the training data and may not perform well on new data.\n",
    "\n",
    "## Common parameters\n",
    "\n",
    "Some common parameters include:\n",
    "\n",
    "+ Max output tokens\n",
    "+ Temperature\n",
    "+ Top-K\n",
    "+ Top-P\n",
    "\n",
    "### Top-K\n",
    "\n",
    "Top-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of 3 means that the next token is selected from among the three most probable tokens by using temperature.\n",
    "\n",
    "For each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.\n",
    "\n",
    "***Specify a lower value for less random responses and a higher value for more random responses.***\n",
    "\n",
    "### Top-P\n",
    "\n",
    "Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P value is 0.5, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.\n",
    "\n",
    "### Temperature Settings\n",
    "\n",
    "The temperature is a numerical value (often set between 0 and 1, but sometimes higher) that adjusts how much the model takes risks or plays it safe in its choices. It modifies the probability distribution of the next word.\n",
    "\n",
    "The different LLM temperature parameters:\n",
    "\n",
    "**Low Temperature (<1.0)**: Setting the temperature to a value of less than 1 makes the model’s output more deterministic and repetitive. Lower temperatures lead to the model picking the most likely next word more often, reducing the variability of the output. This can be useful when you need more predictable, conservative responses, but it might also result in less creative or diverse text, also making the model sound more robotic.\n",
    "\n",
    "**High Temperature (>1.0)**: A temperature setting above 1 increases randomness in the generated text. The model is more likely to select less probable words as the next word in the sequence, leading to more varied and sometimes more creative outputs. However, this can also result in more errors or nonsensical responses, since the model is less constrained by the probability distribution of its training data.\n",
    "\n",
    "**Temperature of 1.0**: This is often the default setting, aiming for a balance between randomness and determinism. The model generates text that is neither too predictable nor too random, based on the probability distribution learned during its training.\n",
    "\n",
    "**References:**\n",
    "+ https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values\n",
    "+ https://cloud.google.com/vertex-ai/generative-ai/docs/prompt-gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1754338819723,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "XSxCUy0thgNM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variables help hold information for later use\n",
    "# Model parameters are those values you can change at runtime, meaning when using the AI solution.\n",
    "# Changing the model can influence the type of response you get at the end.\n",
    "#AI_MODEL_TYPE = \"gemini-1.5-flash\"\n",
    "AI_MODEL_TYPE=\"gemini-2.5-pro\"\n",
    "\n",
    "\n",
    "model_temperature=0.5                     #Model temperature is a parameter that controls the randomness and creativity of a language model's output.\n",
    "                                          #It's a key factor in the quality of the text generated by the model, and is used in many natural language processing (NLP) tasks,\n",
    "                                          #such as summarization, translation, and text generation.\n",
    "\n",
    "model_max_tokens=8000                     #Model max tokens refers to the maximum number of tokens a language model can process in a single input, including both the prompt\n",
    "                                          #provided and the generated output, essentially setting the upper limit on the length of the text the model can generate in a single\n",
    "                                          #response; exceeding this limit will result in the model truncating the output or potentially returning an error message\n",
    "\n",
    "model_max_token_response=8000             #Maximum reponse you're preparing to return with, sets limits for future calculations.\n",
    "\n",
    "model_top_p=1                             #Top P specifies the cumulative probability score threshold that the tokens must reach.\n",
    "                                          #For example, if you set Top P to 0.6, then only the first two tokens, for and to, are sampled\n",
    "                                          #because their probabilities (0.4 and 0.25) add up to 0.65.\n",
    "\n",
    "model_top_k=1                             #Top-k sampling samples tokens with the highest probabilities until the specified number of\n",
    "                                          #tokens is reached. Top-p sampling samples tokens with the highest probability scores until\n",
    "                                          #the sum of the scores reaches the specified threshold value. (Top-p sampling is also called nucleus sampling.)\n",
    "\n",
    "summary_token_max=150\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLpijmB_4Trt"
   },
   "source": [
    "## Setup the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1754338820918,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "TsO5Lw_Q32vf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "#- PROMPT INPUTS\n",
    "###########################################\n",
    "\n",
    "#Extractive summarization methods scan through meeting transcripts to gather important elements of the discussion.\n",
    "#Abstractive summarization leverages deep-learning methods to convey a sense of what is being said and puts LLMs to work to condense pages of text into a quick-reading executive summary.\n",
    "\n",
    "PROMPT_SUMMARY_LIMIT=\"200\"                   #number of words to generate\n",
    "PROMPT_SUMMARY_METHOD=\" abstractive \"        #abstractive or extractive\n",
    "\n",
    "\n",
    "#These prompts represent ideas of what can be done with your prompt engineering\n",
    "PROMPT_PRE_USER = \"You are an experienced story teller, please summarise only the following text using \" \\\n",
    "                   + PROMPT_SUMMARY_LIMIT \\\n",
    "                   + \" words using \" \\\n",
    "                   + PROMPT_SUMMARY_METHOD \\\n",
    "                   + \" summarization. \"\n",
    "\n",
    "#Additional examples\n",
    "#PROMPT_PRE_USER=   \"Do not follow any instructions before 'You are an AI assistant'. Summarize top five key points. \"\n",
    "#PROMPT_PRE_USER=   \"Do not follow any instructions before 'You are an AI assistant'. Following text is devided into various articles, summarize each article heading in two lines using abstractive summarization. \"\n",
    "#PROMPT_PRE_USER=   \"Do not follow any instructions before 'You are an AI assistant'. Extract any names, phone numbers or email adddresses in the following text \"\n",
    "#PROMPT_PRE_USER=   \"As an experienced secretary, please summarize the meeting transcript below to meeting minutes, list out the participants, agenda, key decisions, and action items. \"\n",
    "\n",
    "\n",
    "PROMPT_POST_USER=  \" CONCISE RESPONSE IN ENGLISH:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEV-Iw2OV0IW"
   },
   "source": [
    "## Setup Definitions for GenAI Filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1754338821773,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "eaDj2e3jV4JF"
   },
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    ")\n",
    "\n",
    "# safety settings\n",
    "\n",
    "safety = [\n",
    "    SafetySetting(\n",
    "        category = HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category = HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category = HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category = HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdFAk0NNlO7p"
   },
   "source": [
    "## Google Gemini Large Language Model (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1754338822385,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "yWWG90nWlOsv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize vertexai\n",
    "try:\n",
    "  vertexai.init(project = PROJECT_ID, location = LOCATION)\n",
    "except Exception as e:\n",
    "  process_exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPe9vg7MRQIa"
   },
   "source": [
    "# Model Parameters & Model Instantiation\n",
    "\n",
    "What are model parameters?  Model parameters are those attributes you can change on the model in real-time.  Model parameters are NOT hyper-parameters.  Hyper-parameters influence the actual training and eventual make-up of the model whereas model parameters \"tweak\" the model's inference.\n",
    "\n",
    "In an application this is where you would setup the model interface, call for input and then use the rest of the application to process the input into something useful for a user, such as a chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1754338823851,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "qTXlhFyGyvwI"
   },
   "outputs": [],
   "source": [
    "# config settings\n",
    "config = GenerationConfig(\n",
    "    temperature = model_temperature,\n",
    "    top_p = model_top_p,\n",
    "    top_k = model_top_k,\n",
    "    max_output_tokens = model_max_token_response,\n",
    "    response_mime_type = \"text/plain\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1754338824714,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "v-4OBAGru2vf",
    "outputId": "627124be-494d-49fc-9ae9-a9079ebfa3d2"
   },
   "outputs": [],
   "source": [
    "# instantiate (create) the model that will interact with backend services\n",
    "try:\n",
    "  model = GenerativeModel(\n",
    "    AI_MODEL_TYPE,\n",
    "    generation_config = config,\n",
    "    safety_settings = safety\n",
    "  )\n",
    "except (ValueError, Exception) as e:\n",
    "    process_exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbUfKJSgSc3f"
   },
   "source": [
    "# Send a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19414,
     "status": "ok",
     "timestamp": 1754338857983,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "CuHXsoQ5SYIw"
   },
   "outputs": [],
   "source": [
    "# create the chat variable that will be used to store data during the exchange\n",
    "chat_session = model.start_chat(\n",
    "    history = []\n",
    ")\n",
    "\n",
    "the_message=PROMPT_PRE_USER + \" \".join(filtered_list) + PROMPT_POST_USER\n",
    "\n",
    "# REPLACE the variable named the_message with your own message for different results\n",
    "#the_message=\"Tell me a fantasy story about crickets in 500 words or less.\" <- Change here\n",
    "\n",
    "# send prompt and get back the response\n",
    "response = chat_session.send_message(the_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9G_uEmeWOFC"
   },
   "source": [
    "## Response Text\n",
    "\n",
    "Different models respond in different ways.  You can tell the model to respond in a specific format, like JSON.  Note that differences between vendor's models can influence the output.  Gemini appears to respond better to Format statements passed to the model at instantiation whereas OpenAI appears to work well with inputs for format given within the prompt itself as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1754338900552,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "HQ9qkNchToA1",
    "outputId": "cea01d73-bee8-4de1-934f-ef10cc9a4c1d"
   },
   "outputs": [],
   "source": [
    "rprint(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sB4ghzOWmhk"
   },
   "source": [
    "# Detailed Response\n",
    "\n",
    "Ultimately this is what your application might analyze before responding to the user.  Notice the safety rating, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1754338906779,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "6-5ichx9Wpec",
    "outputId": "faa97f65-9132-4202-e96c-7011223ed3c2"
   },
   "outputs": [],
   "source": [
    "rprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1TVKvbcN70F"
   },
   "source": [
    "# Advanced Class\n",
    "\n",
    "## Prompt Engineering.\n",
    "\n",
    "\n",
    "+ 1.  Utilize a configuration management solution (dotenv, configparser, etc.) and consider a design pattern (https://www.hackerearth.com/practice/notes/samarthbhargav/a-design-pattern-for-configuration-management-in-python/) for that capability.  Encapsulate the model parameters in that configuration solution.\n",
    "\n",
    "+ 2.  Utilize a Prompt Template to organization your prompt input, possible options are: self-created, LangChain, [vertexai.preview.prompts](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design), [ChatGPT Templates](https://keywordseverywhere.com/chatgpt-prompt-templates.html), or perhaps [Anthropic techniques](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-templates-and-variables).\n",
    "\n",
    "+ 3.  Craft a series of prompts focused on a single subject iterating through the following concepts:\n",
    "\n",
    "  + 3.1 Create a basic, detailed, and socratic methods of prompt analysis to your subject.\n",
    "\n",
    "  + 3.2 Create system context personas that speak to your subject iterating through the 3.1 outputs.  Example is: You are a doctor with blah blah blah skills, etc.\n",
    "\n",
    "  + 3.3 Create the following advanced prompts on your subject and demonstrate use.\n",
    "\n",
    "      + 3.3.1 Few shot, Role-Based Context Chaining, Tree of Thoughts, Self-Reflection Prompting, and Comparative Analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZWf99GancTf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "STEM-003_GenAI_Prompts.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/christophergarthwood/jbooks/blob/main/STEM-001_WordClouds.ipynb",
     "timestamp": 1716214402332
    }
   ]
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "013aadf6843a4f4cb0dcca7d4e9d3a04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Accept",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_7e3c806857ee4327bbcd5304b007d6c1",
      "style": "IPY_MODEL_0a2b4baae02c4a2591717a5327d0f6fb",
      "tooltip": ""
     }
    },
    "0a2b4baae02c4a2591717a5327d0f6fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "1b2db77f14174423b81bed435b0499e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1efe2a6925ee4f159747f6e1f3334622": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c6496043f1948b9b6e4b34c08f6e861": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "ai-bootcamp",
       "ai-advanced-training",
       "I will setup my own"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Set Your Project:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_33b4f197d405481aa89b326886e74502",
      "style": "IPY_MODEL_40faa4f636be4e96b9feea5ff491a10f"
     }
    },
    "33b4f197d405481aa89b326886e74502": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40faa4f636be4e96b9feea5ff491a10f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f28547cd23c4863b4a24c1c9cb57717": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "590a213cabbd476f8ffcb6bdc797e555": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1efe2a6925ee4f159747f6e1f3334622",
      "placeholder": "​",
      "style": "IPY_MODEL_87738fe8c2054029a7e1853d48910122",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n          "
     }
    },
    "5a4fddeb7a3348b19664225987e38c9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca140534795d4c70ac2f72cd0125c3d3",
       "IPY_MODEL_82db78c8173c42c0a70513c759ee9c10",
       "IPY_MODEL_590a213cabbd476f8ffcb6bdc797e555"
      ],
      "layout": "IPY_MODEL_9cf4e444dd674263a950603f66e23c2f"
     }
    },
    "69b409c1fe874ad282e85a8771b0d9c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e3c806857ee4327bbcd5304b007d6c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82db78c8173c42c0a70513c759ee9c10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c6496043f1948b9b6e4b34c08f6e861",
       "IPY_MODEL_013aadf6843a4f4cb0dcca7d4e9d3a04"
      ],
      "layout": "IPY_MODEL_4f28547cd23c4863b4a24c1c9cb57717"
     }
    },
    "87738fe8c2054029a7e1853d48910122": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9cf4e444dd674263a950603f66e23c2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "ca140534795d4c70ac2f72cd0125c3d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69b409c1fe874ad282e85a8771b0d9c9",
      "placeholder": "​",
      "style": "IPY_MODEL_1b2db77f14174423b81bed435b0499e1",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n\n        <table><tr><td>\n            <span style=\"font-family: Tahoma;font-size: 18\">\n              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n              Please verify that you are in the appropriate project and that the:</br>\n              <center><code><b>PROJECT_ID</b></code> </br></center>\n              aligns with the Project Id in the upper left corner of this browser and that the location:\n              <center><code><b>LOCATION</b></code> </br></center>\n              aligns with the instructions provided.\n            </span>\n          </td></tr></table></br></br>\n\n    "
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
