{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVnTpGQ8xoF7"
      },
      "source": [
        "# Reading NetCDF's using Pandas DataFrame\n",
        "## Satellite Image Processing / BioCAST Seed Creation\n",
        "\n",
        "Includes references to plotting using Matplotlib and related tools.\n",
        "\n",
        "This program reads in two inputs: 1) APS level 4 satellite image 7 Day composite 2) BioCast generated advection product.  Both are NetCDF's.\n",
        "The program's goal is to read the inputs, store them, and perform a union of the data with a basic gaussian smoothing to create a \"full\" image (sometimes a problem due to cloud coverage).\n",
        "Output the data using NetCDF Best Practices, APS meta-data as an authoritative source, and custom meta-data geared towards future requirements."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME     = \"ai-training-2024-08-09-bucket\"\n",
        "PROJECT_ID      = \"ai-training-2024-08-09\"\n",
        "LOCATION        = \"us-central1\"\n",
        "secret_name     = \"ai-training-key-secret\"\n",
        "secret_version  = \"latest\"\n",
        "project_id      = \"usfs-tf-admin\"\n",
        "resource_name   = f\"projects/{project_id}/secrets/{secret_name}/versions/{secret_version}\""
      ],
      "metadata": {
        "id": "Kx542Z3D3j3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#- Google Colab Check\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "RunningInCOLAB = False\n",
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if RunningInCOLAB:\n",
        "    print(\"You are running this notebook in Google Colab.\")\n",
        "else:\n",
        "    print(\"You are running this notebook with Jupyter iPython runtime.\")\n",
        "    print(\"Assumption is you have the required libraries to execute this notebook.\")"
      ],
      "metadata": {
        "id": "YnZXrQOM3ubW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib.util"
      ],
      "metadata": {
        "id": "SaduG9Mt3wHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "libraries=[\"numpy\", \"pandas\", \"scipy\", \"sklearn\", \"matplotlib\", \"cartopy\", \"seaborn\", \"netCDF4\"]\n",
        "import importlib.util\n",
        "\n",
        "for library in libraries:\n",
        "    if library == \"Pillow\":\n",
        "      spec = importlib.util.find_spec(\"PIL\")\n",
        "    else:\n",
        "      spec = importlib.util.find_spec(library)\n",
        "    if spec is None:\n",
        "      print(\"Installing library \" + library)\n",
        "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"])\n",
        "    else:\n",
        "      print(\"Library \" + library + \" already installed.\")"
      ],
      "metadata": {
        "id": "CYuFSsVW3yBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libararies"
      ],
      "metadata": {
        "id": "XjUCz0np4JC0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz8pAS0zxoF8"
      },
      "outputs": [],
      "source": [
        "# Python 3.7.3\n",
        "############################################\n",
        "# INCLUDES\n",
        "############################################\n",
        "#libraries specific to this example\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import matplotlib as matplt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#a set of libraries that perhaps should always be in Python source\n",
        "import os\n",
        "import datetime\n",
        "import sys\n",
        "import gc\n",
        "import getopt\n",
        "import inspect\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "#a darn useful library for creating paths and one I recommend you load to your environment\n",
        "from pathlib import Path\n",
        "\n",
        "from pydoc import help                          # can type in the python console `help(name of function)` to get the documentation\n",
        "\n",
        "#Data Science Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import scipy.ndimage\n",
        "\n",
        "#Plotting libraries\n",
        "import matplotlib as matplt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cartopy.crs as ccrs\n",
        "\n",
        "# for NetCDF output\n",
        "import netCDF4 as nc\n",
        "from netCDF4 import Dataset\n",
        "\n",
        "#a darn useful library for creating paths and one I recommend you load to your environment\n",
        "from pathlib import Path\n",
        "\n",
        "warnings.filterwarnings('ignore')               # don't print out warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pull Support Routines"
      ],
      "metadata": {
        "id": "-NrsAizu4ZHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf ./folderOnColab && echo \"Ok, removed.\" || { echo \"No folder to remove.\"; exit 1; }\n",
        "#!mkdir -p ./folderOnColab && echo \"Folder created.\" || { echo \"Failed to create folder, it might already exist.\";  }\n",
        "#!gsutil -m cp -r gs://usfs-gcp-rand-test-data-usc1/public_source/jbooks/ANewHope.txt ./folderOnColab\n",
        "\n",
        "target_folder=\"./folderOnColab\"\n",
        "target_repo=\"https://raw.githubusercontent.com//christophergarthwood/jbooks/main\"\n",
        "target_files=[\"support_debug.ipynb\", \"support_functions.ipynb\"]\n",
        "print(f\"Creating a folder ({target_folder}) to store project data.\")\n",
        "subprocess.run([\"mkdir\", \"-p\" , target_folder])\n",
        "if os.path.isdir(target_folder):\n",
        "  print(\"Performing wget on:\")\n",
        "  for idx, filename in enumerate(target_files):\n",
        "    print(f\"...{filename} to target folder: {target_folder}\")\n",
        "    try:\n",
        "      subprocess.run([\"wget\", f\"--directory-prefix={target_folder}\", f\"{target_repo}/{filename}\"])\n",
        "    except Exception as e:\n",
        "      print(\"\")\n",
        "      print(f\"ERROR: There was a problem performing wget on the target file ({filename}), see Exception: {str(e)}\")\n",
        "      print(\"...talk to the instructor.\")\n",
        "    if os.path.isfile(target_folder+os.sep+filename):\n",
        "      print(\"...verified copy.\")\n",
        "      print(\"...importing code.\")\n",
        "      target_filename=f\"{target_folder+os.sep+filename}\"\n",
        "      os.environ[\"target_filename\"]=target_filename\n",
        "      %run $target_filename\n",
        "    else:\n",
        "      print(f\"...copy NOT verified, check the {target_folder} for the existence of {filename}\")\n",
        "else:\n",
        "    print(\"ERROR: Local folder not found/created.  Check the output to ensure your folder is created.\")\n",
        "    print(f\"...target folder: {target_folder}\")\n",
        "    print(\"...if you can't find the problem contact the instructor.\")\n"
      ],
      "metadata": {
        "id": "epur7DKY4bM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Library Configuration for Pandas and Numpy"
      ],
      "metadata": {
        "id": "Td9fnkYn4i3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEq6oIvKxoF8"
      },
      "outputs": [],
      "source": [
        "msg_info(\"Setting Library Configuration\")\n",
        "set_library_configuration()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Px8gvbxoF8"
      },
      "source": [
        "# Variable declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEhxXPrexoF9"
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "# GLOBAL VARIABLES\n",
        "############################################\n",
        "DEBUG = 1                            #General ledger output so you know what's happening.\n",
        "DEBUG_DATA = 1                       #Extremely verbose output, change to zero (0) to supress the volume of output.\n",
        "\n",
        "# CODE CONSTRAINTS\n",
        "VERSION_NAME    = \"OFM_seed_generator\"\n",
        "VERSION_ACRONYM = \"OFMSG\"\n",
        "VERSION_MAJOR   = 0\n",
        "VERSION_MINOR   = 0\n",
        "VERSION_RELEASE = 1\n",
        "VERSION_TITLE   = VERSION_NAME + \" (\" + VERSION_ACRONYM + \") \" + str(VERSION_MAJOR) + \".\" + str(VERSION_MINOR) + \".\" + str(VERSION_RELEASE) + \" generated SEED.\"\n",
        "\n",
        "FILL_VALUE=-32767.0\n",
        "ENCODING  =\"utf-8\"\n",
        "############################################\n",
        "# GLOBAL CONSTANTS\n",
        "############################################\n",
        "MINIMAL_VALUE=0.0001                  #Normally a _FillValue would be used but different NetCDF implementations\n",
        "                                      #are treating some values differently or don't even have the concept.\n",
        "METADATA_DESCRIPTION = \"Product composite using a daily satellite image coupled with a BioCast background field; used to SEED the optical field.\"\n",
        "METADATA_DISTRIBUTION= \"Distribution Statement A: Approved for public release; distribution unlimited.\"\n",
        "METADATA_REFERENCES  = \"None\"\n",
        "METADATA_INSTITUTION = \"Naval Research Lab (NRL)\"\n",
        "METADATA_SOURCE      = VERSION_NAME + \" (\" + VERSION_ACRONYM + \") \" + str(VERSION_MAJOR) + \".\" + str(VERSION_MINOR) + \".\" + str(VERSION_RELEASE)\n",
        "METADATA_COMMENT     = \"None\"\n",
        "\n",
        "\n",
        "############################################\n",
        "# APPLICATION VARIABLES\n",
        "############################################\n",
        "aps_filename  = \"\"                     #APS NetCDF (daily file) filename\n",
        "aps_df        = pd.DataFrame()         #Dataframe that holds APS geospatial data\n",
        "aps_products  = {}                     #Dictionary of \"products\" each encapsulated in a dataframe\n",
        "\n",
        "biocast_filename= \"\"                   #BIOCAST NetCDF Tau 0 (initial forecast), background (to fill in APS)\n",
        "biocast_panda   = pd.DataFrame()       #Dataframe that holds BIOCAST geospatial data\n",
        "biocast_products={}                    #Dictionary of \"Products\" each encapsulated in a dataframe\n",
        "\n",
        "output_filename = \"\"\n",
        "output_panda    = pd.DataFrame()\n",
        "output_products={}\n",
        "\n",
        "todayIs = datetime.datetime.now()\n",
        "date_to_process = todayIs\n",
        "clobber = \"false\"\n",
        "aps_variable_domain=[]\n",
        "biocast_variable_domain=[]\n",
        "output_variable_domain=[]\n",
        "\n",
        "netcdf_metadata={}\n",
        "\n",
        "geospatial_lat_nm='lat'\n",
        "geospatial_lon_nm='lon'\n",
        "aps_lat_nm=\"y\"\n",
        "aps_lon_nm=\"x\"\n",
        "#note x,y values are shown below as they are part of the APS meta-data\n",
        "#based on the NetCDF Best Practice subject x,y vars should not exist.\n",
        "#keeping for continuity between BIOCAST code already written to read APS input.\n",
        "geospatial_vars = [geospatial_lat_nm, geospatial_lon_nm, aps_lat_nm, aps_lon_nm]\n",
        "\n",
        "############################################\n",
        "# GLOBAL CONFIGURATION\n",
        "############################################\n",
        "os.environ['PYTHONIOENCODING']=ENCODING\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k6DEGsixoF9"
      },
      "source": [
        "# Define some Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-pspVcdxoF9"
      },
      "outputs": [],
      "source": [
        "#temporary environment variable declaration\n",
        "os.environ[\"NETCDF_METADATA_nm_title\"]=\"title\";\n",
        "os.environ[\"NETCDF_METADATA_nm_create_date\"]=\"createDate\";\n",
        "os.environ[\"NETCDF_METADATA_nm_rp_org\"]=\"rp_organization\";\n",
        "os.environ[\"NETCDF_METADATA_nm_rp_name\"]=\"rp_name\";\n",
        "os.environ[\"NETCDF_METADATA_nm_rp_phone\"]=\"rp_phone\";\n",
        "os.environ[\"NETCDF_METADATA_nm_rp_email\"]=\"rp_email\";\n",
        "os.environ[\"NETCDF_METADATA_nm_conventions\"]=\"conventions\";\n",
        "os.environ[\"NETCDF_METADATA_val_title\"]=\"\";\n",
        "os.environ[\"NETCDF_METADATA_val_create_date\"]=\"\";\n",
        "os.environ[\"NETCDF_METADATA_val_rp_org\"]=\"Naval Research Lab (NRL)\";\n",
        "os.environ[\"NETCDF_METADATA_val_rp_name\"]=\"Christopher G Wood\";\n",
        "os.environ[\"NETCDF_METADATA_val_rp_phone\"]=\"228-688-4755\";\n",
        "os.environ[\"NETCDF_METADATA_val_rp_email\"]=\"christopher.wood@nrlssc.navy.mil\";\n",
        "os.environ[\"NETCDF_METADATA_val_conventions\"]=\"National System for Geospatial Integlligence (NSG) Meatadata Foundation (NMF), Version 3.0, 2016-08-31 (NGA.STD.0012-3.0)\"\n",
        "os.environ[\"NETCDF_METADATA_nm_nb_latitude\"]=\"geoBoundingBox_northBoundLatitude\";\n",
        "os.environ[\"NETCDF_METADATA_nm_sb_latitude\"]=\"geoBoundingBox_southBoundLatitude\";\n",
        "os.environ[\"NETCDF_METADATA_nm_eb_longitude\"]=\"geoBoundingBox_eastBoundLongitude\";\n",
        "os.environ[\"NETCDF_METADATA_nm_wb_longitude\"]=\"geoBoundingBox_westBoundLongitude\";\n",
        "os.environ[\"NETCDF_METADATA_nm_geo_desc\"]=\"geoDescription\";\n",
        "os.environ[\"NETCDF_METADATA_nm_refsys_code\"]=\"referenceSystem_code\";\n",
        "os.environ[\"NETCDF_METADATA_nm_refsys_title\"]=\"referenceSystem_title\";\n",
        "os.environ[\"NETCDF_METADATA_val_nb_latitude\"]=\"\";\n",
        "os.environ[\"NETCDF_METADATA_val_sb_latitude\"]=\"\";\n",
        "os.environ[\"NETCDF_METADATA_val_eb_longitude\"]=\"\";\n",
        "os.environ[\"NETCDF_METADATA_val_wb_longitude\"]=\"\";\n",
        "os.environ[\"NETCDF_METADATA_val_geo_desc\"]=\"Pseudo-Mercator\";\n",
        "os.environ[\"NETCDF_METADATA_val_refsys_code\"]=\"Geodetic Geographic 3D\";\n",
        "os.environ[\"NETCDF_METADATA_val_refsys_title\"]=\"EPSG:4326 (WGS 84)\";\n",
        "os.environ[\"NETCDF_METADATA_nm_start_date\"]=\"start_date\";\n",
        "os.environ[\"NETCDF_METADATA_nm_start_time\"]=\"start_time\";\n",
        "os.environ[\"NETCDF_METADATA_nm_stop_date\"]=\"stop_date\";\n",
        "os.environ[\"NETCDF_METADATA_nm_stop_time\"]=\"stop_time\";\n",
        "os.environ[\"NETCDF_METADATA_val_start_date\"]=\"\"\n",
        "os.environ[\"NETCDF_METADATA_val_start_time\"]=\"\";\n",
        "os.environ[\"NETCDF_METADATA_val_stop_date\"]=\"\";\n",
        "os.environ[\"NETCDF_METADATA_val_stop_time\"]=\"\";\n",
        "os.environ[\"NETCDF_METADATA_nm_lineage\"]=\"lineage\";\n",
        "os.environ[\"NETCDF_METADATA_val_lineage\"]=\"\";\n",
        "os.environ[\"NETCDF_METADATA_nm_ism_diststatement\"]=\"ism_NoticeText\";\n",
        "os.environ[\"NETCDF_METADATA_nm_ism_class\"]=\"ism_classification\";\n",
        "os.environ[\"NETCDF_METADATA_nm_ism_ownerproducer\"]=\"ism_ownerProducer\";\n",
        "os.environ[\"NETCDF_METADATA_val_ism_diststatement\"]=\"Distribution Statement A: Approved for public release; distribution unlimited.\";\n",
        "os.environ[\"NETCDF_METADATA_val_ism_class\"]=\"Unclassified\";\n",
        "os.environ[\"NETCDF_METADATA_val_ism_ownerproducer\"]=\"USA\";"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display your Library Versions"
      ],
      "metadata": {
        "id": "8J_P4JMd42-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msg_info(\"Library Diagnostics\")\n",
        "lib_diagnostics()"
      ],
      "metadata": {
        "id": "69G1FU6B42BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrxA6z31xoF9"
      },
      "source": [
        "# Function Declaration (lots of functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd9nUAP3xoF9"
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "# WARNING / ERROR Management\n",
        "############################################\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()\n",
        "\n",
        "\n",
        "############################################\n",
        "# FUNCTIONS\n",
        "############################################\n",
        "\n",
        "def prototype(incMonth):\n",
        "\n",
        "    msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "    msg_info(\"The month you passed in was \" + str(incMonth))\n",
        "    msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "    return 1\n",
        "\n",
        "\n",
        "def get_full_version():\n",
        "\n",
        "    resultant = str(VERSION_NAME) + \"  v\" + str(VERSION_MAJOR) + \".\" + str(VERSION_MINOR) + \".\" + str(VERSION_RELEASE)\n",
        "    return resultant\n",
        "\n",
        "def get_version():\n",
        "\n",
        "    resultant = str(VERSION_MAJOR) + \".\" + str(VERSION_MINOR) + \".\" + str(VERSION_RELEASE)\n",
        "    return resultant\n",
        "\n",
        "def printversion():\n",
        "\n",
        "    print(get_full_version())\n",
        "\n",
        "def printusage():\n",
        "\n",
        "    print(\"\")\n",
        "    printversion()\n",
        "    print(\"  -v, --version    prints the version of this software package.\")\n",
        "    print(\"  -o, --ofile  =  *name of the NetCDF output file (seed for 3DOG ingest).\")\n",
        "    print(\"  -a, --afile  =  *name of the NetCDF input file (APS generated lvl4 file).\")\n",
        "    print(\"  -b, --bfile  =  *name of the NetCDF BIOCAST input file (BIOCAST generated, Tau 0).\")\n",
        "    print(\"  -c, --clobber=   overwrite existing output file.\")\n",
        "    print(\"\")\n",
        "    print(\"  * - indicates required argument.\")\n",
        "\n",
        "def show_columns_plain(inc_ary):\n",
        "    new_ary = []\n",
        "    for col in inc_ary:\n",
        "        new_ary.append(np.char.lower(col))\n",
        "    new_ary.sort\n",
        "    myOutputString = \" \"\n",
        "    for col in new_ary:\n",
        "        myOutputString = myOutputString + \" \" + str(col)\n",
        "    return myOutputString\n",
        "\n",
        "def show_columns_true(inc_ary):\n",
        "    new_ary = []\n",
        "    for col in inc_ary:\n",
        "        new_ary.append(col)\n",
        "    new_ary.sort\n",
        "    myOutputString = \" \"\n",
        "    for col in new_ary:\n",
        "        myOutputString = myOutputString + \" \" + str(col)\n",
        "    return myOutputString\n",
        "\n",
        "\n",
        "###\n",
        "# valid string:\n",
        "#  We don't want the following:\n",
        "#   - at the start of the file name (might be construed as a switch)\n",
        "#  $, &, |, ;, <, >, `, !, *, \", \\ (to start with)\n",
        "###\n",
        "def validstring(testsubject):\n",
        "\n",
        "    if testsubject[0] == \"-\":\n",
        "        return 0\n",
        "    elif \"$\" in testsubject or \"&\" in testsubject or \"|\" in testsubject:\n",
        "        return 0\n",
        "    elif \";\" in testsubject or \"`\" in testsubject or \"!\" in testsubject:\n",
        "        return 0\n",
        "    elif \"*\" in testsubject or '\"' in testsubject or \"\\\\\" in testsubject:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "\n",
        "def inputusage(flag):\n",
        "\n",
        "    print(\"\")\n",
        "    printversion()\n",
        "    if flag == \"input\":\n",
        "        print(\"Input filename must reference a real file and cannot contain:\")\n",
        "        print('  $, &, |, ;, <, >, `, !, *, \", \\ ')\n",
        "        print(\"  or start with the character - \")\n",
        "    elif flag == \"biocast\":\n",
        "        print(\"BIOCAST Input filename must reference a real file and cannot contain:\")\n",
        "        print('  $, &, |, ;, <, >, `, !, *, \", \\ ')\n",
        "        print(\"  or start with the character - \")\n",
        "    elif flag == \"output\":\n",
        "        print(\"Output filename must not exist and cannot contain:\")\n",
        "        print('  $, &, |, ;, <, >, `, !, *, \", \\ ')\n",
        "        print(\"  or start with the character - \")\n",
        "        print(\" Set -c or --clobber to overwrite existing file.\")\n",
        "    elif flag == \"collision\":\n",
        "        print(\"Input file(s) and/or output file cannot be the same.\")\n",
        "        print(\"...You were joking right?\")\n",
        "    else:\n",
        "        print(\"Undefined input error.\")\n",
        "\n",
        "\n",
        "def failure_management(inc_reason, inc_exception):\n",
        "    msg_warning(\"Exception:\")\n",
        "    msg_warning(\"....\" + str(inc_exception))\n",
        "    msg_warning(\"\")\n",
        "    msg_warning(inc_reason)\n",
        "    printusage()\n",
        "    exit(1)\n",
        "\n",
        "\n",
        "def argument_management(inc_opts, inc_args):\n",
        "\n",
        "    global date_to_process, aps_filename, biocast_filename, output_filename, area, clobber\n",
        "\n",
        "    insufficient_args = \"Unable to process the seed as insufficient command line parameters were passed.\"\n",
        "\n",
        "    for opt, arg in inc_opts:\n",
        "        if opt == \"-h\":\n",
        "            printusage()\n",
        "            sys.exit(2)\n",
        "        elif opt in (\"-a\", \"-A\", \"-ifile\", \"--ifile\"):\n",
        "            aps_filename = arg\n",
        "        elif opt in (\"-b\", \"-B\", \"-bfile\", \"--bfile\"):\n",
        "            biocast_filename = arg\n",
        "        elif opt in (\"-o\", \"-O\", \"-ofile\", \"--ofile\"):\n",
        "            output_filename = arg\n",
        "        elif opt in (\"-c\", \"--clobber\"):\n",
        "            clobber = arg\n",
        "        elif opt in (\"-V\", \"-v\", \"-version\", \"-Version\", \"--version\", \"--Version\"):\n",
        "            printversion()\n",
        "            sys.exit(2)\n",
        "\n",
        "\n",
        "\n",
        "    if clobber != \"true\" and clobber != \"false\":\n",
        "        msg_warning(\"Clobber is invalid.\")\n",
        "        msg_warning(clobber)\n",
        "        printusage()\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        if (\"aps_filename\" in locals()) or (\"aps_filename\" in globals()) and (len(aps_filename) > 0):\n",
        "            if DEBUG_DATA:\n",
        "                msg_debug(\" APS Input file: \" + str(aps_filename))\n",
        "            if not validstring(aps_filename) or not os.path.isfile(aps_filename):\n",
        "                msg_warning(\"APS input filename is invalid or file doesn't exist.\")\n",
        "                inputusage(\"input\")\n",
        "                exit(1)\n",
        "        else:\n",
        "            msg_warning(insufficient_args)\n",
        "            printusage()\n",
        "            exit(1)\n",
        "    except Exception as e:\n",
        "        failure_management(insufficient_args, str(e))\n",
        "\n",
        "    try:\n",
        "        if (\"biocast_filename\" in locals()) or (\"biocast_filename\" in globals()) and (len(biocast_filename) > 0):\n",
        "            if DEBUG_DATA:\n",
        "                msg_debug(\" BIOCAST Input file: \" + str(biocast_filename))\n",
        "            if not validstring(biocast_filename) or not os.path.isfile(biocast_filename):\n",
        "                msg_warning(\"BIOCAST Input filename is invalid or file doesn't exist.\")\n",
        "                inputusage(\"biocast\")\n",
        "                exit(1)\n",
        "        else:\n",
        "            msg_warning(insufficient_args)\n",
        "            printusage()\n",
        "            exit(1)\n",
        "    except Exception as e:\n",
        "        failure_management(insufficient_args, str(e))\n",
        "\n",
        "    try:\n",
        "        if (\"output_filename\" in locals()) or (\"output_filename\" in globals()) and (len(output_filename) > 0):\n",
        "            if DEBUG:\n",
        "                msg_debug(\"Output file: \" + str(output_filename))\n",
        "            if not validstring(output_filename) or (os.path.exists(output_filename) and clobber != \"true\"):\n",
        "                msg_warning(\"Output filename is invalid or file exists and clobber was not set.\")\n",
        "                inputusage(\"output\")\n",
        "                exit(1)\n",
        "        else:\n",
        "            msg_warning(\"Unable to process the nowcast without an output file defined.\")\n",
        "            printusage()\n",
        "            exit(1)\n",
        "    except Exception as e:\n",
        "        failure_management(insufficient_args, str(e))\n",
        "\n",
        "    if output_filename == aps_filename:\n",
        "        inputusage(\"collision\")\n",
        "        exit(1)\n",
        "\n",
        "    if output_filename == biocast_filename:\n",
        "        inputusage(\"collision\")\n",
        "        exit(1)\n",
        "\n",
        "    if aps_filename == biocast_filename:\n",
        "        inputusage(\"collision\")\n",
        "        exit(1)\n",
        "\n",
        "def get_variable_domain(inc_dataset, inc_domain):\n",
        "    if DEBUG:\n",
        "        msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #based on analysis of variables, the ProductUnits attribute is only present in those\n",
        "    #variables that have actual geophysical properties, a.k.a. true products\n",
        "    for var in inc_dataset.get_variables_by_attributes(productUnits=lambda v: v is not None):\n",
        "        inc_domain.append(var.name)\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "def establish_common_domain(inc_aps_domain, inc_biocast_domain):\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    for var in inc_aps_domain:\n",
        "        if var in inc_biocast_domain:\n",
        "            output_variable_domain.append(var)\n",
        "            debug.msg_debug(\"...Found match between both:\"+str(var))\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "def show_netcdf_details(inc_dataset, inc_var_domain):\n",
        "    if DEBUG:\n",
        "        msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "        try:\n",
        "            msg_debug(\".......version:\" + str(inc_dataset.data_model))\n",
        "        except Exception as e:\n",
        "            failure_management(\"While loading the model climatology a problem occurred.\", str(e))\n",
        "\n",
        "        if (inc_dataset.isopen):\n",
        "          for attr in inc_var_domain:\n",
        "            msg_debug(\"........processing \" + str(attr))\n",
        "            try:\n",
        "                variables=inc_dataset.get_variables_by_attributes(name=attr)\n",
        "                for var in variables:\n",
        "                    msg_debug(\"................name:\"+str(var.name))\n",
        "                    msg_debug(\"................dims:\"+str(var.ndim))\n",
        "                    msg_debug(\"................type:\"+str(var.dtype))\n",
        "                    msg_debug(\"...............shape:\"+str(var.shape))\n",
        "            except Exception as e:\n",
        "                msg_warning(\"Unable to gather details on variable:\" + str(attr), str(e))\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "def show_data(inc_dataframe):\n",
        "\n",
        "    if isinstance(inc_dataframe, pd.DataFrame):\n",
        "        try:\n",
        "            if DEBUG_DATA:\n",
        "                msg_debug(\"--------------------------------------------------------------\")\n",
        "                msg_info(\"Entering \" + inspect.stack()[0][3])\n",
        "                msg_debug(\"HEAD\")\n",
        "                print(inc_dataframe.head())\n",
        "                msg_debug(\"TAIL\")\n",
        "                print(inc_dataframe.tail())\n",
        "                msg_debug(\"DESCRIBE\")\n",
        "                print(inc_dataframe.describe())\n",
        "                msg_debug(\"axes: {}\".format(inc_dataframe.axes))\n",
        "                msg_debug(\"shape: {}\".format(inc_dataframe.shape))\n",
        "                msg_debug(\"size: {}\".format(inc_dataframe.size))\n",
        "                msg_info(\"Exiting \" + inspect.stack()[0][3])\n",
        "                msg_debug(\"--------------------------------------------------------------\")\n",
        "        except Exception as e:\n",
        "            msg_warning(\"Unable to show details on the data frame passed.\", str(e))\n",
        "    else:\n",
        "        msg_warning(\"In show_data() the incoming dataframe was not a Pandas.DataFrame.\")\n",
        "        msg_warning(\"This warning is shown but program execution will continue.\")\n",
        "    return\n",
        "\n",
        "def create_output(inc_outputfilename, inc_lat_dim, inc_lon_dim, inc_depth_max, inc_depths, inc_fused_data):\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    # dataset = Dataset(inc_outputfilename, 'w', format='NETCDF4_CLASSIC')\n",
        "    dataset = netcdf.netcdf_file(inc_outputfilename, \"w\")\n",
        "\n",
        "#    meta_data_input=str(PROJ) + \" \" + str(PROJ_AREA) + \" Machine Learning Prediction \" + str(VERSION_NAME) + \"v\" + str(get_version())\n",
        "    meta_data_input=\"put something here\"\n",
        "    dataset.title = meta_data_input.encode(ENCODING, errors='ignore').strip()\n",
        "    meta_data_input=\" \"\n",
        "    dataset.subtitle = meta_data_input.encode(ENCODING, errors='ignore')\n",
        "\n",
        "    depth = dataset.createDimension(\"depth\", inc_depth_max)\n",
        "    time = dataset.createDimension(\"time\", 1)\n",
        "    lat = dataset.createDimension(\"lat\", inc_lat_dim)\n",
        "    lon = dataset.createDimension(\"lon\", inc_lon_dim)\n",
        "    x = dataset.createDimension(\"x\", inc_lat_dim)\n",
        "    y = dataset.createDimension(\"y\", inc_lon_dim)\n",
        "    z = dataset.createDimension(\"z\", inc_depth_max)\n",
        "\n",
        "    lat_reference = dataset.createVariable(\"lat\", \"f8\", (\"lat\",))\n",
        "    # put units, long_name and other details\n",
        "    lon_reference = dataset.createVariable(\"lon\", \"f8\", (\"lon\",))\n",
        "    # put units, long_name and other details\n",
        "    depth_reference = dataset.createVariable(\"depths\", \"f8\", (\"depth\",))\n",
        "    # put units, long_name and other details\n",
        "\n",
        "    netcdf_variable_output_domain = None\n",
        "    netcdf_variable_output_domain = variable_output_domain.copy()\n",
        "    netcdf_variable_output_domain.append(\"sat_CHL\")\n",
        "    netcdf_variable_output_domain.append(\"sat_sst\")\n",
        "\n",
        "    for var in netcdf_variable_output_domain:\n",
        "        if DEBUG:\n",
        "            msg_debug(\"...Processing \" + str(var))\n",
        "        var_reference = dataset.createVariable(var, \"f8\", (\"time\", \"depth\", \"lat\", \"lon\"))\n",
        "        # put units, long_name and other details\n",
        "\n",
        "    if DEBUG_DATA:\n",
        "        msg_debug(\"  Output file name: \" + str(inc_outputfilename))\n",
        "        msg_debug(\"        DIMENSIONS: \")\n",
        "        for dimname in dataset.dimensions.keys():\n",
        "            dim = dataset.dimensions[dimname]\n",
        "            msg_debug(\"....\" + str(dimname) + \":\" + str(dim))\n",
        "        for varname in dataset.variables.keys():\n",
        "            var = dataset.variables[varname]\n",
        "            msg_debug(\"....\" + str(varname))\n",
        "            # debug.msg_debug(\"....\" + str(varname) + \":\" + str(var.dtype) + \" - \" + str(var.dimensions) + \" - \" + str(var.shape))\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def create_navigation_netcdf_vars(inc_coordinate_hash, output_dataset, inc_DEPTHS):\n",
        "\n",
        "    msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_debug(\"...latitude\")\n",
        "    lat_netcdf_variable = output_dataset.variables[\"lat\"]\n",
        "    lat_array = inc_coordinate_hash[\"lat\"]\n",
        "    lat_netcdf_variable[:] = lat_array\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_debug(\"...longitude\")\n",
        "    lon_netcdf_variable = output_dataset.variables[\"lon\"]\n",
        "    lon_array = inc_coordinate_hash[\"lon\"]\n",
        "    lon_netcdf_variable[:] = lon_array\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_debug(\"...depths\")\n",
        "    depth_netcdf_variable = output_dataset.variables[\"depths\"]\n",
        "    depth_array = inc_DEPTHS\n",
        "    depth_netcdf_variable[:] = depth_array\n",
        "\n",
        "    msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "\n",
        "def show_data_distribution(inc_dict1, inc_dict2, inc_variable_domain, inc_name1, inc_name2, inc_outputfile_prefix):\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    for var in inc_variable_domain:\n",
        "        matplt.use('Agg')\n",
        "        fig = plt.figure(figsize=(12, 8))\n",
        "        ax = fig.add_subplot(1, 1, 1)\n",
        "        #inc_dataframe[var].hist(bins=10, grid=True)\n",
        "        sns.distplot(inc_dict1[var], label=inc_name1, kde=False, bins=10)\n",
        "        sns.distplot(inc_dict2[var], label=inc_name2, kde=False, bins=10)\n",
        "        plt.legend(prop={'size':12})\n",
        "        plt.xlabel(str(var), fontsize=15)\n",
        "        plt.ylabel(\"Frequency\", fontsize=15)\n",
        "        plt.title(str(var) + \" Distribution\", fontsize=18)\n",
        "        output_filename=str(inc_outputfile_prefix) + \"_\" + str(var) + \"_map.png\"\n",
        "        plt.savefig(output_filename)\n",
        "        plt.close()\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "def show_map_coverage(inc_dataframe_reference, inc_dataframe, inc_name):\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    coordinate_offset=0.5\n",
        "    new_df = inc_dataframe\n",
        "    TARGET_DATAFRAME = inc_dataframe_reference\n",
        "\n",
        "    central_longitude       = np.median(TARGET_DATAFRAME[geospatial_lon_nm])\n",
        "    central_latitude        = np.median(TARGET_DATAFRAME[geospatial_lat_nm])\n",
        "    west                    = np.min(TARGET_DATAFRAME[geospatial_lon_nm]) - coordinate_offset\n",
        "    east                    = np.max(TARGET_DATAFRAME[geospatial_lon_nm]) + coordinate_offset\n",
        "    north                   = np.max(TARGET_DATAFRAME[geospatial_lat_nm]) + coordinate_offset\n",
        "    south                   = np.min(TARGET_DATAFRAME[geospatial_lat_nm]) - coordinate_offset\n",
        "\n",
        "    slice_central_longitude = new_df[geospatial_lon_nm].median()\n",
        "    slice_central_latitude  = new_df[geospatial_lat_nm].median()\n",
        "    slice_west              = new_df[geospatial_lon_nm].min() - coordinate_offset\n",
        "    slice_east              = new_df[geospatial_lon_nm].max() + coordinate_offset\n",
        "    slice_north             = new_df[geospatial_lat_nm].max() + coordinate_offset\n",
        "    slice_south             = new_df[geospatial_lat_nm].min() - coordinate_offset\n",
        "\n",
        "    if (DEBUG_DATA):\n",
        "        msg_debug(\"    {}      {}\".format(\"Coord Hash\", \"Depth Slice\"))\n",
        "        msg_debug(\"{}  {}      {}\".format(\"Cent Long\", central_longitude, slice_central_longitude))\n",
        "        msg_debug(\"{}  {}      {}\".format(\"Cent Lat\", central_latitude, slice_central_latitude))\n",
        "        msg_debug(\"{}  {}      {}\".format(\"West\", west, slice_west))\n",
        "        msg_debug(\"{}  {}      {}\".format(\"East\", east, slice_east))\n",
        "        msg_debug(\"{}  {}      {}\".format(\"North\", north, slice_north))\n",
        "        msg_debug(\"{}  {}      {}\".format(\"South\", south, slice_south))\n",
        "\n",
        "    matplt.use('Agg')\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    ax = fig.add_subplot(1, 1, 1, projection=ccrs.Miller())\n",
        "    ax.set_extent([west, east, south, north])\n",
        "    ax.coastlines()\n",
        "    ax.stock_img()\n",
        "    ax.gridlines()\n",
        "    ax.add_wms(wms=\"http://vmap0.tiles.osgeo.org/wms/vmap0\", layers=[\"basic\"])\n",
        "\n",
        "    DATA_COLOR = \"red\"\n",
        "    DATA_SIZE = 0.10\n",
        "\n",
        "    ax.scatter(new_df[geospatial_lon_nm], new_df[geospatial_lat_nm], c=DATA_COLOR, s=DATA_SIZE)\n",
        "\n",
        "    output_filename=str(inc_name) + \"_map.png\"\n",
        "    msg_debug(\"Graphics output filename:\"+str(output_filename))\n",
        "    plt.savefig(output_filename)\n",
        "    plt.close()\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "def take_out_trash():\n",
        "    if (gc.isenabled()):\n",
        "        try:\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            msg_warning(\"Unable to garbage collect.\")\n",
        "    else:\n",
        "        try:\n",
        "            gc.enable()\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            msg_warning(\"Tried to turn on automatic garbage collection and collect but failed.\")\n",
        "\n",
        "def gather_aps_metadata(inc_auth_netcdf):\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #set global attribution, copy the originating APS NetCDF and replicate\n",
        "    try:\n",
        "        global_name_list=inc_auth_netcdf.ncattrs()\n",
        "    except Exception as e:\n",
        "            msg_warning(\"Unable to invoke .ncattrs on the APS netcdf file in {}.{}.\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    msg_debug(\"...gathering and posting APS global attribution\")\n",
        "    value=\"\"\n",
        "    for name in global_name_list:\n",
        "        try:\n",
        "            value=inc_auth_netcdf.getncattr(name)\n",
        "            msg_debug(\"..........{} -> {}\".format(name, value))\n",
        "        except Exception as e:\n",
        "            msg_warning(\"Unable to invoke .getncattr({}) on the APS netcdf file in {}.{}.\".format(name,__name__, inspect.stack()[0][3]))\n",
        "        netcdf_metadata[name]=value\n",
        "\n",
        "    msg_debug(\"...finished gathering and posting APS global attribution\")\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "def load_project_meta_data():\n",
        "    if DEBUG:\n",
        "        msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #assumes exposure of environmental variables from OFM Consolidated System (regional meta-data environment file)\n",
        "    #design is to fail gracefully with logging (warn) in case environment file is not loaded\n",
        "    #example:\n",
        "    #    export NETCDF_METADATA_nm_title=\"title\";                                      #Name of file (perhaps includes content)\n",
        "    env_var_names=[\"title\", \"create_date\", \"rp_org\", \"rp_name\", \"rp_phone\", \"rp_email\", \"conventions\", \"nb_latitude\", \"sb_latitude\", \"eb_longitude\", \"wb_longitude\",\n",
        "                \"geo_desc\", \"refsys_code\", \"refsys_title\", \"ism_diststatement\", \"ism_class\", \"ism_ownerproducer\"]\n",
        "\n",
        "    for var_name in env_var_names:\n",
        "        if (DEBUG_DATA):\n",
        "            msg_debug(\"......{}\".format(var_name))\n",
        "        try:\n",
        "            metadata_name=os.getenv(\"NETCDF_METADATA_nm_\"+str(var_name))\n",
        "            metadata_value=os.getenv(\"NETCDF_METADATA_val_\"+str(var_name))\n",
        "            netcdf_metadata[metadata_name]=metadata_value.encode(ENCODING,errors='ignore').strip()\n",
        "        except Exception as e:\n",
        "            msg_warning(\"{} environmental variable not found in {}.{}.\".format(var_name, __name__, inspect.stack()[0][3]))\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "def post_all_metadata (out_netcdf):\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    msg_debug(\"...starting iteration through netcdf dictionary\")\n",
        "\n",
        "    #iterate through the data and push it to the netcdf now\n",
        "    for key in netcdf_metadata:\n",
        "        try:\n",
        "            out_netcdf.setncattr(key, netcdf_metadata[key])\n",
        "        except Exception as e:\n",
        "            msg_warning(\"Unable to invoke .setncattr({}) on the output netcdf file in {}.{}.\".format(key,__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    msg_debug(\"...finished iteration through netcdf dictionary\")\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "def post_metadata(auth_dataset, out_dataset):\n",
        "    if DEBUG:\n",
        "        msg_info(\"Entering {}.{}\".format(__name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #load the project level NetCDF meta-data\n",
        "    load_project_meta_data()\n",
        "\n",
        "    try:\n",
        "        # MINIMAL ATTRIBUTES (NetCDF Best Practices and CF), ensures nothing is missing from project netcdf meta-data\n",
        "        metadata_name=\"create_date\"\n",
        "        metadata_input=str(datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
        "        final_metadata_input = metadata_input.encode(ENCODING,errors='ignore').strip()\n",
        "        netcdf_metadata[metadata_name]=final_metadata_input\n",
        "    except Exception as e:\n",
        "            msg_warning(\"{} unable to set metadata attribute {} in {}.{} to the dictionary.\".format(metadata_name, var_name, __name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #history\n",
        "    try:\n",
        "        metadata_name=\"history\"\n",
        "        metadata_input=str(\"Created with \" + str(get_version()))\n",
        "        final_metadata_input = metadata_input.encode(ENCODING,errors='ignore').strip()\n",
        "        netcdf_metadata[metadata_name]=final_metadata_input\n",
        "    except Exception as e:\n",
        "            msg_warning(\"{} unable to set metadata attribute {} in {}.{} to the dictionary.\".format(metadata_name, var_name, __name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #title\n",
        "    try:\n",
        "        metadata_name=\"title\"\n",
        "        if (metadata_name in netcdf_metadata.keys()):\n",
        "            if (len(netcdf_metadata[metadata_name])> 0):\n",
        "                metadata_input = netcdf_metadata[metadata_name]\n",
        "        else:\n",
        "            metadata_input =  VERSION_TITLE\n",
        "        netcdf_metadata[metadata_name]=metadata_input\n",
        "    except Exception as e:\n",
        "            msg_warning(\"{} unable to set metadata attribute {} in {}.{} to the dictionary.\".format(metadata_name, var_name, __name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #description\n",
        "    try:\n",
        "        metadata_name=\"description\"\n",
        "        if (metadata_name in netcdf_metadata.keys()):\n",
        "            if (len(netcdf_metadata[metadata_name])> 0):\n",
        "                metadata_input = netcdf_metadata[metadata_name]\n",
        "        else:\n",
        "            metadata_input =  METADATA_DESCRIPTION\n",
        "        netcdf_metadata[metadata_name]=metadata_input\n",
        "    except Exception as e:\n",
        "            msg_warning(\"{} unable to set metadata attribute {} in {}.{} to the dictionary.\".format(metadata_name, var_name, __name__, inspect.stack()[0][3]))\n",
        "\n",
        "\n",
        "    #distribution statement\n",
        "    try:\n",
        "        metadata_name=\"distribution_statement\"\n",
        "        if (metadata_name in netcdf_metadata.keys()):\n",
        "            if (len(netcdf_metadata[metadata_name])> 0):\n",
        "                metadata_input = netcdf_metadata[metadata_name]\n",
        "        else:\n",
        "            metadata_input =  METADATA_DISTRIBUTION\n",
        "        netcdf_metadata[metadata_name]=metadata_input\n",
        "    except Exception as e:\n",
        "            msg_warning(\"{} unable to set metadata attribute {} in {}.{} to the dictionary.\".format(metadata_name, var_name, __name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #references\n",
        "    try:\n",
        "        metadata_name=\"references\"\n",
        "        if (metadata_name in netcdf_metadata.keys()):\n",
        "            if (len(netcdf_metadata[metadata_name])> 0):\n",
        "                metadata_input = netcdf_metadata[metadata_name]\n",
        "        else:\n",
        "            metadata_input =  METADATA_REFERENCES\n",
        "        netcdf_metadata[metadata_name]=metadata_input\n",
        "    except Exception as e:\n",
        "            msg_warning(\"{} unable to set metadata attribute {} in {}.{} to the dictionary.\".format(metadata_name, var_name, __name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #institution\n",
        "    try:\n",
        "        metadata_name=\"institution\"\n",
        "        if (metadata_name in netcdf_metadata.keys()):\n",
        "            if (len(netcdf_metadata[metadata_name])> 0):\n",
        "                metadata_input = netcdf_metadata[metadata_name]\n",
        "        else:\n",
        "            metadata_input =  METADATA_INSTITUTION\n",
        "        netcdf_metadata[metadata_name]=metadata_input\n",
        "    except Exception as e:\n",
        "            msg_warning(\"{} unable to set metadata attribute {} in {}.{} to the dictionary.\".format(metadata_name, var_name, __name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #source\n",
        "    try:\n",
        "        metadata_name=\"source\"\n",
        "        if (metadata_name in netcdf_metadata.keys()):\n",
        "            if (len(netcdf_metadata[metadata_name])> 0):\n",
        "                metadata_input = netcdf_metadata[metadata_name]\n",
        "        else:\n",
        "            metadata_input =  METADATA_SOURCE\n",
        "        netcdf_metadata[metadata_name]=metadata_input\n",
        "    except Exception as e:\n",
        "            msg_warning(\"{} unable to set metadata attribute {} in {}.{} to the dictionary.\".format(metadata_name, var_name, __name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #comment\n",
        "    try:\n",
        "        metadata_name=\"comment\"\n",
        "        if (metadata_name in netcdf_metadata.keys()):\n",
        "            if (len(netcdf_metadata[metadata_name])> 0):\n",
        "                metadata_input = netcdf_metadata[metadata_name]\n",
        "        else:\n",
        "            metadata_input =  METADATA_COMMENT\n",
        "        netcdf_metadata[metadata_name]=metadata_input\n",
        "    except Exception as e:\n",
        "            msg_warning(\"{} unable to set metadata attribute {} in {}.{} to the dictionary.\".format(metadata_name, var_name, __name__, inspect.stack()[0][3]))\n",
        "\n",
        "    #take the original APS data and push (as authoritative) into the NetCDF output file\n",
        "    msg_debug(\"...gathering aps meta-data, as authoritative.\")\n",
        "    gather_aps_metadata(auth_dataset)\n",
        "\n",
        "    ####################################################################################################################################\n",
        "    #custom changes, metadata APS has that should be in the default/required Global attribution\n",
        "    #source = \"OFM_seed_generator (OFMSG) 0.0.1\" ;\n",
        "    try:\n",
        "        source=[]\n",
        "        source.append(netcdf_metadata['inputParameters'] + \" from the following files: \" + netcdf_metadata['inputFiles'])\n",
        "        source.append(netcdf_metadata[os.getenv(\"NETCDF_METADATA_nm_rp_org\")])     #long term standards\n",
        "        source.append(netcdf_metadata['file'])                                     #APS value\n",
        "        source.append(netcdf_metadata[\"geospatial_lat_max\"])                       #APS value\n",
        "        source.append(netcdf_metadata[\"geospatial_lat_min\"])                       #APS value\n",
        "        source.append(netcdf_metadata[\"geospatial_lon_max\"])                       #APS value\n",
        "        source.append(netcdf_metadata[\"geospatial_lon_min\"])                       #APS value\n",
        "        source.append(\", \".join(output_variable_domain))\n",
        "        source.append(netcdf_metadata['inputParameters'] + \" from the following files: \" + netcdf_metadata['inputFiles'])\n",
        "        source.append(netcdf_metadata['ism_NoticeText'])                           #Long term standard, alreadyloaded\n",
        "\n",
        "        target=['history', 'institution', 'source']                                #CF / NetCDF Best Practices\n",
        "        target.append(os.getenv(\"NETCDF_METADATA_nm_nb_latitude\"))                 #long term standards\n",
        "        target.append(os.getenv(\"NETCDF_METADATA_nm_sb_latitude\"))                 #long term standards\n",
        "        target.append(os.getenv(\"NETCDF_METADATA_nm_eb_longitude\"))                #long term standards\n",
        "        target.append(os.getenv(\"NETCDF_METADATA_nm_wb_longitude\"))                #long term standards\n",
        "        target.append('prodList')                                                  #corrected because this is the intersection of two files\n",
        "        target.append('lineage')                                                   #long term standards\n",
        "        target.append('distribution_statement')                                    #standard classification concern\n",
        "\n",
        "\n",
        "        for idx in range(0, len(target)):\n",
        "            try:\n",
        "                metadata_name=target[idx]\n",
        "                netcdf_metadata[metadata_name]=source[idx]  #default within consolidated source metadata env vars\n",
        "            except Exception as e:\n",
        "                msg_warning(\"Unable to set metadata attribute {} from {} in {}.{} to the dictionary.\".format(metadata_name, target_key, __name__, inspect.stack()[0][3]))\n",
        "    except Exception as e:\n",
        "            msg_warning(\"Failed to set custom attribution, some meta-data is likely changed or missing from a source document in {}.{}.\".format( __name__, inspect.stack()[0][3]))\n",
        "\n",
        "    ####################################################################################################################################\n",
        "\n",
        "    #final write\n",
        "    msg_debug(\"...write the actual output to the file.\")\n",
        "    post_all_metadata(out_dataset)\n",
        "\n",
        "    if DEBUG:\n",
        "        msg_info(\"Exiting {}.{}\".format(__name__, inspect.stack()[0][3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeqrPyf_xoF-"
      },
      "source": [
        "# Main Method - setup environment to support program execution\n",
        "## Define filenames"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf ./folderOnColab && echo \"Ok, removed.\" || { echo \"No folder to remove.\"; exit 1; }\n",
        "#!mkdir -p ./folderOnColab && echo \"Folder created.\" || { echo \"Failed to create folder, it might already exist.\";  }\n",
        "#!gsutil -m cp -r gs://usfs-gcp-rand-test-data-usc1/public_source/jbooks/ANewHope.txt ./folderOnColab\n",
        "\n",
        "target_folder=\"./folderOnColab\"\n",
        "target_files=[\"npp.201997.MMDD.2019103.0413.D.L4-7DLP.viirsn.MissBight.v08.750m.nc\", \"MissBight_2020010900.nc\", \"example_seed.nc\"]\n",
        "print(f\"Creating a folder ({target_folder}) to store project data.\")\n",
        "subprocess.run([\"mkdir\", \"-p\" , target_folder])\n",
        "if os.path.isdir(target_folder):\n",
        "  for idx, filename in enumerate(target_files):\n",
        "    print(f\"Copying {filename} to target folder: {target_folder}\")\n",
        "    subprocess.run([\"gsutil\", \"-m\" , \"cp\", \"-r\", f\"gs://{BUCKET_NAME}/public_source/jbooks/{filename}\",  target_folder], check=True)\n",
        "else:\n",
        "    print(\"ERROR: Local folder not found/created.  Check the output to ensure your folder is created.\")\n",
        "    print(f\"...target folder: {target_folder}\")\n",
        "    print(\"...if you can't find the problem contact the instructor.\")\n"
      ],
      "metadata": {
        "id": "YonLRmpR55S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SuqPQMlxoF-"
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "# MAIN\n",
        "############################################\n",
        "#try:\n",
        "#     opts, args = getopt.getopt(\n",
        "#          argv, \"VvHhB:b:A:a::O:o:c:\", [\"afile=\", \"bfile=\", \"ofile=\", \"clobber=\"]\n",
        "#      )\n",
        "#except getopt.GetoptError:\n",
        "#      printusage()\n",
        "#      sys.exit(2)\n",
        "#argument_management(opts, args)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "#Setup filenames\n",
        "aps_filename    =target_folder+os.sep+\"npp.201997.MMDD.2019103.0413.D.L4-7DLP.viirsn.MissBight.v08.750m.nc\"\n",
        "biocast_filename=target_folder+os.sep+\"MissBight_2020010900.nc\"\n",
        "output_filename =target_folder+os.sep+\"example_seed.nc\"\n",
        "\n",
        "\n",
        "msg_info(\"Started \" + str(VERSION_NAME) + \".\")\n",
        "\n",
        "msg_info(\" \")\n",
        "msg_debug(\"...library versioning output\")\n",
        "lib_diagnostics()\n",
        "msg_info(\" \")\n",
        "msg_debug(\"    APS Filename: \"+aps_filename)\n",
        "msg_debug(\"BioCast Filename: \"+biocast_filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj6zjcGexoF-"
      },
      "source": [
        "# Read Input NetCDF Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY3V6bBfxoF-"
      },
      "outputs": [],
      "source": [
        "#READ Input NetCDF\n",
        "#argument management has already handled existence, validity, etc.\n",
        "\n",
        "#write defensive code that fails fast, use try/exceptions where it makes sense.\n",
        "try:\n",
        "    aps_netcdf = Dataset(aps_filename, \"r\", format=\"NETCDF4\")\n",
        "    #note that APS output is not packed\n",
        "    aps_netcdf.set_auto_scale(True)\n",
        "except Exception as e:\n",
        "    failure_management(\"Error encountered while reading initial NetCDF\", str(e))\n",
        "\n",
        "if (DEBUG_DATA):\n",
        "    msg_debug(\"NetCDF (aps) opened.\")\n",
        "    get_variable_domain(aps_netcdf, aps_variable_domain)\n",
        "    show_netcdf_details(aps_netcdf, aps_variable_domain)\n",
        "\n",
        "try:\n",
        "    biocast_netcdf = Dataset(biocast_filename, \"r\", format=\"NETCDF4\")\n",
        "    #note that BIOCAST output is not packed\n",
        "    biocast_netcdf.set_auto_scale(True)\n",
        "except Exception as e:\n",
        "    failure_management(\"Error encountered while reading initial BIOCAST NetCDF\", str(e))\n",
        "\n",
        "if (DEBUG_DATA):\n",
        "    msg_debug(\"NetCDF (biocast) opened.\")\n",
        "    get_variable_domain(biocast_netcdf, biocast_variable_domain)\n",
        "    show_netcdf_details(biocast_netcdf, biocast_variable_domain)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfh4tNAAxoF-"
      },
      "source": [
        "# Build Panda.DataFrame() structures for data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "xBagaWD5xoF-"
      },
      "outputs": [],
      "source": [
        "#BUILD DATAFRAMES (Create common geospatial structure)\n",
        "try:\n",
        "    #iterate through the set of NetCDF variables in each input file\n",
        "    #find the union of both variable sets, that's the most you can merge\n",
        "    establish_common_domain(aps_variable_domain, biocast_variable_domain)\n",
        "    output_variable_domain=['chlor_a']\n",
        "except Exception as e:\n",
        "    failure_management(\"Unable to match variables in APS to BIOCAST NetCDF.\", str(e))\n",
        "\n",
        "msg_info(\"APS Coordinates to pd.DataFrame().\")\n",
        "#flatten the output from APS, which is a grid of points, and get the unique list of lats, that's Y\n",
        "#technically APS outputs an x,y set of values that a 1D but that name is not consistent.\n",
        "lat = np.array(aps_netcdf.variables[geospatial_lat_nm][:][:]).flatten()\n",
        "lat = np.unique(lat)\n",
        "lon = np.array(aps_netcdf.variables[geospatial_lon_nm][:][:]).flatten()\n",
        "lon = np.unique(lon)\n",
        "\n",
        "latSeries=pd.Series(lat)\n",
        "lonSeries=pd.Series(lon)\n",
        "\n",
        "#define a Panda.DataFrame()\n",
        "frame={geospatial_lat_nm: latSeries, geospatial_lon_nm: lonSeries}\n",
        "\n",
        "#instantiate a dataframe\n",
        "aps_panda=pd.DataFrame(frame)\n",
        "\n",
        "#ensure the data is cast as expected\n",
        "aps_panda[geospatial_lat_nm].astype('float64')\n",
        "aps_panda[geospatial_lon_nm].astype('float64')\n",
        "\n",
        "#clean-up to ensure memory is kept minimal...not sure if this really matters.\n",
        "del lat, lon, latSeries, lonSeries, frame\n",
        "\n",
        "#biocast is a lat and lon of 1d, we're going to keep it that way\n",
        "msg_info(\"BIOCAST Coordinates to pd.DataFrame().\")\n",
        "\n",
        "#could access via lat=biocast_netcdf[geosptaial_lat_nm]\n",
        "#            then print(lat[0]) for a value.\n",
        "lat   =np.array(biocast_netcdf.variables[geospatial_lat_nm][:][:]).flatten()\n",
        "lon   =np.array(biocast_netcdf.variables[geospatial_lon_nm][:][:]).flatten()\n",
        "latSeries=pd.Series(lat)\n",
        "lonSeries=pd.Series(lon)\n",
        "frame={geospatial_lat_nm: latSeries, geospatial_lon_nm: lonSeries}\n",
        "biocast_panda=pd.DataFrame(frame)\n",
        "biocast_panda[geospatial_lat_nm].astype('float64')\n",
        "biocast_panda[geospatial_lon_nm].astype('float64')\n",
        "\n",
        "del lat, lon, latSeries, lonSeries, frame\n",
        "\n",
        "#keep an independent geographic setup for output (to keep things consistent), but use BIOCAST as the basis for that system\n",
        "msg_info(\"Copy BIOCAST Coordinates to Output pd.DataFrame().\")\n",
        "output_panda = biocast_panda.copy()\n",
        "\n",
        "take_out_trash()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nOkoRNXxoF-"
      },
      "source": [
        "# Gather NetCDF Variable data and store for future use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMD61xdoxoF-"
      },
      "outputs": [],
      "source": [
        "#dictionary to hold 1 to N data frames of products.\n",
        "for var in output_variable_domain:\n",
        "    msg_debug(\"Processing {}\".format(var))\n",
        "    varAry=np.array(aps_netcdf.variables[var][:][:])\n",
        "    df = pd.DataFrame(data=varAry)\n",
        "    aps_products[var]=df\n",
        "\n",
        "    #handle cloud coverage by setting non-valid data to 0.0\n",
        "    #note that _FillValue should be used by it has an \"improper name\"\n",
        "    #and the value is set packed versus unpacked.\n",
        "    aps_mangled=aps_products[var].values\n",
        "    aps_fixed = np.where(aps_mangled > MINIMAL_VALUE, aps_mangled, 0.0)\n",
        "    df = pd.DataFrame(data=aps_fixed)\n",
        "    aps_products[var]=df\n",
        "\n",
        "    varAry=np.array(biocast_netcdf.variables[var][0][0][:][:])\n",
        "    df = pd.DataFrame(data=varAry)\n",
        "    biocast_products[var] = df\n",
        "\n",
        "#clean up varaible use\n",
        "del varAry, df\n",
        "\n",
        "#force garbage collection, read the routine, this is not native.\n",
        "take_out_trash()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYkyKRAExoF-"
      },
      "source": [
        "# Get a Quick view of the APS Product (Original)\n",
        "## Note that `var` is the last variable processed in an array of potential products unioned between both inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJftkTHFxoF-"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize = (40,10))\n",
        "plt.imshow(aps_products[var],cmap='inferno')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atsKw5YVxoF-"
      },
      "source": [
        "# Detailed debugging example\n",
        "## Feel free to skip this execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEFtnsFKxoF-"
      },
      "outputs": [],
      "source": [
        "for var in output_variable_domain:\n",
        "    if (DEBUG_DATA):\n",
        "        msg_debug(\"APS columns:\")\n",
        "        aps_cols=show_columns_true(aps_products[var].columns)\n",
        "        msg_debug(\"APS DataFrame details:\")\n",
        "        show_data(aps_products[var])\n",
        "        msg_debug(\"...\" + aps_cols)\n",
        "        msg_debug(\"BIOCAST columns:\")\n",
        "        biocast_cols=show_columns_true(biocast_products[var].columns)\n",
        "        msg_debug(\"BIOCAST DataFrame details:\")\n",
        "        show_data(biocast_products[var])\n",
        "        msg_debug(\"...\" + biocast_cols)\n",
        "\n",
        "try:\n",
        "    del aps_cols, biocast_cols\n",
        "    take_out_trash()\n",
        "except Exception as e:\n",
        "    print(\"IGNORE -> Failed to deallocate memory that wasn't used.\", str(e))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72cKJposxoF_"
      },
      "source": [
        "# Product Merge\n",
        "### Loop through each product in the dictionaries, look inside APS data and if you find a \"gap\" fill it with biocast saving results to the output_products dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH8-Gfc9xoF_"
      },
      "outputs": [],
      "source": [
        "for var in output_variable_domain:\n",
        "    msg_info(\"Processing \" + str(var))\n",
        "    msg_debug(\"...biocast shape \" + str(biocast_products[var].values.shape))\n",
        "    msg_debug(\".......aps shape \" + str(aps_products[var].values.shape))\n",
        "    outputAry_shape=biocast_products[var].values.shape\n",
        "    lat_y = outputAry_shape[0]\n",
        "    lon_x = outputAry_shape[1]\n",
        "    outputAry = np.full([lat_y,lon_x],0.0)\n",
        "    outputFilter = np.full([lat_y,lon_x],0.0)                               #Assign APS(1) versus BIOCAST (0) to a mask for a Convolution matrix\n",
        "    msg_debug(\"....output shape \" + str(outputAry.shape))\n",
        "    apsAry=aps_products[var].values\n",
        "    biocastAry=biocast_products[var].values\n",
        "    msg_debug(\"....looping between datasets and assigning data to satellite coverage holes.\")\n",
        "    for index,value in np.ndenumerate(apsAry):\n",
        "        if (value > MINIMAL_VALUE):\n",
        "            outputAry[index]=value\n",
        "            outputFilter[index]=1\n",
        "        else:\n",
        "            outputAry[index]=biocastAry[index]\n",
        "    msg_debug(\"....finished looping for \" + str(var))\n",
        "\n",
        "    msg_debug(\"....applying gausian filter.\")\n",
        "    sigma_x = .5\n",
        "    sigma_y = .5\n",
        "    sigma = [sigma_y, sigma_x]\n",
        "    outputAry=sp.ndimage.filters.gaussian_filter(outputAry, sigma, mode='constant')\n",
        "    df = pd.DataFrame(data=outputAry)\n",
        "    output_products[var]=df\n",
        "try:\n",
        "    del lat_y, lon_x, apsAry, biocastAry\n",
        "    take_out_trash()\n",
        "except Exception as e:\n",
        "    print(\"IGNORE -> Failed to deallocate memory that wasn't used.\", str(e))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDLYKbD2xoF_"
      },
      "source": [
        "# Create the output NetCDF\n",
        "### Now that you have marshaled and merged all data, create the output NetCDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H64Fq2ZVxoF_"
      },
      "outputs": [],
      "source": [
        "#CREATE Output NetCDF\n",
        "#argument management has already handled existence, validity, etc.\n",
        "try:\n",
        "    output_netcdf = Dataset(output_filename, \"w\", format=\"NETCDF4\")\n",
        "    if DEBUG:\n",
        "        msg_debug(\"NetCDF write initiated.\")\n",
        "        msg_debug(\".......version:\" + str(output_netcdf.data_model))\n",
        "\n",
        "    post_metadata(aps_netcdf, output_netcdf)\n",
        "\n",
        "    #define dimensions using biocast as the model of basis\n",
        "    msg_debug(\".......dimensions\")\n",
        "    for dim in aps_netcdf.dimensions.items():\n",
        "        output_netcdf.createDimension(str(dim[1].name), int(dim[1].size))\n",
        "        msg_debug(\"............wrote \" + str(dim[1].name) +\" (\" + str(dim[1].size) + \")\")\n",
        "\n",
        "\n",
        "    msg_debug(\".......variables (products)\")\n",
        "    final_output_domain=output_variable_domain\n",
        "    for name, variable in aps_netcdf.variables.items():\n",
        "        if (name in final_output_domain):\n",
        "            msg_debug(\"netcdf_var=output_netcdf.createVariable({}, \\\"f8\\\", ({},) )\".format(name, \"\\\"lat\\\",\\\"lon\\\"\"))\n",
        "            netcdf_var=output_netcdf.createVariable(str(name), \"f8\", (\"lat\",\"lon\",) )\n",
        "            msg_debug(\"............{}\".format(name))\n",
        "            for attrname in variable.ncattrs():\n",
        "                attrval=getattr(variable, attrname)\n",
        "                netcdf_var.setncattr_string(attrname,attrval)\n",
        "                msg_debug(\"...............wrote {} = {}\".format(attrname,attrval))\n",
        "            netcdf_var.setncattr_string(\"_FillValue\",-32767.0)\n",
        "            output_products[name].fillna(0.0)\n",
        "            netcdf_var[:] = output_products[name].values\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    failure_management(\"Error encountered while trying to write the output NetCDF\", str(e))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsbwboCCxoF_"
      },
      "source": [
        "# Get a Quick view of the Output Product (Final Merged Output)\n",
        "## Note that `var` is the last variable processed in an array of potential products unioned between both inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHVx2TmgxoF_"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize = (40,10))\n",
        "plt.imshow(output_products[var],cmap='nipy_spectral')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8NsAovoxoF_"
      },
      "outputs": [],
      "source": [
        "#CLOSE ALL FILES\n",
        "msg_info(\"Closing out NetCDF files.\")\n",
        "try:\n",
        "    output_netcdf.close()\n",
        "except Exception as e:\n",
        "    failure_management(\"Failed to close output (seed) NetCDF, you could have a corrupted file.\", str(e))\n",
        "\n",
        "try:\n",
        "    aps_netcdf.close()\n",
        "except Exception as e:\n",
        "    failure_management(\"Failed to close the input APS NetCDF.  Caution, this could result in a corrupted file.\", str(e))\n",
        "\n",
        "try:\n",
        "    biocast_netcdf.close()\n",
        "except Exception as e:\n",
        "    failure_management(\"Failed to close the input BIOCAST NetCDF.  Caution, this could result in a corrupted file.\", str(e))\n",
        "\n",
        "msg_info(\"Finished \" + str(VERSION_NAME) + \".\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}