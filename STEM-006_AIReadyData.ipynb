{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jg3iJooMQjWA"
   },
   "source": [
    "# Artificial Intelligence\n",
    "## AI Ready Data - 006\n",
    "### Download, curate, and process weather and tree data.\n",
    "\n",
    "<center>\n",
    "<table align=\"center\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/christophergarthwood/jbooks/blob/main/STEM-006_AIReadyData.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/notebooks?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Link to Colab Enterprise\n",
    "    </a>\n",
    "  </td>   \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/christophergarthwood/jbooks/blob/main/STEM-006_AIReadyData.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Link to Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "</center>\n",
    "</br></br></br>\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Christopher G Wood](https://github.com/christophergarthwood)  |\n",
    "\n",
    "# Overview\n",
    "\n",
    "Using various data sources we will download, review, and package data in various formats exploring the options for \"AI Ready\" data and what that means.\n",
    "\n",
    "## What is a \"**AI Ready**\" Data?\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "#### Data Formats\n",
    "+ [List of ML File Formats](https://github.com/trailofbits/ml-file-formats)\n",
    "+ [ML Guide to Data Formats](https://www.hopsworks.ai/post/guide-to-file-formats-for-machine-learning)\n",
    "+ [Why are ML Data Structures Different?](https://stackoverflow.blog/2023/01/04/getting-your-data-in-shape-for-machine-learning/)\n",
    "\n",
    "#### FAIR\n",
    "+ [FAIR and AI-Ready](https://repository.niddk.nih.gov/public/NIDDKCR_Office_Hours_AI-Readiness_and_Preparing_AI-Ready_+Datasets_12_2023.pdf)\n",
    "+ [AI-Ready-Data](https://www.rishabhsoft.com/blog/ai-ready-data)\n",
    "+ [AI-Ready FAIR Data](https://medium.com/@sean_hill/ai-ready-fair-data-accelerating-science-through-responsible-ai-and-data-stewardship-3b4f21c804fd)\n",
    "+ [AI-Ready Data ... Quality](https://www.elucidata.io/blog/building-ai-ready-data-why-quality-matters-more-than-quantity)\n",
    "+ [AI-Ready Data Explained](https://acodis.io/hubfs/pdfs/AI-ready%20data%20Explained%20Whitepaper%20(1).pdf)\n",
    "\n",
    "+ [GCP with BigQuery DataFrames](https://cloud.google.com/blog/products/data-analytics/building-aiml-apps-in-python-with-bigquery-dataframes)\n",
    "\n",
    "#### Format Libraries / Standards\n",
    "+ [Earth Science Information partners (ESIP)](https://www.esipfed.org/checklist-ai-ready-data/)\n",
    "+ [Zarr - Storage of N-dimensional arrays (tensors)](https://zarr.dev/#description)\n",
    "  + [Zarr explained](https://aijobs.net/insights/zarr-explained/)\n",
    "+ [Apache Parquet](https://parquet.apache.org/)\n",
    "  + [All about Parquet](https://medium.com/data-engineering-with-dremio/all-about-parquet-part-01-an-introduction-b62a5bcf70f8)\n",
    "+ [PySTAC - SpatioTemporal Asset Catalogs](https://pystac.readthedocs.io/en/stable/)\n",
    "  + [John Hogland's Spatial Modeling Tutorials](https://github.com/jshogland/SpatialModelingTutorials/blob/main/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1737665372909,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "M3qlCehNBu-_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's define some variables (information holders) for our project overall\n",
    "\n",
    "global PROJECT_ID, BUCKET_NAME, LOCATION\n",
    "BUCKET_NAME = \"cio-training-vertex-colab\"\n",
    "PROJECT_ID = \"usfs-ai-bootcamp\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "BOLD_START = \"\\033[1m\"\n",
    "BOLD_END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508,
     "referenced_widgets": [
      "1015b1ce380943d48c7a6386444c768a",
      "f5405303114146a28663cb1c46db7beb",
      "bdc74a6d20a041819b6fc6d1c3f85a25",
      "033cb5a8c21c4ca5a13b5d03e8b78fa0",
      "2b447186de7a4571821ca59295744ce3",
      "cf363ab273bf4eeba7fbea4d79ed3a64",
      "479233081fbc44c8afe9a044ebb95faf",
      "202c8798fdda4778bf09a2124c37a6c7",
      "ad3b9f4855d541e49303c03fb9f07db7",
      "0d8988d435644a928829ef71435a3e83",
      "ebb9ca01cf52473699f21f15c5ec7d34",
      "689fe090492947608518efc9f3bd6d5d",
      "dd3de71c792f4762af3b280ac9b7f5e6",
      "fc3fc48bdebe4501998819fbda36152c",
      "f196621910824a00a97d06e5433cb294",
      "31b012d7d41a4d06933aa38e8e3bd74a"
     ]
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1737665373417,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "3TrA1A5nIeCV",
    "outputId": "65ad0051-dd12-4511-e88f-62bf0f35300e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4350f2a342643cfa8e08ef3962508db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now create a means of enforcing project id selection\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def wait_for_button_press():\n",
    "\n",
    "    button_pressed = False\n",
    "\n",
    "    # Create widgets\n",
    "    html_widget = widgets.HTML(\n",
    "        value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "\n",
    "        <table><tr><td>\n",
    "            <span style=\"font-family: Tahoma;font-size: 18\">\n",
    "              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n",
    "              Please verify that you are in the appropriate project and that the:</br>\n",
    "              <center><code><b>PROJECT_ID</b></code> </br></center>\n",
    "              aligns with the Project Id in the upper left corner of this browser and that the location:\n",
    "              <center><code><b>LOCATION</b></code> </br></center>\n",
    "              aligns with the instructions provided.\n",
    "            </span>\n",
    "          </td></tr></table></br></br>\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    project_list = [\n",
    "        \"usfs-ai-bootcamp\",\n",
    "        \"usfa-ai-advanced-training\",\n",
    "        \"I will setup my own\",\n",
    "    ]\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=project_list,\n",
    "        value=project_list[0],\n",
    "        description=\"Set Your Project:\",\n",
    "    )\n",
    "\n",
    "    html_widget2 = widgets.HTML(\n",
    "        value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "          \"\"\"\n",
    "    )\n",
    "\n",
    "    button = widgets.Button(description=\"Accept\")\n",
    "\n",
    "    # Function to handle the selection change\n",
    "    def on_change(change):\n",
    "        global PROJECT_ID\n",
    "        if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
    "            # print(\"Selected option:\", change['new'])\n",
    "            PROJECT_ID = change[\"new\"]\n",
    "\n",
    "    # Observe the dropdown for changes\n",
    "    dropdown.observe(on_change)\n",
    "\n",
    "    def on_button_click(b):\n",
    "        nonlocal button_pressed\n",
    "        global PROJECT_ID\n",
    "        button_pressed = True\n",
    "        # button.disabled = True\n",
    "        button.close()  # Remove the button from display\n",
    "        with output:\n",
    "            # print(f\"Button pressed...continuing\")\n",
    "            # print(f\"Selected option: {dropdown.value}\")\n",
    "            PROJECT_ID = dropdown.value\n",
    "\n",
    "    button.on_click(on_button_click)\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Create centered layout\n",
    "    centered_layout = widgets.VBox(\n",
    "        [\n",
    "            html_widget,\n",
    "            widgets.HBox([dropdown, button]),\n",
    "            html_widget2,\n",
    "        ],\n",
    "        layout=widgets.Layout(\n",
    "            display=\"flex\", flex_flow=\"column\", align_items=\"center\", width=\"100%\"\n",
    "        ),\n",
    "    )\n",
    "    # Display the layout\n",
    "    display(centered_layout)\n",
    "\n",
    "\n",
    "wait_for_button_press()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zramkw-P93C-"
   },
   "source": [
    "## Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1737666212893,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "shY7a4DVQjWB",
    "outputId": "606fe1d2-f3de-47cf-f457-bb449cca5b52",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are likely running this notebook with Jupyter iPython runtime at 2025-03-13 16:50:47.880951 in the usfs-ai-bootcamp lab.\n"
     ]
    }
   ],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# - Google Colab Check\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "import datetime\n",
    "\n",
    "RunningInCOLAB = False\n",
    "RunningInCOLAB = \"google.colab\" in str(get_ipython())\n",
    "current_time = datetime.datetime.now()\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    print(\n",
    "        f\"You are running this notebook in Google Colab at {current_time} in the {BOLD_START}{PROJECT_ID}{BOLD_END}lab.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"You are likely running this notebook with Jupyter iPython runtime at {current_time} in the {PROJECT_ID} lab.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZVkISRuLURi"
   },
   "source": [
    "## Library Management\n",
    "### Load Libraries necessary for this operation via pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1737665373426,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "logDyNfnLURj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import key libraries necessary to support dynamic installation of additional libraries\n",
    "import sys\n",
    "\n",
    "# Use subprocess to support running operating system commands from the program, using the \"bang\" (!)\n",
    "# symbology is supported, however that does not translate to an actual python script, this is a more\n",
    "# agnostic approach.\n",
    "import subprocess\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74882,
     "status": "ok",
     "timestamp": 1737665448288,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "ldXG-5fhsV1e",
    "outputId": "5941b5ee-93f4-41df-ccc2-8915e0e73ed7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library backoff already installed.\n",
      "Installing library python-dotenv\n",
      "Library seaborn already installed.\n",
      "Library piexif already installed.\n",
      "Library unidecode already installed.\n",
      "Library icecream already installed.\n",
      "Library watermark already installed.\n",
      "Installing library watermark[GPU]\n",
      "Library rich already installed.\n",
      "Installing library rich[jupyter]\n",
      "Library numpy already installed.\n",
      "Library pydot already installed.\n",
      "Installing library polars[all]\n",
      "Installing library dask[complete]\n",
      "Library xarray already installed.\n",
      "Library pandas already installed.\n",
      "Library pystac already installed.\n",
      "Installing library pystac[jinja2]\n",
      "Installing library pystac[orjson]\n",
      "Installing library pystac[validation]\n",
      "Library fastparquet already installed.\n",
      "Library zarr already installed.\n",
      "Library gdown already installed.\n"
     ]
    }
   ],
   "source": [
    "# Identify the libraries you'd like to add to this Runtime environment.\n",
    "\n",
    "libraries = [\n",
    "    \"backoff\",\n",
    "    \"python-dotenv\",\n",
    "    \"seaborn\",\n",
    "    \"piexif\",\n",
    "    \"unidecode\",\n",
    "    \"icecream\",\n",
    "    \"watermark\",\n",
    "    \"watermark[GPU]\",\n",
    "    \"rich\",\n",
    "    \"rich[jupyter]\",\n",
    "    \"numpy\",\n",
    "    \"pydot\",\n",
    "    \"polars[all]\",\n",
    "    \"dask[complete]\",\n",
    "    \"xarray\",\n",
    "    \"pandas\",\n",
    "    \"pystac\",\n",
    "    \"pystac[jinja2]\",\n",
    "    \"pystac[orjson]\",\n",
    "    \"pystac[validation]\",\n",
    "    \"fastparquet\",\n",
    "    \"zarr\",\n",
    "    \"gdown\",\n",
    "]\n",
    "\n",
    "# Loop through each library and test for existence, if not present install quietly\n",
    "for library in libraries:\n",
    "    if library == \"Pillow\":\n",
    "        spec = importlib.util.find_spec(\"PIL\")\n",
    "    else:\n",
    "        spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "        print(\"Installing library \" + library)\n",
    "        subprocess.run([\"pip\", \"install\", library, \"--quiet\"], check=True)\n",
    "    else:\n",
    "        print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "# Specialized install for GPU enabled capability with CUDF\n",
    "# pip install --extra-index-url=https://pypi.nvidia.com \"cudf-cu12==25.2.*\" \"dask-cudf-cu12==25.2.*\" \"cuml-cu12==25.2.*\" \"cugraph-cu12==25.2.*\" \"nx-cugraph-cu12==25.2.*\" \"cuspatial-cu12==25.2.*\"     \"cuproj-cu12==25.2.*\" \"cuxfilter-cu12==25.2.*\" \"cucim-cu12==25.2.*\"\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--extra-index-url=https://pypi.nvidia.com\",\n",
    "            \"cudf-cu12\",\n",
    "            \"dask-cudf-cu12\",\n",
    "            \"--quiet\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "except (subprocess.CalledProcessError, RuntimeError, Exception) as e:\n",
    "    print(repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO_Hq5eq9joH"
   },
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5041,
     "status": "ok",
     "timestamp": 1737665453315,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "PJuXEPlkSo9p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Import additional libraries that add value to the project related to NLP\n",
    "\n",
    "# - Set of libraries that perhaps should always be in Python source\n",
    "import backoff\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "import gc\n",
    "import getopt\n",
    "import glob\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "import socket\n",
    "import sys\n",
    "import textwrap\n",
    "import tqdm\n",
    "import traceback\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "#- Datastructures\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "#- Profiling\n",
    "from time import perf_counter\n",
    "import gc\n",
    "import io\n",
    "import tracemalloc\n",
    "import psutil\n",
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "#- Text formatting\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.traceback import install\n",
    "from tabulate import tabulate\n",
    "import locale\n",
    "\n",
    "# - Displays system info\n",
    "from watermark import watermark as the_watermark\n",
    "from py3nvml import py3nvml\n",
    "\n",
    "# - Additional libraries for this work\n",
    "import math\n",
    "from base64 import b64decode\n",
    "from IPython.display import Image, Markdown\n",
    "import pandas, IPython.display as display, io, jinja2, base64\n",
    "from IPython.display import clear_output  # used to support real-time plotting\n",
    "import requests\n",
    "import unidecode\n",
    "import pydot\n",
    "\n",
    "# - Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import dask as da\n",
    "import xarray as xr\n",
    "import cupy_xarray  # never actually invoked in source itself use ds=ds.cupy.as_cupy()\n",
    "import pystac as pys\n",
    "import pystac\n",
    "from pystac.utils import datetime_to_str\n",
    "\n",
    "# from stacframes import df_from\n",
    "import fastparquet as fq\n",
    "import zarr\n",
    "from zarr import Group\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "try:\n",
    "    import cudf\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import cupy\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Tensorflow and related AI libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import data as tf_data\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "\n",
    "# - Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.cbook import get_sample_data\n",
    "from matplotlib.offsetbox import AnnotationBbox, DrawingArea, OffsetImage, TextArea\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.patches import Circle\n",
    "from PIL import Image as PIL_Image\n",
    "import PIL.ImageOps\n",
    "import matplotlib.image as mpimg\n",
    "from imageio import imread\n",
    "import seaborn as sns\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from pylab import *\n",
    "\n",
    "# - Image meta-data for Section 508 compliance\n",
    "import piexif\n",
    "from piexif.helper import UserComment\n",
    "\n",
    "# - Progress bar\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataclass used to represent each metric used during execution\n",
    "#\n",
    "@dataclass\n",
    "class runtime_metrics:\n",
    "    id: str\n",
    "\n",
    "    # see @Profile\n",
    "    runtime: float = field(init=False, default=0.0)\n",
    "\n",
    "    # reference: https://docs.python.org/3/library/profile.html\n",
    "    profile_data: cProfile.Profile = field(init=False)\n",
    "\n",
    "    # reference: https://www.geeksforgeeks.org/how-to-get-file-size-in-python/\n",
    "    file_size: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "    \n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_throughput: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_throughput: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_read_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_write_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_read_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_write_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # calculated in MBs, reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "    mem_current: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # calculated in MBs, reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "    mem_peak: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "                Id---------------------------------------------\n",
    "                                      Id: {self.id}\n",
    "                Runtime----------------------------------------\n",
    "                                 Runtime: {self.runtime:,.2f} milliseconds\n",
    "\n",
    "                I/O Size---------------------------------------\n",
    "                               File Size: {self.file_size:,.2f} bytes\n",
    "\n",
    "                I/O Counts-------------------------------------\n",
    "                      Targeted disk read: {self.io_disk_read_count:,.2f} counts\n",
    "                     Targeted disk write: {self.io_disk_write_count:,.2f} counts\n",
    "                       General disk read: {self.io_os_read_count:,.2f} counts\n",
    "                      General disk write: {self.io_os_write_count:,.2f} counts\n",
    "\n",
    "                I/O Time---------------------------------------\n",
    "                 Targeted disk read time: {self.io_disk_read_time:,.2f} milliseconds\n",
    "                Targeted disk write time: {self.io_disk_write_time:,.2f} milliseconds\n",
    "                  General disk read time: {self.io_os_read_time:,.2f} milliseconds\n",
    "                 General disk write time: {self.io_os_write_time:,.2f} milliseconds\n",
    "\n",
    "                Memory------------------------------------------\n",
    "                                 Current: {self.mem_current:,.2f} MB\n",
    "                                    Peak: {self.mem_peak:,.2f} MB\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "    #def __repr__(self):\n",
    "    #    return f'{self.__class__.__name__}(name={self.name!r}, unit_price={self.unit_price!r}, quantity={self.quantity_on_hand!r})'\n",
    "\n",
    "    # TODO - CGW\n",
    "    # def __post_init__(self):\n",
    "    #    self.id = f'{self.phrase}_{self.word_type.name.lower()}'\n",
    "\n",
    "    # worthy consideration - https://www.geeksforgeeks.org/psutil-module-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Profiling function custom created to track IO, memory, and runtme.\n",
    "# Reference: https://jiffyclub.github.io/snakeviz/\n",
    "# Reference: https://www.machinelearningplus.com/python/cprofile-how-to-profile-your-python-code/\n",
    "# Reference: https://cloud.google.com/stackdriver/docs/instrumentation/setup/python\n",
    "# Reference: https://www.turing.com/kb/python-code-with-cprofile\n",
    "\n",
    "def profile(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "\n",
    "        # custom metrics values\n",
    "        current_memories = 0.0\n",
    "        peak_memories = 0.0\n",
    "        current_metric = runtime_metrics(id=func.__name__)\n",
    "        disk = \"sdc\"\n",
    "\n",
    "        #####################################################################################################\n",
    "        # - Cprofiler startup\n",
    "        # Reference: https://www.google.com/search?client=firefox-b-1-d&q=python+example+use+of+cprofle+for+a+single+function#cobssid=s\n",
    "        #####################################################################################################\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        #####################################################################################################\n",
    "        # - Memory tracking\n",
    "        #  Reference: https://docs.python.org/3/library/tracemalloc.html\n",
    "        #  Reference: https://www.kdnuggets.com/how-to-trace-memory-allocation-in-python\n",
    "        #  Reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "        #####################################################################################################\n",
    "        tracemalloc.start()\n",
    "\n",
    "        #####################################################################################################\n",
    "        # - Disk tracking\n",
    "        # Reference: https://stackoverflow.com/questions/16945664/insight-needed-into-python-psutil-output#:~:text=1%20Answer%201%20%C2%B7%20read_count:%20number%20of,write_bytes:%20number%20of%20bytes%20written%20%C2%B7%20read_time:\n",
    "        #####################################################################################################\n",
    "        iocnt1 = psutil.disk_io_counters(perdisk=True)[disk]\n",
    "        disk_io_counters1 = psutil.disk_io_counters()\n",
    "        read_bytes_start = iocnt1.read_bytes\n",
    "        write_bytes_start = iocnt1.write_bytes\n",
    "        read_counters_start = iocnt1.read_count\n",
    "        write_counters_start = iocnt1.write_count\n",
    "        read_time_start = iocnt1.read_time\n",
    "        write_time_start = iocnt1.write_time\n",
    "        \n",
    "        read_os_bytes_start = disk_io_counters1.read_bytes\n",
    "        write_os_bytes_start = disk_io_counters1.write_bytes\n",
    "        read_os_counters_start = disk_io_counters1.read_count\n",
    "        write_os_counters_start = disk_io_counters1.write_count\n",
    "        read_os_time_start = disk_io_counters1.read_time\n",
    "        write_os_time_start = disk_io_counters1.write_time\n",
    "        \n",
    "        #####################################################################################################\n",
    "        # - Actual function call\n",
    "        #####################################################################################################\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        # disk close out\n",
    "        # targeted I/O\n",
    "        iocnt2 = psutil.disk_io_counters(perdisk=True)[\"sdc\"]\n",
    "        disk_io_counters2 = psutil.disk_io_counters()\n",
    "\n",
    "        #targeted disk\n",
    "        read_bytes_end = iocnt2.read_bytes\n",
    "        write_bytes_end = iocnt2.write_bytes\n",
    "        read_counters_end = iocnt2.read_count\n",
    "        write_counters_end = iocnt2.write_count\n",
    "        read_time_end = iocnt2.read_time\n",
    "        write_time_end = iocnt2.write_time\n",
    "        #general OS\n",
    "        read_os_bytes_end = disk_io_counters2.read_bytes\n",
    "        write_os_bytes_end = disk_io_counters2.write_bytes\n",
    "        read_os_counters_end = disk_io_counters2.read_count\n",
    "        write_os_counters_end = disk_io_counters2.write_count\n",
    "        read_os_time_end = disk_io_counters2.read_time\n",
    "        write_os_time_end = disk_io_counters2.write_time\n",
    "\n",
    "        #targeted disk\n",
    "        read_throughput = (read_bytes_end - read_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        write_throughput = (write_bytes_end - write_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        read_counters = (read_counters_end - read_counters_start)\n",
    "        write_counters = (read_counters_end - read_counters_start)\n",
    "        read_time =  (read_time_end - read_time_start)\n",
    "        write_time = (write_time_end - write_time_start)\n",
    "        current_metric.io_disk_read_throughput = read_throughput\n",
    "        current_metric.io_disk_write_throughput = write_throughput\n",
    "        current_metric.io_disk_read_count = read_counters\n",
    "        current_metric.io_disk_write_count = write_counters\n",
    "        current_metric.io_disk_read_time = read_time\n",
    "        current_metric.io_disk_write_time = write_time\n",
    "\n",
    "        #general OS\n",
    "        read_os_throughput = (read_os_bytes_end - read_os_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        write_os_throughput = (write_os_bytes_end - write_os_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        read_os_counters = (read_os_counters_end - read_os_counters_start)\n",
    "        write_os_counters = (read_os_counters_end - read_os_counters_start)\n",
    "        read_os_time =  (read_os_time_end - read_os_time_start)\n",
    "        write_os_time = (write_os_time_end - write_os_time_start)\n",
    "        current_metric.io_os_read_throughput = read_os_throughput\n",
    "        current_metric.io_os_write_throughput = write_os_throughput\n",
    "        current_metric.io_os_read_count = read_os_counters\n",
    "        current_metric.io_os_write_count = write_os_counters\n",
    "        current_metric.io_os_read_time = read_os_time\n",
    "        current_metric.io_os_write_time = write_os_time\n",
    "\n",
    "\n",
    "\n",
    "        # memory close\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()        \n",
    "        current_metric.mem_current = current / (1024 * 1024)\n",
    "        current_metric.mem_peak = peak / (1024 * 1024)\n",
    "        tracemalloc.clear_traces()\n",
    "\n",
    "\n",
    "        # CProfiler disabled\n",
    "        pr.disable()\n",
    "        current_metric.profile_data=pr\n",
    "        # s = io.StringIO()\n",
    "        # sortby = SortKey.CUMULATIVE\n",
    "        # ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "        # ps.print_stats()\n",
    "        # print(s.getvalue())\n",
    "        end_time = time.perf_counter()\n",
    "        # other characteristics\n",
    "        current_metric.runtime = end_time - start_time\n",
    "\n",
    "        print(current_metric)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Complex Function to Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def complex_function():\n",
    "    # Define the size of the matrix\n",
    "    matrix_size = 2048\n",
    "    # Generate two random matrices\n",
    "    matrix_a = np.random.rand(matrix_size, matrix_size)\n",
    "    matrix_b = np.random.rand(matrix_size, matrix_size)\n",
    "    result_matrix = np.matmul(matrix_a, matrix_b)\n",
    "    np.savez(\n",
    "        \"./folderOnColab/data/local_test.npy\",\n",
    "        the_matrix=result_matrix,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FUa8QJT9tw_"
   },
   "source": [
    "## Function Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lib Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1737665453699,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "v_CqUVLZ98Mz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lib_diagnostics() -> None:\n",
    "\n",
    "    import pkg_resources\n",
    "\n",
    "    package_name_length = 20\n",
    "    package_version_length = 10\n",
    "\n",
    "    # Show notebook details\n",
    "    #%watermark?\n",
    "    #%watermark --github_username christophergwood --email christopher.g.wood@gmail.com --date --time --iso8601 --updated --python --conda --hostname --machine --githash --gitrepo --gitbranch --iversions --gpu\n",
    "    # Watermark\n",
    "    print(\n",
    "        the_watermark(\n",
    "            author=f\"{AUTHOR_NAME}\",\n",
    "            github_username=f\"GITHUB_USERNAME\",\n",
    "            email=f\"{AUTHOR_EMAIL}\",\n",
    "            iso8601=True,\n",
    "            datename=True,\n",
    "            current_time=True,\n",
    "            python=True,\n",
    "            updated=True,\n",
    "            hostname=True,\n",
    "            machine=True,\n",
    "            gitrepo=True,\n",
    "            gitbranch=True,\n",
    "            githash=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f\"{BOLD_START}Packages:{BOLD_END}\")\n",
    "    print(\"\")\n",
    "    # Get installed packages\n",
    "    the_packages = [\n",
    "        \"nltk\",\n",
    "        \"numpy\",\n",
    "        \"os\",\n",
    "        \"pandas\",\n",
    "        \"keras\",\n",
    "        \"seaborn\",\n",
    "        \"fastparquet\",\n",
    "        \"zarr\",\n",
    "        \"dask\",\n",
    "        \"pystac\",\n",
    "        \"polars\",\n",
    "        \"xarray\",\n",
    "    ]  # Functions are like legos that do one thing, this function outputs library version history of effort.\n",
    "\n",
    "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "    for package_idx, package_name in enumerate(installed):\n",
    "        if package_name in the_packages:\n",
    "            installed_version = installed[package_name]\n",
    "            print(\n",
    "                f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\"\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        print(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
    "        print(\n",
    "            f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        print(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
    "            print(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
    "            print(f\"{'     current':<40}#: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"No GPU available, using CPU.\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 508 Compliance Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1737665453948,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "Nl_kxpFUKKD5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Routines designed to support adding ALT text to an image generated through Matplotlib.\n",
    "\n",
    "\n",
    "def capture(figure):\n",
    "    buffer = io.BytesIO()\n",
    "    figure.savefig(buffer)\n",
    "    # return F\"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "    return f\"data:image/jpg;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "\n",
    "\n",
    "def make_accessible(figure, template, **kwargs):\n",
    "    return display.Markdown(\n",
    "        f\"\"\"![]({capture(figure)} \"{template.render(**globals(), **kwargs)}\")\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# requires JPG's or TIFFs\n",
    "def add_alt_text(image_path, alt_text):\n",
    "    try:\n",
    "        if os.path.isfile(image_path):\n",
    "            img = PIL_Image.open(image_path)\n",
    "            if \"exif\" in img.info:\n",
    "                exif_dict = piexif.load(img.info[\"exif\"])\n",
    "            else:\n",
    "                exif_dict = {}\n",
    "\n",
    "            w, h = img.size\n",
    "            if \"0th\" not in exif_dict:\n",
    "                exif_dict[\"0th\"] = {}\n",
    "            exif_dict[\"0th\"][piexif.ImageIFD.XResolution] = (w, 1)\n",
    "            exif_dict[\"0th\"][piexif.ImageIFD.YResolution] = (h, 1)\n",
    "\n",
    "            software_version = \" \".join(\n",
    "                [\"STEM-001 with Python v\", str(sys.version).split(\" \")[0]]\n",
    "            )\n",
    "            exif_dict[\"0th\"][piexif.ImageIFD.Software] = software_version.encode(\n",
    "                \"utf-8\"\n",
    "            )\n",
    "\n",
    "            if \"Exif\" not in exif_dict:\n",
    "                exif_dict[\"Exif\"] = {}\n",
    "            exif_dict[\"Exif\"][piexif.ExifIFD.UserComment] = UserComment.dump(\n",
    "                alt_text, encoding=\"unicode\"\n",
    "            )\n",
    "\n",
    "            exif_bytes = piexif.dump(exif_dict)\n",
    "            img.save(image_path, \"jpeg\", exif=exif_bytes)\n",
    "        else:\n",
    "            rprint(\n",
    "                f\"Cound not fine {image_path} for ALT text modification, please check your paths.\"\n",
    "            )\n",
    "\n",
    "    except (FileExistsError, FileNotFoundError, Exception) as e:\n",
    "        process_exception(e)\n",
    "\n",
    "\n",
    "# Appears to solve a problem associated with GPU use on Colab, see: https://github.com/explosion/spaCy/issues/11909\n",
    "def getpreferredencoding(do_setlocale=True):\n",
    "    return \"UTF-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libary Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_library_configuration() -> None:\n",
    "\n",
    "    ############################################\n",
    "    # - JUPYTER NOTEBOOK OUTPUT CONTROL / FORMATTING\n",
    "    ############################################\n",
    "    # pandas set floating point to 4 places to things don't run loose\n",
    "    debug.msg_info(\"Setting Pandas and Numpy library options.\")\n",
    "    pd.set_option(\n",
    "        \"display.max_colwidth\", 10\n",
    "    )  # None if you want to view the full json blob in the printed dataframe, use this\n",
    "    pd.options.display.float_format = \"{:,.4f}\".format\n",
    "    np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Exception Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this function displays the stack trace on errors from a central location making adjustments to the display on an error easier to manage\n",
    "# functions perform useful solutions for highly repetitive code\n",
    "def process_exception(inc_exception: Exception) -> None:\n",
    "    if DEBUG_STACKTRACE == 1:\n",
    "        traceback.print_exc()\n",
    "        console.print_exception(show_locals=True)\n",
    "    else:\n",
    "        rprint(repr(inc_exception))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Stats for a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quick_df_stats(\n",
    "    inc_df: pd.DataFrame,\n",
    "    inc_header_count: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load the data and return as a pd.DataFrame.\n",
    "\n",
    "            Parameters:\n",
    "                   inc_df (pd.DataFrame): Dataframe to be inspected, displayed\n",
    "                   inc_header_count (int): Anticipated number of columns to read in (validation check)\n",
    "\n",
    "            Returns:\n",
    "                    Printed output\n",
    "    \"\"\"\n",
    "    print(\"Data Resolution has: \" + str(inc_df.columns))\n",
    "    print(\"\\n\")\n",
    "    print(f\"\"\"{\"size\":20} : {inc_df.size:15,} \"\"\")\n",
    "    print(f\"\"\"{\"shape\":20} : {str(inc_df.shape):15} \"\"\")\n",
    "    print(f\"\"\"{\"ndim\":20} : {inc_df.ndim:15,} \"\"\")\n",
    "    print(f\"\"\"{\"column size\":20} : {inc_df.columns.size:15,} \"\"\")\n",
    "\n",
    "    # index added so you get an extra column\n",
    "    print(f\"\"\"{\"Read\":20} : {inc_df.columns.size:15,} \"\"\")\n",
    "    print(f\"\"\"{\"Expected\":20} : {inc_header_count:15,} \"\"\")\n",
    "    if (inc_df.columns.size) == inc_header_count:\n",
    "        print(f\"{BOLD_START}Expectations met{BOLD_END}.\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Expectations {BOLD_START}not met{BOLD_END}, check your datafile, columns don't match.\"\n",
    "        )\n",
    "    rprint(\"\\n\")\n",
    "    # rprint(str(inc_df.describe()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the FIADB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66496,
     "status": "ok",
     "timestamp": 1737665520749,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "AZcbq467sgrc",
    "outputId": "0a82f71c-4bcc-45bb-8681-8cce7803c49a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://research.fs.usda.gov/programs/fia#data-and-tools\n",
    "# Forest Inventory Asset Database (FIADB)\n",
    "\n",
    "\n",
    "def download_fiadb() -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    dataset_long_names = [\n",
    "        \"ALASKA_AK\",\n",
    "        \"CALIFORNIA_CA\",\n",
    "        \"HAWAII_HI\",\n",
    "        \"IDAHO_ID\",\n",
    "        \"NEVADA_NV\",\n",
    "        \"OREGON_OR\",\n",
    "        \"WASHINGTON_WA\",\n",
    "        \"ARIZONA_AZ\",\n",
    "        \"ARKANSAS_AR\",\n",
    "        \"COLORADO_CO\",\n",
    "        \"IOWA_IA\",\n",
    "        \"KANSAS_KS    \",\n",
    "        \"LOUISIANA_LA\",\n",
    "        \"MINNESOTA_MN\",\n",
    "        \"MISSOURI_MO\",\n",
    "        \"MONTANA_MT\",\n",
    "        \"NEBRASKA_NE\",\n",
    "        \"NEW_MEXICO_NM\",\n",
    "        \"NORTH_DAKOTA_ND\",\n",
    "        \"OKLAHOMA_OK\",\n",
    "        \"SOUTH_DAKOTA_SD\",\n",
    "        \"TEXAS_TX\",\n",
    "        \"U    TAH_UT\",\n",
    "        \"WYOMING_WY\",\n",
    "        \"ALABAMA_AL\",\n",
    "        \"CONNECTICUT_CT\",\n",
    "        \"DELAWARE_DE\",\n",
    "        \"FLORIDA_FL\",\n",
    "        \"GEORGIA_GA\",\n",
    "        \"ILLINOIS_IL\",\n",
    "        \"INDIANA_IN\",\n",
    "        \"KENTUCKY_KY\",\n",
    "        \"MAINE_ME\",\n",
    "        \"MARYLAND    _MD\",\n",
    "        \"MASSACHUSETTS_MA\",\n",
    "        \"MICHIGAN_MI\",\n",
    "        \"MISSISSIPPI_MS\",\n",
    "        \"NEW_HAMPSHIRE_NH\",\n",
    "        \"NEW_JERSEY_NJ\",\n",
    "        \"NEW_YORK_NY\",\n",
    "        \"NORTH_CAROLINA_NC\",\n",
    "        \"OHIO_OH\",\n",
    "        \"PENNSYLVANIA_PA\",\n",
    "        \"    RHODE_ISLAND_RI\",\n",
    "        \"SOUTH_CAROLINA_SC\",\n",
    "        \"TENNESSEE_TN\",\n",
    "        \"VERMONT_VT\",\n",
    "        \"VIRGINIA_VA\",\n",
    "        \"WEST_VIRGINIA_WV\",\n",
    "        \"WISCONSIN_WI\",\n",
    "        \"GUAM_GU\",\n",
    "        \"FEDERATED_STATES_OF_MICRONES_FM    \",\n",
    "        \"NORTHERN_MARIANA_ISLANDS_MP\",\n",
    "        \"PALAU_PW\",\n",
    "        \"AMERICAN_SAMOA_AS\",\n",
    "        \"PUERTO_RICO_PR\",\n",
    "        \"US_VIRGIN_ISLANDS_VI\",\n",
    "    ]\n",
    "    dataset_short_names = [\n",
    "        \"AK\",\n",
    "        \"AL\",\n",
    "        \"AR\",\n",
    "        \"AS\",\n",
    "        \"AZ\",\n",
    "        \"CA\",\n",
    "        \"CO\",\n",
    "        \"CT\",\n",
    "        \"DE\",\n",
    "        \"FL\",\n",
    "        \"GA\",\n",
    "        \"GU\",\n",
    "        \"HI\",\n",
    "        \"IA\",\n",
    "        \"ID\",\n",
    "        \"IL\",\n",
    "        \"IN\",\n",
    "        \"KS\",\n",
    "        \"KY\",\n",
    "        \"LA\",\n",
    "        \"MA\",\n",
    "        \"MD\",\n",
    "        \"ME\",\n",
    "        \"MI\",\n",
    "        \"MN\",\n",
    "        \"MO\",\n",
    "        \"MP\",\n",
    "        \"MS\",\n",
    "        \"MT\",\n",
    "        \"NC\",\n",
    "        \"ND\",\n",
    "        \"NE\",\n",
    "        \"NH\",\n",
    "        \"NJ\",\n",
    "        \"NM\",\n",
    "        \"NV\",\n",
    "        \"NY\",\n",
    "        \"OH\",\n",
    "        \"OK\",\n",
    "        \"OR\",\n",
    "        \"PA\",\n",
    "        \"PR\",\n",
    "        \"PW\",\n",
    "        \"RI\",\n",
    "        \"SC\",\n",
    "        \"SD\",\n",
    "        \"SFM\",\n",
    "        \"TN\",\n",
    "        \"TX\",\n",
    "        \"UT\",\n",
    "        \"VA\",\n",
    "        \"VI\",\n",
    "        \"VT\",\n",
    "        \"WA\",\n",
    "        \"WI\",\n",
    "        \"WV\",\n",
    "        \"WY\",\n",
    "    ]\n",
    "    # dataset_pattern=\"https://apps.fs.usda.gov/fia/datamart/CSV/MT_VEG_SUBPLOT.zip\"\n",
    "    dataset_pattern = \"https://apps.fs.usda.gov/fia/datamart/CSV/\"\n",
    "\n",
    "    rprint(\"Performing `wget` on target FIA records.\")\n",
    "    target_folder = WORKING_FOLDER\n",
    "    if os.path.isdir(target_folder):\n",
    "        target_directory = f\"{target_folder}{os.sep}downloads\"\n",
    "        for idx, filename in enumerate(dataset_short_names):\n",
    "            if os.path.isdir(target_directory):\n",
    "                target_filename = f\"{filename}_CSV.zip\"\n",
    "                target_url = f\"{dataset_pattern}{target_filename}\"\n",
    "                try:\n",
    "                    rprint(\n",
    "                        f\"...copying {dataset_long_names[idx]} to target folder: {target_directory}\"\n",
    "                    )\n",
    "                    subprocess.run(\n",
    "                        [\n",
    "                            \"/usr/bin/wget\",\n",
    "                            \"--show-progress\",\n",
    "                            f\"--directory-prefix={target_directory}\",\n",
    "                            f\"{target_url}\",\n",
    "                        ],\n",
    "                        check=True,\n",
    "                    )\n",
    "                    rprint(\"......completed\")\n",
    "                except (subprocess.CalledProcessError, Exception) as e:\n",
    "                    rprocess_exception(e)\n",
    "            else:\n",
    "                rprint(\n",
    "                    f\"...target folder: {target_directory} isn't present for {filename} download.\"\n",
    "                )\n",
    "            break\n",
    "    else:\n",
    "        rprint(\n",
    "            \"ERROR: Local downloads folder not found/created.  Check the output to ensure your folder is created.\"\n",
    "        )\n",
    "        rprint(f\"...target folder: {target_directory}\")\n",
    "        rprint(\"...if you can't find the problem contact the instructor.\")\n",
    "\n",
    "    # Process the downloaded data, open it up\n",
    "    rprint(\"Uncompressing the downloads...\")\n",
    "    if os.path.isdir(target_folder):\n",
    "        source_directory = f\"{target_folder}{os.sep}downloads\"\n",
    "        target_directory = f\"{target_folder}{os.sep}data\"\n",
    "        if os.path.isdir(target_directory) and os.path.isdir(source_directory):\n",
    "            for idx, filename in enumerate(dataset_short_names):\n",
    "                target_filename = f\"{filename}_CSV.zip\"\n",
    "                final_directory = f\"{target_directory}{os.sep}{filename}{os.sep}\"\n",
    "                try:\n",
    "                    if os.path.isfile(f\"{source_directory}{os.sep}{target_filename}\"):\n",
    "                        rprint(\n",
    "                            f\"...unzipping {dataset_long_names[idx]} to created target folder: {final_directory}\"\n",
    "                        )\n",
    "                        subprocess.run([\"mkdir\", \"-p\", final_directory], check=True)\n",
    "                        subprocess.run(\n",
    "                            [\n",
    "                                \"/usr/bin/unzip\",\n",
    "                                \"-o\",\n",
    "                                \"-qq\",\n",
    "                                \"-d\",\n",
    "                                f\"{final_directory}\",\n",
    "                                f\"{source_directory}{os.sep}{target_filename}\",\n",
    "                            ],\n",
    "                            check=True,\n",
    "                        )\n",
    "                        process1 = subprocess.Popen(\n",
    "                            [\n",
    "                                \"/usr/bin/find\",\n",
    "                                f\"{final_directory}\",\n",
    "                                \"-type\",\n",
    "                                \"f\",\n",
    "                                \"-print\",\n",
    "                            ],\n",
    "                            stdout=subprocess.PIPE,\n",
    "                        )\n",
    "                        process2 = subprocess.Popen(\n",
    "                            [\"wc\", \"-l\"], stdin=process1.stdout, stdout=subprocess.PIPE\n",
    "                        )\n",
    "\n",
    "                        # Close the output of process1 to allow process2 to receive EOF\n",
    "                        process1.stdout.close()\n",
    "                        output, error = process2.communicate()\n",
    "                        process2.stdout.close()\n",
    "                        number_files = output.decode().strip()\n",
    "                        rprint(f\"......completed, {number_files} files extracted.\")\n",
    "                    else:\n",
    "                        rprint(\n",
    "                            f\"......failed, unable to find ({source_directory}{os.sep}{target_filename}{os.sep})\"\n",
    "                        )\n",
    "                except (subprocess.CalledProcessError, Exception) as e:\n",
    "                    process_exception(e)\n",
    "                break\n",
    "        else:\n",
    "            rprint(\n",
    "                f\"...either the source directory ({source_directory})  or the ({target_directory}) isn't present for extraction.\"\n",
    "            )\n",
    "    else:\n",
    "        rprint(\n",
    "            \"ERROR: Local downloads folder not found/created.  Check the output to ensure your folder is created.\"\n",
    "        )\n",
    "        rprint(f\"...target folder: {target_directory}\")\n",
    "        rprint(\"...if you can't find the problem contact the instructor.\")\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDLoEWmVR9Sk"
   },
   "source": [
    "#### Download NOAA GDS 0.25 Degree Data for a Range of Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1737665520751,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "IZrJ8FhpR78m",
    "outputId": "111a3aec-6ad8-4889-c3ed-7f8e9ed4e737",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://polar.ncep.noaa.gov/global/data_access.shtml\n",
    "# Global Forecast System (GFS), 0.25 degree resolution\n",
    "def download_noaa() -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    dataset_url = \"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gfs.20250304/00/atmos/gfs.t00z.atmf000.nc\"\n",
    "    dataset_url_pattern_begin = (\n",
    "        f\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gfs.\"\n",
    "    )\n",
    "    dataset_filename_pattern = \"gfs.t00z.atmf000.nc\"\n",
    "    dataset_url_pattern_end = f\"/00/atmos/{dataset_filename_pattern}\"\n",
    "    dataset_date_month = \"03\"\n",
    "    dataset_day_start = int(4)\n",
    "    dataset_day_end = int(12)\n",
    "\n",
    "    rprint(\"...Performing `wget` on target GFS records.\")\n",
    "    target_folder = WORKING_FOLDER\n",
    "    if os.path.isdir(target_folder):\n",
    "        target_directory = f\"{target_folder}{os.sep}data\"\n",
    "        if os.path.isdir(target_directory):\n",
    "            for idx, day in enumerate(range(dataset_day_start, dataset_day_end)):\n",
    "                if day < 10:\n",
    "                    day = f\"0{day}\"\n",
    "                target_date = f\"2025{dataset_date_month}{day}\"\n",
    "                target_url = \"\".join(\n",
    "                    [dataset_url_pattern_begin, target_date, dataset_url_pattern_end]\n",
    "                )\n",
    "\n",
    "                # remove potentially partial downloads\n",
    "                target_partial_file = (\n",
    "                    f\"{target_folder}{os.sep}{dataset_filename_pattern}\"\n",
    "                )\n",
    "                if os.path.isfile(target_partial_file):\n",
    "                    rprint(\n",
    "                        f\"...removing {dataset_filename_pattern} as it is likely a partial download.\"\n",
    "                    )\n",
    "                    subprocess.run(\n",
    "                        [\"/usr/bin/rm\", \"-rf\", f\"{target_partial_file}\"], check=True\n",
    "                    )\n",
    "                try:\n",
    "                    rprint(\n",
    "                        f\"......copying {target_url} to target folder: {target_directory}\"\n",
    "                    )\n",
    "                    # subprocess.run([\"/usr/bin/wget\", \"--show-progress\", f\"--directory-prefix={target_directory}\", f\"{target_url}\"], check=True)\n",
    "                    # subprocess.run([\"/usr/bin/wget\", \"--quiet\", f\"--directory-prefix={target_directory}\", f\"{target_url}\"], check=True)\n",
    "                    rprint(f\"......completed download of day:{target_date}\")\n",
    "                    target_filename = \"_\".join([target_date, dataset_filename_pattern])\n",
    "                    os.rename(dataset_filename_pattern, target_filename)\n",
    "                    rprint(f\".........renamed file to {target_filename}\")\n",
    "                    if os.path.isfile(target_filename):\n",
    "                        rprint(f\".........SUCCESS.\")\n",
    "                    else:\n",
    "                        rprint(f\".........inspect download, there could be a problem.\")\n",
    "                except (subprocess.CalledProcessError, Exception) as e:\n",
    "                    process_exception(e)\n",
    "\n",
    "        else:\n",
    "            rprint(\n",
    "                f\"ERROR: Target folder, {target_directory}, isn't present for {target_date} download.\"\n",
    "            )\n",
    "            raise SystemError\n",
    "    else:\n",
    "        rprint(\n",
    "            \"ERROR: Local downloads folder not found/created.  Check the output to ensure your folder is created.\"\n",
    "        )\n",
    "        rprint(f\"...target folder: {target_directory}\")\n",
    "        raise SystemError\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Single Specific NetCDF (MS Bight in Gulf of America) from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_test() -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    THE_FILE = \"ACS.txt\"\n",
    "    THE_ID = \"12L8VRY6J1Sj-B1vIf-ODh4kjHWHqIzm8\"\n",
    "\n",
    "    THE_FILE = \"MissBight_2020010900.nc\"\n",
    "    THE_ID = \"1uYMFrdeVD7_qvG2wRbyu6ir9C6b4wAZC\"\n",
    "\n",
    "    target_folder = f\"{WORKING_FOLDER}{os.sep}data\"\n",
    "\n",
    "    target_ids = [THE_ID]\n",
    "    target_filenames = [THE_FILE]\n",
    "\n",
    "    for idx, the_id in enumerate(target_ids):\n",
    "        try:\n",
    "            if os.path.isfile(f\"{target_folder}{os.sep}{target_filenames[idx]}\"):\n",
    "                rprint(f\"...no need to download {target_filenames[idx]} again.\")\n",
    "            else:\n",
    "                rprint(f\"...downloading {target_filenames[idx]}.\")\n",
    "                subprocess.run(\n",
    "                    [\n",
    "                        \"gdown\",\n",
    "                        f\"{the_id}\",\n",
    "                        \"--no-check-certificate\",\n",
    "                        \"--continue\",\n",
    "                        \"-O\",\n",
    "                        f\"{target_folder}{os.sep}{target_filenames[idx]}\",\n",
    "                    ],\n",
    "                    check=True,\n",
    "                )\n",
    "        except (subprocess.CalledProcessError, Exception) as e:\n",
    "            process_exception(e)\n",
    "            raise SystemError\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTrnAspo3CWR"
   },
   "source": [
    "#### Check your resources from a CPU/GPU perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1737665454280,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "hmGUvC7M3B0H",
    "outputId": "638ecead-d943-42f2-a2eb-99790d7fe3bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hardware_stats() -> None:\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    print(\n",
    "        f\"{BOLD_START}List Devices{BOLD_END} #########################################\"\n",
    "    )\n",
    "    try:\n",
    "        from tensorflow.python.client import device_lib\n",
    "\n",
    "        rprint(device_lib.list_local_devices())\n",
    "        print(\"\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    print(\n",
    "        f\"{BOLD_START}Devices Counts{BOLD_END} ########################################\"\n",
    "    )\n",
    "    try:\n",
    "        rprint(\n",
    "            f\"Num GPUs Available: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\"\n",
    "        )\n",
    "        rprint(\n",
    "            f\"Num CPUs Available: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\"\n",
    "        )\n",
    "        print(\"\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    print(\n",
    "        f\"{BOLD_START}Optional Enablement{BOLD_END} ####################################\"\n",
    "    )\n",
    "    try:\n",
    "        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    if gpus:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        try:\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "            rprint(\n",
    "                str(\n",
    "                    str(len(gpus))\n",
    "                    + \" Physical GPUs,\"\n",
    "                    + str(len(logical_gpus))\n",
    "                    + \" Logical GPU\"\n",
    "                )\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            # Visible devices must be set before GPUs have been initialized\n",
    "            rprint(str(repr(e)))\n",
    "        print(\"\")\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_house() -> None:\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    gc.collect()\n",
    "\n",
    "    # could leave the GPU unstable so holding off.\n",
    "    # torch.cuda.empty_cache()\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuke_file(target_filename: str) -> None:\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    if os.path.isfile(target_filename):\n",
    "        try:\n",
    "            # removing existing file, else you would append\n",
    "            subprocess.run([\"rm\", \"-rf\", f\"{target_filename}\"], check=True)\n",
    "        except (subprocess.CalledProcessError, Exception) as e:\n",
    "            process_exception(e)\n",
    "            raise SystemError\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46JMSTY2QjWD"
   },
   "source": [
    "## Input Sources\n",
    "### Create the storage locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1737665454282,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "pu8f4w5XK6i8",
    "outputId": "2aa1c04c-7c5a-4348-93a1-67ea14f79c83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the folder that will hold our content.\n",
    "def create_storage_locations(inc_directory: str) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    target_folder = inc_directory\n",
    "    sub_folders = [\"downloads\", \"data\"]\n",
    "    rprint(f\"Creating project infrastructure:\")\n",
    "    try:\n",
    "        for idx, subdir in enumerate(sub_folders):\n",
    "            target_directory = f\"{target_folder}{os.sep}{subdir}\"\n",
    "            rprint(f\"...creating ({target_directory}) to store project data.\")\n",
    "            if os.path.isfile(target_directory):\n",
    "                raise OSError(\n",
    "                    f\"Cannot create your folder ({target_directory}) a file of the same name already exists there, work with your instructor or remove it yourself.\"\n",
    "                )\n",
    "            elif os.path.isdir(target_directory):\n",
    "                print(\n",
    "                    f\"......folder named ({target_directory}) {BOLD_START}already exists{BOLD_END}, we won't try to create a new folder.\"\n",
    "                )\n",
    "            else:\n",
    "                subprocess.run([\"mkdir\", \"-p\", target_directory], check=True)\n",
    "    except (subprocess.CalledProcessError, Exception) as e:\n",
    "        process_exception(e)\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read NetCDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_netcdfs(inc_source_filenames: []) -> []:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    the_list = []\n",
    "\n",
    "    rprint(f\"...reading NetCDF4 from list of {len(inc_source_filenames)} files:\")\n",
    "    for target_filename in inc_source_filenames:\n",
    "        try:\n",
    "            rprint(f\"......reading NetCDF4 ({target_filename})\")\n",
    "            the_netcdf = Dataset(target_filename, \"r\", format=\"NETCDF4\")\n",
    "            the_list.append(the_netcdf)\n",
    "        except Exception as e:\n",
    "            process_exception(e)\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    return the_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_basic_map(data) -> None:\n",
    "\n",
    "    # plt.figure()\n",
    "    lat = the_netcdf.variables[\"latitude\"][:]\n",
    "    lon = the_netcdf.variables[\"longitude\"][:]\n",
    "    data = the_netcdf.variables[\"water_temp\"][0, 0, :, :]\n",
    "\n",
    "    m = Basemap(\n",
    "        projection=\"mill\",\n",
    "        lat_ts=10,\n",
    "        llcrnrlon=lon.min(),\n",
    "        urcrnrlon=lon.max(),\n",
    "        llcrnrlat=lat.min(),\n",
    "        urcrnrlat=lat.max(),\n",
    "        resolution=\"c\",\n",
    "    )\n",
    "\n",
    "    Lon, Lat = meshgrid(lon, lat)\n",
    "    x, y = m(Lon, Lat)\n",
    "\n",
    "    # cs = m.pcolormesh(x,y,data,shading='flat', cmap=plt.cm.jet)\n",
    "    cs = m.pcolormesh(x, y, data, cmap=plt.cm.jet)\n",
    "\n",
    "    m.drawcoastlines()\n",
    "    m.fillcontinents()\n",
    "    m.drawmapboundary()\n",
    "    m.drawparallels(np.arange(-90.0, 120.0, 30.0), labels=[1, 0, 0, 0])\n",
    "    m.drawmeridians(np.arange(-180.0, 180.0, 60.0), labels=[0, 0, 0, 1])\n",
    "\n",
    "    colorbar(cs)\n",
    "    plt.title(\"Example 1: Global RTOFS SST from NOMADS\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate Various Data Storage Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pandas(inc_payload: {}) -> pd.DataFrame():\n",
    "\n",
    "    if DEBUG_USING_GPU == 1:\n",
    "        import cudf.pandas\n",
    "\n",
    "        cudf.pandas.install()\n",
    "        import pandas as pd\n",
    "    else:\n",
    "        import pandas as pd\n",
    "\n",
    "    latSeries = pd.Series(inc_payload[LAT_LNAME].flatten())\n",
    "    lonSeries = pd.Series(inc_payload[LONG_LNAME].flatten())\n",
    "    varSeries = pd.Series(inc_payload[PRODUCT_LNAME][0, 0, :, :].flatten())\n",
    "\n",
    "    # define a Panda.DataFrame()\n",
    "    frame = {\n",
    "        LAT_LNAME: latSeries,\n",
    "        LONG_LNAME: lonSeries,\n",
    "        PRODUCT_LNAME: varSeries,\n",
    "    }\n",
    "\n",
    "    # instantiate a dataframe\n",
    "    df = pd.DataFrame(frame)\n",
    "\n",
    "    # ensure the data is cast as expected\n",
    "    df[LAT_LNAME].astype(\"float64\")\n",
    "    df[LONG_LNAME].astype(\"float64\")\n",
    "    df[PRODUCT_LNAME].astype(\"float64\")\n",
    "\n",
    "    # clean up behind yourself\n",
    "    del latSeries, lonSeries, varSeries, frame\n",
    "    clean_house()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pandas(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_PANDAS_EXT}\"\n",
    "\n",
    "    # create dataframe\n",
    "    df = build_pandas(inc_payload)\n",
    "    # quick stats\n",
    "    quick_df_stats(df, 3)\n",
    "\n",
    "    # Get memory usage of each column in bytes\n",
    "    memory_usage_per_column = df.memory_usage(deep=True)\n",
    "\n",
    "    # Get total memory usage of the DataFrame in bytes\n",
    "    total_memory_usage = df.memory_usage().sum()\n",
    "    print(f\"Original Dataframe memory use: {total_memory_usage:20,}\")\n",
    "\n",
    "    # WRITE\n",
    "    nuke_file(target_filename)\n",
    "    start_time = time.perf_counter()\n",
    "    write_pandas(target_filename, df)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"...Pandas Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    # READ\n",
    "    start_time = time.perf_counter()\n",
    "    read_pandas(target_filename)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"...Pandas Read Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pandas(target_pandas_filename: str, df: pd.DataFrame) -> None:\n",
    "    df.to_pickle(target_pandas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pandas(target_pandas_filename: str) -> None:\n",
    "    df = pd.read_pickle(target_pandas_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_numpy(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = (\n",
    "        f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_NUMPY_EXT}\"\n",
    "    )\n",
    "\n",
    "    # WRITE\n",
    "    nuke_file(target_filename)\n",
    "    start_time = time.perf_counter()\n",
    "    write_numpy_native(target_filename, inc_payload)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"...Numpy Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    df = build_pandas(inc_payload)\n",
    "    nuke_file(target_filename)\n",
    "    start_time = time.perf_counter()\n",
    "    write_numpy_as_df(target_filename, df)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"...Numpy pd.DataFrame Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    # READ\n",
    "    start_time = time.perf_counter()\n",
    "    read_numpy(target_filename)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"...Numpy Read Execution time: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_numpy_as_df(target_numpy_filename: str, df: pd.DataFrame) -> None:\n",
    "    df_numpy = df.to_numpy()\n",
    "    np.savez(target_numpy_filename, df_numpy)\n",
    "\n",
    "\n",
    "def write_numpy_native(target_numpy_filename: str, inc_payload: {}) -> None:\n",
    "    # varying sized arrays\n",
    "    np.savez(\n",
    "        target_numpy_filename,\n",
    "        LAT_LNAME=inc_payload[LAT_LNAME],\n",
    "        LONG_LNAME=inc_payload[LONG_LNAME],\n",
    "        PRODUCT_LNAME=inc_payload[PRODUCT_LNAME],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_numpy(target_numpy_filename: str) -> None:\n",
    "    loaded_arr = np.load(target_numpy_filename + \".npz\")\n",
    "\n",
    "    # to unpack\n",
    "    # new_lat = loaded_arr[\"lat\"]\n",
    "    # new_lon = loaded_arr[\"lon\"]\n",
    "    # new_product = loaded_arr[\"product\"]\n",
    "    # del loaded_arr, new_lat, new_lon, new_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pytorch(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = (\n",
    "        f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_TORCH_EXT}\"\n",
    "    )\n",
    "\n",
    "    nuke_file(target_filename)\n",
    "    start_time = time.perf_counter()\n",
    "    write_pytorch(target_filename, inc_payload)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"...PyTorch Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    read_pytorch(target_filename)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"...PyTorch Read Execution time: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pytorch(target_pytorch_filename: str, inc_payload: {}):\n",
    "    if DEBUG_USING_GPU == 1:\n",
    "        dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "\n",
    "    # input_tensor = input_tensor.to(device)\n",
    "    lat_tensor = torch.tensor(inc_payload[LAT_LNAME].flatten()).to(dev)\n",
    "    lon_tensor = torch.tensor(inc_payload[LONG_LNAME].flatten()).to(dev)\n",
    "    var_tensor = torch.tensor(inc_payload[PRODUCT_LNAME].flatten()).to(dev)\n",
    "\n",
    "    # lonSeries=pd.Series(lon.flatten())\n",
    "    # varSeries=pd.Series(varAry[0,0,:,:].flatten())\n",
    "\n",
    "    # Save the tensor\n",
    "    # Save multiple tensors as a list\n",
    "    tensors_list = [lat_tensor, lon_tensor, var_tensor]\n",
    "    torch.save(tensors_list, target_pytorch_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pytorch(target_pytorch_filename: str) -> None:\n",
    "    # Load the tensor from the file\n",
    "    tensor_loaded = torch.load(target_pytorch_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tensorflow(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    if DEBUG_USING_GPU == 1:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "        os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "        os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "        # gpus = tf.config.list_physical_devices('GPU')\n",
    "        # tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_TENSORFLOW_EXT}\"\n",
    "\n",
    "    nuke_file(target_filename)\n",
    "    start_time = time.perf_counter()\n",
    "    write_tensorflow(target_filename, inc_payload)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"...TensorFlow Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    read_tensorflow(target_filename)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"...TensorFlow Read Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tensorflow(target_tensorflow_filename: str, inc_payload: {}):\n",
    "\n",
    "    with tf.device(THE_DEVICE_NAME):\n",
    "        # create a TFRecord to store the data\n",
    "        lat_list = tf.train.FloatList(value=inc_payload[LAT_LNAME].flatten().tolist())\n",
    "        lon_list = tf.train.FloatList(value=inc_payload[LONG_LNAME].flatten().tolist())\n",
    "        varAry_list = tf.train.FloatList(\n",
    "            value=inc_payload[PRODUCT_LNAME].flatten().tolist()\n",
    "        )\n",
    "        feature = {\n",
    "            LAT_LNAME: tf.train.Feature(float_list=lat_list),\n",
    "            LONG_LNAME: tf.train.Feature(float_list=lon_list),\n",
    "            PRODUCT_LNAME: tf.train.Feature(float_list=varAry_list),\n",
    "        }\n",
    "        tfRecord = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature)\n",
    "        ).SerializeToString()\n",
    "        # dataset = tf.data.Dataset.from_tensor_slices((lat_tensor, lon_tensor, var_tensor))\n",
    "        # print(f\"Size of TFRecord is {sys.getsizeof(tfRecord):20,} bytes.\")\n",
    "        # tf.io.write_file(target_tensorflow_filename, tf.io.serialize_tensor(tensors_list))\n",
    "        with tf.io.TFRecordWriter(target_tensorflow_filename) as writer:\n",
    "            writer.write(tfRecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example_proto):\n",
    "    feature_description = {\n",
    "        \"latitude\": tf.io.FixedLenSequenceFeature(\n",
    "            [], dtype=tf.float32, allow_missing=True\n",
    "        ),\n",
    "        \"longitude\": tf.io.FixedLenSequenceFeature(\n",
    "            [], dtype=tf.float32, allow_missing=True\n",
    "        ),\n",
    "        \"product\": tf.io.FixedLenSequenceFeature(\n",
    "            [], dtype=tf.float32, allow_missing=True\n",
    "        ),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tensorflow(target_tensorflow_filename: str) -> None:\n",
    "\n",
    "    with tf.device(THE_DEVICE_NAME):\n",
    "        dataset = tf.data.TFRecordDataset(target_tensorflow_filename)\n",
    "        tfRecord = dataset.map(parse_tfrecord_fn)\n",
    "        # for record in tfRecord:\n",
    "        #    lat = record[\"latitude\"]\n",
    "        #    lon = record[\"longitude\"]\n",
    "        #    varAry = record[\"product\"]\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_xarray(the_netcdf) -> None:\n",
    "def process_xarray(inc_netcdf_filename: str) -> None:\n",
    "\n",
    "    # xarray on GPU: https://github.com/xarray-contrib/cupy-xarray\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    \"\"\"\n",
    "    the_netcdf = Dataset(inc_netcdf_filename, \"r\", format=\"NETCDF4\")\n",
    "\n",
    "    geospatial_lat_nm=LAT_SNAME\n",
    "    geospatial_lon_nm=LONG_SNAME\n",
    "    product_nm=PRODUCT_LNAME\n",
    "    lat   =np.array(the_netcdf.variables[geospatial_lat_nm][:][:],dtype=np.double)\n",
    "    lon   =np.array(the_netcdf.variables[geospatial_lon_nm][:][:],dtype=np.double)\n",
    "    varAry=np.array(the_netcdf.variables[product_nm][0][0][:][:],dtype=np.double)\n",
    "    lat   =lat.flatten()\n",
    "    lon   =lon.flatten()\n",
    "    varAry=varAry[0, 0,]\n",
    "    #error in cupy for saving to netcdf, defaulting to CPU only\n",
    "    if DEBUG_USING_GPU==1:\n",
    "        lat   =cupy.array(inc_payload[LAT_LNAME].flatten(),dtype=cupy.double)\n",
    "        lon   =cupy.array(inc_payload[LONG_LNAME].flatten(),dtype=cupy.double)\n",
    "        varAry=cupy.array(inc_payload[PRODUCT_LNAME][0, 0,],dtype=cupy.double)\n",
    "    else:\n",
    "        lat   =np.array(inc_payload[LAT_LNAME].flatten(),dtype=np.double)\n",
    "        lon   =np.array(inc_payload[LONG_LNAME].flatten(),dtype=np.double)\n",
    "        varAry=np.array(inc_payload[PRODUCT_LNAME][0, 0,],dtype=np.double)\n",
    "\n",
    "    #build the xarray dataset from scratch\n",
    "    #Reference: https://docs.xarray.dev/en/stable/generated/xarray.Dataset.html\n",
    "    ds_xr = xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            data=([LAT_LNAME, LONG_LNAME], varAry),\n",
    "            data=([\"geospatial_coordinates\"], varAry),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            LAT_LNAME=(LAT_LNAME,lat),\n",
    "            LONG_LNAME=(LONG_LNAME,lon),\n",
    "        ),\n",
    "        attrs=dict(description=\"Weather related data.\"),\n",
    "    )\n",
    "\n",
    "    #https://github.com/xarray-contrib/cupy-xarray\n",
    "    ds_xr = ds_xr.cupy.as_cupy()\n",
    "    \"\"\"\n",
    "\n",
    "    ds_xr = xr.open_dataset(inc_netcdf_filename, engine=\"netcdf4\")\n",
    "\n",
    "    # WRITE - NetCDF\n",
    "    try:\n",
    "        target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_XARRAY_EXT}\"\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_xarray_netcdf(target_filename, ds_xr)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Xarray Write (NetCDF) Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        read_xarray_netcdf(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Xarray Read (NetCDF) Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    # WRITE - HDF5\n",
    "    try:\n",
    "        target_filename = (\n",
    "            f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.h5\"\n",
    "        )\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_xarray_hdf5(target_filename, ds_xr)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Xarray Write (HDF5) Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        read_xarray_hdf5(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Xarray Read (HDF5) Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    # WRITE - ZARR\n",
    "    try:\n",
    "        target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_ZARR_EXT}\"\n",
    "        print(f\"delete {target_filename}\")\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_xarray_zarr(target_filename, ds_xr)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Xarray Write (Zarr) Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        read_xarray_zarr(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Xarray Read (Zarr) Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    # WRITE - PICKLE\n",
    "    try:\n",
    "        target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}{OUTPUT_PANDAS_EXT}2\"\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_xarray_pickle(target_filename, ds_xr)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Xarray Write (Pickle) Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        read_xarray_pickle(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Xarray Read (Pickle) Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    del ds_xr\n",
    "    clean_house()\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_xarray_netcdf(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    ds_xr.to_netcdf(target_xarray_filename)\n",
    "\n",
    "\n",
    "def write_xarray_hdf5(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    ds_xr.to_netcdf(target_xarray_filename)\n",
    "\n",
    "\n",
    "def write_xarray_zarr(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    ds_xr.to_zarr(target_xarray_filename)\n",
    "\n",
    "\n",
    "def write_xarray_pickle(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    pkl = pickle.dumps(ds_xr, protocol=-1)\n",
    "    with open(target_xarray_filename, \"wb\") as file:\n",
    "        # Use pickle.dump() to serialize and write the data to the file\n",
    "        pickle.dump(pkl, target_xarray_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xarray_netcdf(target_xarray_filename: str) -> None:\n",
    "    ds_xr_loaded = xr.open_dataset(target_xarray_filename, engine=\"netcdf4\")\n",
    "\n",
    "\n",
    "def read_xarray_hdf5(target_xarray_filename: str) -> None:\n",
    "    ds_xr_loaded = xr.open_dataset(target_xarray_filename, engine=\"netcdf4\")\n",
    "\n",
    "\n",
    "def read_xarray_zarr(target_xarray_filename: str) -> None:\n",
    "    ds_xr_loaded = xr.open_zarr(target_xarray_filename)\n",
    "\n",
    "\n",
    "def read_xarray_pickle(target_xarray_filename: str) -> None:\n",
    "    try:\n",
    "        with open(target_xarray_filename, \"rb\") as file:\n",
    "            ds_xr = pickle.load(file, protocol=-1)\n",
    "    except (FileNotFoundError, Exception) as e:\n",
    "        process_exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apache Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_parquet(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    # build the xarray dataset from scratch\n",
    "    df = build_pandas(inc_payload)\n",
    "\n",
    "    # WRITE - NetCDF\n",
    "    try:\n",
    "        target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_PARQUET_EXT}\"\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_parquet(target_filename, df)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Parquet Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        # READ\n",
    "        start_time = time.perf_counter()\n",
    "        read_parquet(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Parquet Read Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    del df\n",
    "    clean_house()\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_parquet(target_parquet_filename: str, df: pd.DataFrame) -> None:\n",
    "    df.to_parquet(target_parquet_filename, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_parquet(target_parquet_filename: str) -> None:\n",
    "    df_parquet = pd.read_parquet(target_parquet_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_zarr(the_netcdf) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    # Zarr\n",
    "    # Save xarray Dataset to Zarr\n",
    "    # ds_zarr=ds_xr.to_zarr('data.zarr')\n",
    "    import zarr\n",
    "    import cupy as cp\n",
    "\n",
    "    zarr.config.enable_gpu()\n",
    "    # ds_xr = xr.open_dataset(target_filename)\n",
    "    geospatial_lat_nm = LAT_SNAME\n",
    "    geospatial_lon_nm = LONG_SNAME\n",
    "    product_nm = PRODUCT_LNAME\n",
    "    lat = np.array(the_netcdf.variables[geospatial_lat_nm][:][:], dtype=np.double)\n",
    "    lon = np.array(the_netcdf.variables[geospatial_lon_nm][:][:], dtype=np.double)\n",
    "    varAry = np.array(the_netcdf.variables[product_nm][:][:], dtype=np.double)\n",
    "    lat = lat.flatten()\n",
    "    lon = lon.flatten()\n",
    "    varAry = varAry[0, 0, :, :].flatten()\n",
    "\n",
    "    # root = zarr.open_group('data/group.zarr', mode='w')\n",
    "    # store = zarr.storage.MemoryStore()\n",
    "    # store= zarr.storage.LocalStore(target_zarr_filename, read_only=False)\n",
    "    # store = zarr.storage.ZipStore(target_zarr_filename, mode='w')\n",
    "    # root = zarr.create_group(store=store)\n",
    "\n",
    "    root = zarr.group()\n",
    "    z_lat_grp = root.create_group(LAT_LNAME)\n",
    "    z_lat_ary = z_lat_grp.create_array(\n",
    "        name=LAT_LNAME, shape=lat.shape, chunks=\"auto\", dtype=np.float32\n",
    "    )\n",
    "    z_lat_ary[:] = lat\n",
    "\n",
    "    z_lon_grp = root.create_group(LONG_LNAME)\n",
    "    z_lon_ary = z_lon_grp.create_array(\n",
    "        name=LONG_LNAME, shape=lon.shape, chunks=\"auto\", dtype=np.float32\n",
    "    )\n",
    "    z_lon_ary[:] = lon\n",
    "\n",
    "    z_product_grp = root.create_group(PRODUCT_LNAME)\n",
    "    z_product_ary = z_product_grp.create_array(\n",
    "        name=PRODUCT_LNAME, shape=varAry.shape, chunks=\"auto\", dtype=np.float32\n",
    "    )\n",
    "    z_product_ary[:] = varAry\n",
    "\n",
    "    print(f\"{BOLD_START}Zarr Node Tree:{BOLD_END}\")\n",
    "    print(root.tree())\n",
    "\n",
    "    # WRITE\n",
    "    try:\n",
    "        target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_ZARR_EXT}\"\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_zarr(target_filename, root)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Zarr Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        # READ\n",
    "        start_time = time.perf_counter()\n",
    "        read_zarr(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Zarr Read Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    # store.close()\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_zarr(target_zarr_filename: str, root: zarr.Group) -> None:\n",
    "    zarr.save(target_zarr_filename, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zarr(target_zarr_filename: str) -> None:\n",
    "    # Load the group from the directory\n",
    "    loaded_group = zarr.open(target_zarr_filename)\n",
    "\n",
    "    \"\"\"\n",
    "    # Verify the structure and data\n",
    "    assert isinstance(loaded_group, zarr.Group)\n",
    "    assert 'root' in loaded_group\n",
    "    assert 'bar' in loaded_group['foo']\n",
    "    assert 'baz' in loaded_group['foo']\n",
    "    np.testing.assert_array_equal(bar, loaded_group['foo']['bar'])\n",
    "    np.testing.assert_array_equal(baz, loaded_group['foo']['baz'])\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "\n",
    "target_dask_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}ACS\"\n",
    "\n",
    "df_dask = dd.from_pandas(df, npartitions=1)\n",
    "\n",
    "# Convert DataFrame rows to dictionaries and create a Dask bag\n",
    "df_bag = db.from_sequence(df.to_dict(orient=\"records\"))\n",
    "\n",
    "# Print the Dask bag (computation is lazy, so compute() is needed to see the result)\n",
    "print(df_bag.compute())\n",
    "\n",
    "schema = {\n",
    "    \"name\": \"People\",\n",
    "    \"doc\": \"Set of people's scores\",\n",
    "    \"type\": \"record\",\n",
    "    \"fields\": [{\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"value\", \"type\": \"int\"}],\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%timeit\n",
    "# df_bag.to_avro(f\"{target_dask_filename}.*.avro\", schema)\n",
    "df_bag.to_avro(f\"{target_dask_filename}.*.avro\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%timeit\n",
    "loaded_df_bag = db.read_avro(f\"{target_dask_filename}.*.avro\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### PySTAC"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def process_pystac(inc_payload:{}) -> None:\n",
    "\n",
    "        rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample pandas DataFrame\n",
    "        data = {\n",
    "            \"id\": [\"item1\", \"item2\"],\n",
    "            \"geometry\": [\n",
    "                {\"type\": \"Point\", \"coordinates\": [1, 1]},\n",
    "                {\"type\": \"Point\", \"coordinates\": [2, 2]},\n",
    "            ],\n",
    "            \"datetime\": [pd.Timestamp(\"2023-01-01\"), pd.Timestamp(\"2023-01-02\")],\n",
    "            \"properties\": [{\"prop1\": \"value1\"}, {\"prop1\": \"value2\"}],\n",
    "        }\n",
    "        \n",
    "        # Create a STAC Catalog\n",
    "        catalog = pystac.Catalog.from_dict(\n",
    "            {\"type\": \"Catalog\", \"id\": \"acs\", \"stac_version\": \"1.0.0\"}\n",
    "        )\n",
    "        \n",
    "        # Convert DataFrame to STAC Items and add to Catalog\n",
    "        for index, row in df.iterrows():\n",
    "            item = pystac.Item(\n",
    "                id=row[\"id\"],\n",
    "                geometry=row[\"geometry\"],\n",
    "                datetime=row[\"datetime\"].to_pydatetime(),\n",
    "                properties=row[\"properties\"],\n",
    "            )\n",
    "            catalog.add_item(item)\n",
    "        \n",
    "        # Write the catalog to a file\n",
    "        catalog.normalize_hrefs(\"./pystac_data\")\n",
    "        catalog.save_object(pystac.Catalog, \"acs.json\")\n",
    "        \n",
    "        # Read STAC catalog into a DataFrame\n",
    "        # df_from_stac = df_from(catalog)\n",
    "        # print(df_from_stac)\n",
    "        \"\"\"\n",
    "        rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_variables(inc_product_name:str, the_netcdf) -> {}:\n",
    "\n",
    "        rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "        geospatial_lat_nm=LAT_SNAME\n",
    "        geospatial_lon_nm=LONG_SNAME\n",
    "        product_nm=inc_product_name\n",
    "        local_payload={}\n",
    "        #note x,y values are shown below as they are part of the APS meta-data\n",
    "        #based on the NetCDF Best Practice subject x,y vars should not exist.\n",
    "        #keeping for continuity between BIOCAST code already written to read APS input.\n",
    "\n",
    "        if DEBUG_USING_GPU==1:\n",
    "            lat   =cupy.array(the_netcdf.variables[geospatial_lat_nm][:][:],dtype=cupy.double)\n",
    "            lon   =cupy.array(the_netcdf.variables[geospatial_lon_nm][:][:],dtype=cupy.double)\n",
    "            varAry=cupy.array(the_netcdf.variables[product_nm][:][:],dtype=cupy.double)\n",
    "        else:\n",
    "            lat   =np.array(the_netcdf.variables[geospatial_lat_nm][:][:],dtype=np.double)\n",
    "            lon   =np.array(the_netcdf.variables[geospatial_lon_nm][:][:],dtype=np.double)\n",
    "            varAry=np.array(the_netcdf.variables[product_nm][:][:],dtype=np.double)\n",
    "\n",
    "        print(f\"...{BOLD_START}{geospatial_lat_nm:10}{\" data type:\":20}{BOLD_END}{str(type(lat)):20}\")\n",
    "        print(f\".......shape:{lat.shape}\")\n",
    "        print(f\"....datatype:{lat.dtype}\")\n",
    "\n",
    "\n",
    "        print(f\"...{BOLD_START}{geospatial_lon_nm:10}{\" data type:\":20}{BOLD_END}{str(type(lon)):20}\")\n",
    "        print(f\".......shape:{lon.shape}\")\n",
    "        print(f\"....datatype:{lon.dtype}\")\n",
    "\n",
    "        print(f\"...{BOLD_START}{\"Oceanographic data type\":20}({product_nm:20}){BOLD_END}{str(type(varAry)):20}\")\n",
    "        print(f\".......shape:{varAry.shape}\")\n",
    "        print(f\"....datatype:{varAry.dtype}\")\n",
    "\n",
    "        local_payload[LAT_LNAME]=lat\n",
    "        local_payload[LONG_LNAME]=lon\n",
    "        local_payload[PRODUCT_LNAME]=varAry\n",
    "\n",
    "        rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "        return local_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main routine that executes all code, does return a data frame of data for further analysis if desired.\n",
    "#\n",
    "#  @param (None)\n",
    "def process(inc_input_directory: str) -> {}:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    # variables\n",
    "    source_nc_list = []\n",
    "    source_filenames_list = []\n",
    "\n",
    "    # setup storage solution\n",
    "    create_storage_locations(inc_input_directory)\n",
    "\n",
    "    # download the data\n",
    "    download_test()\n",
    "\n",
    "    # identify target files\n",
    "    print(\"...marshaling data files:\")\n",
    "    target_directory = f\"{inc_input_directory}{os.sep}data\"\n",
    "    if os.path.isdir(target_directory):\n",
    "        for file in os.listdir(target_directory):\n",
    "            print(f\"......processing {file} from {target_directory}\")\n",
    "            filename, file_extension = os.path.splitext(file)\n",
    "            if file_extension.lower() in LOWER_EXTENSIONS:\n",
    "                source_filenames_list.append(os.path.join(target_directory, file))\n",
    "    else:\n",
    "        print(\n",
    "            \"Target directory ({target_directory}) does not exist, cannot continue execution.  Check your paths.\"\n",
    "        )\n",
    "        raise SystemError\n",
    "\n",
    "    print(source_filenames_list)\n",
    "\n",
    "    # iterate through netCDFs and read them into an array\n",
    "    source_nc_list = read_netcdfs(source_filenames_list)\n",
    "\n",
    "    for idx, the_netcdf in enumerate(source_nc_list):\n",
    "        the_payload = gather_variables(PRODUCT_LNAME, the_netcdf)\n",
    "        # process_numpy(the_payload)\n",
    "        # process_pandas(the_payload)\n",
    "        # process_pytorch(the_payload)\n",
    "        # process_tensorflow(the_payload)\n",
    "        #process_xarray(source_filenames_list[idx])\n",
    "        #process_zarr(the_netcdf)\n",
    "        process_parquet(the_payload)\n",
    "\n",
    "        break\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Routine (call all other routines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN PROGRAM\n",
      "Author: Christopher G Wood\n",
      "\n",
      "Github username: GITHUB_USERNAME\n",
      "\n",
      "Email: christopher.g.wood@gmail.com\n",
      "\n",
      "Last updated: 2025-03-14T10:24:28.266841-05:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.12.9\n",
      "IPython version      : 8.30.0\n",
      "\n",
      "Compiler    : GCC 13.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.167.4-microsoft-standard-WSL2\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Hostname: ThulsaDoom\n",
      "\n",
      "Git hash: e51e8809604eeafbcb18e553442050e39802904d\n",
      "\n",
      "Git repo: git@github.com:christophergarthwood/jbooks.git\n",
      "\n",
      "Git branch: Updates\n",
      "\n",
      "\u001b[1mPackages:\u001b[0;0m\n",
      "\n",
      "dask                                    #: 2024.12.1           \n",
      "fastparquet                             #: 2024.11.0           \n",
      "keras                                   #: 3.9.0               \n",
      "numpy                                   #: 1.26.4              \n",
      "pandas                                  #: 2.2.3               \n",
      "polars                                  #: 1.24.0              \n",
      "pystac                                  #: 1.12.1              \n",
      "seaborn                                 #: 0.13.2              \n",
      "xarray                                  #: 2024.11.0           \n",
      "zarr                                    #: 3.0.5               \n",
      "TensorFlow version                      #: 2.18.0              \n",
      "     gpu.count:                         #: 1\n",
      "     cpu.count:                         #: 1\n",
      "Torch version                           #: 2.6.0+cu124         \n",
      "     GPUs available?                    #: True\n",
      "     count                              #: 1\n",
      "     current                            #: NVIDIA GeForce RTX 2060\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ get_hardware_stats\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ get_hardware_stats\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mList Devices\u001b[0;0m #########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1741965868.296515    3500 gpu_device.cc:2022] Created device /device:GPU:0 with 4056 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    name: <span style=\"color: #008000; text-decoration-color: #008000\">\"/device:CPU:0\"</span>\n",
       "device_type: <span style=\"color: #008000; text-decoration-color: #008000\">\"CPU\"</span>\n",
       "memory_limit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">268435456</span>\n",
       "locality <span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "incarnation: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12009975941296969911</span>\n",
       "xla_global_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>\n",
       ",\n",
       "    name: <span style=\"color: #008000; text-decoration-color: #008000\">\"/device:XLA_CPU:0\"</span>\n",
       "device_type: <span style=\"color: #008000; text-decoration-color: #008000\">\"XLA_CPU\"</span>\n",
       "memory_limit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17179869184</span>\n",
       "locality <span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "incarnation: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4311330369197458435</span>\n",
       "physical_device_desc: <span style=\"color: #008000; text-decoration-color: #008000\">\"device: XLA_CPU device\"</span>\n",
       "xla_global_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>\n",
       ",\n",
       "    name: <span style=\"color: #008000; text-decoration-color: #008000\">\"/device:XLA_GPU:0\"</span>\n",
       "device_type: <span style=\"color: #008000; text-decoration-color: #008000\">\"XLA_GPU\"</span>\n",
       "memory_limit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17179869184</span>\n",
       "locality <span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "incarnation: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9795158367762606694</span>\n",
       "physical_device_desc: <span style=\"color: #008000; text-decoration-color: #008000\">\"device: XLA_GPU device\"</span>\n",
       "xla_global_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>\n",
       ",\n",
       "    name: <span style=\"color: #008000; text-decoration-color: #008000\">\"/device:GPU:0\"</span>\n",
       "device_type: <span style=\"color: #008000; text-decoration-color: #008000\">\"GPU\"</span>\n",
       "memory_limit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4253024256</span>\n",
       "locality <span style=\"font-weight: bold\">{</span>\n",
       "  bus_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "  links <span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "incarnation: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">802725943804090429</span>\n",
       "physical_device_desc: <span style=\"color: #008000; text-decoration-color: #008000\">\"device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"</span>\n",
       "xla_global_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">416903419</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    name: \u001b[32m\"/device:CPU:0\"\u001b[0m\n",
       "device_type: \u001b[32m\"CPU\"\u001b[0m\n",
       "memory_limit: \u001b[1;36m268435456\u001b[0m\n",
       "locality \u001b[1m{\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "incarnation: \u001b[1;36m12009975941296969911\u001b[0m\n",
       "xla_global_id: \u001b[1;36m-1\u001b[0m\n",
       ",\n",
       "    name: \u001b[32m\"/device:XLA_CPU:0\"\u001b[0m\n",
       "device_type: \u001b[32m\"XLA_CPU\"\u001b[0m\n",
       "memory_limit: \u001b[1;36m17179869184\u001b[0m\n",
       "locality \u001b[1m{\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "incarnation: \u001b[1;36m4311330369197458435\u001b[0m\n",
       "physical_device_desc: \u001b[32m\"device: XLA_CPU device\"\u001b[0m\n",
       "xla_global_id: \u001b[1;36m-1\u001b[0m\n",
       ",\n",
       "    name: \u001b[32m\"/device:XLA_GPU:0\"\u001b[0m\n",
       "device_type: \u001b[32m\"XLA_GPU\"\u001b[0m\n",
       "memory_limit: \u001b[1;36m17179869184\u001b[0m\n",
       "locality \u001b[1m{\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "incarnation: \u001b[1;36m9795158367762606694\u001b[0m\n",
       "physical_device_desc: \u001b[32m\"device: XLA_GPU device\"\u001b[0m\n",
       "xla_global_id: \u001b[1;36m-1\u001b[0m\n",
       ",\n",
       "    name: \u001b[32m\"/device:GPU:0\"\u001b[0m\n",
       "device_type: \u001b[32m\"GPU\"\u001b[0m\n",
       "memory_limit: \u001b[1;36m4253024256\u001b[0m\n",
       "locality \u001b[1m{\u001b[0m\n",
       "  bus_id: \u001b[1;36m1\u001b[0m\n",
       "  links \u001b[1m{\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "incarnation: \u001b[1;36m802725943804090429\u001b[0m\n",
       "physical_device_desc: \u001b[32m\"device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\u001b[0m\n",
       "xla_global_id: \u001b[1;36m416903419\u001b[0m\n",
       "\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mDevices Counts\u001b[0;0m ########################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num GPUs Available: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num GPUs Available: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num CPUs Available: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num CPUs Available: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mOptional Enablement\u001b[0;0m ####################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> Physical GPUs,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> Logical GPU\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m Physical GPUs,\u001b[1;36m1\u001b[0m Logical GPU\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ get_hardware_stats\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ get_hardware_stats\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ create_storage_locations\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ create_storage_locations\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating project infrastructure:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating project infrastructure:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span>creating <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">downloads</span><span style=\"font-weight: bold\">)</span> to store project data.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0mcreating \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/\u001b[0m\u001b[95mdownloads\u001b[0m\u001b[1m)\u001b[0m to store project data.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......folder named (./folderOnColab/downloads) \u001b[1malready exists\u001b[0;0m, we won't try to create a new folder.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span>creating <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">data</span><span style=\"font-weight: bold\">)</span> to store project data.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0mcreating \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/\u001b[0m\u001b[95mdata\u001b[0m\u001b[1m)\u001b[0m to store project data.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......folder named (./folderOnColab/data) \u001b[1malready exists\u001b[0;0m, we won't try to create a new folder.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Exiting __main__ create_storage_locations\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Exiting __main__ create_storage_locations\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ download_test\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ download_test\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span>no need to download MissBight_2020010900.nc again.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0mno need to download MissBight_2020010900.nc again.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Exiting __main__ download_test\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Exiting __main__ download_test\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...marshaling data files:\n",
      "......processing MLDATAREADY-0-0-1.zarr from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1.parquet from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1.h5 from ./folderOnColab/data\n",
      "......processing MissBight_2020010900.nc from ./folderOnColab/data\n",
      "......processing local_test.npy.npz from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1.xr from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1pkl2 from ./folderOnColab/data\n",
      "['./folderOnColab/data/MissBight_2020010900.nc']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ read_netcdfs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ read_netcdfs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span>reading NetCDF4 from list of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> files:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0mreading NetCDF4 from list of \u001b[1;36m1\u001b[0m files:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">......</span>reading NetCDF4 <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">MissBight_2020010900.nc</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0mreading NetCDF4 \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/data/\u001b[0m\u001b[95mMissBight_2020010900.nc\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ read_netcdfs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ read_netcdfs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ gather_variables\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ gather_variables\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\u001b[1mlat        data type:         \u001b[0;0m<class 'numpy.ndarray'>\n",
      ".......shape:(400,)\n",
      "....datatype:float64\n",
      "...\u001b[1mlon        data type:         \u001b[0;0m<class 'numpy.ndarray'>\n",
      ".......shape:(800,)\n",
      "....datatype:float64\n",
      "...\u001b[1mOceanographic data type(chlor_a             )\u001b[0;0m<class 'numpy.ndarray'>\n",
      ".......shape:(1, 29, 400, 800)\n",
      "....datatype:float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ gather_variables\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ gather_variables\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_parquet\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_parquet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Id---------------------------------------------\n",
      "                                      Id: write_parquet\n",
      "                Runtime----------------------------------------\n",
      "                                 Runtime: 0.13 milliseconds\n",
      "\n",
      "                I/O Size---------------------------------------\n",
      "                               File Size: 0.00 bytes\n",
      "\n",
      "                I/O Counts-------------------------------------\n",
      "                      Targeted disk read: 0.00 counts\n",
      "                     Targeted disk write: 0.00 counts\n",
      "                       General disk read: 0.00 counts\n",
      "                      General disk write: 0.00 counts\n",
      "\n",
      "                I/O Time---------------------------------------\n",
      "                 Targeted disk read time: 0.00 milliseconds\n",
      "                Targeted disk write time: 0.00 milliseconds\n",
      "                  General disk read time: 0.00 milliseconds\n",
      "                 General disk write time: 2.00 milliseconds\n",
      "\n",
      "                Memory------------------------------------------\n",
      "                                 Current: 0.08 MB\n",
      "                                    Peak: 0.09 MB\n",
      "\n",
      "                \n",
      "...Parquet Write Execution time: 0.1315 seconds\n",
      "\n",
      "                Id---------------------------------------------\n",
      "                                      Id: read_parquet\n",
      "                Runtime----------------------------------------\n",
      "                                 Runtime: 0.02 milliseconds\n",
      "\n",
      "                I/O Size---------------------------------------\n",
      "                               File Size: 0.00 bytes\n",
      "\n",
      "                I/O Counts-------------------------------------\n",
      "                      Targeted disk read: 0.00 counts\n",
      "                     Targeted disk write: 0.00 counts\n",
      "                       General disk read: 0.00 counts\n",
      "                      General disk write: 0.00 counts\n",
      "\n",
      "                I/O Time---------------------------------------\n",
      "                 Targeted disk read time: 0.00 milliseconds\n",
      "                Targeted disk write time: 0.00 milliseconds\n",
      "                  General disk read time: 0.00 milliseconds\n",
      "                 General disk write time: 0.00 milliseconds\n",
      "\n",
      "                Memory------------------------------------------\n",
      "                                 Current: 0.06 MB\n",
      "                                    Peak: 1.10 MB\n",
      "\n",
      "                \n",
      "...Parquet Read Execution time: 0.0222 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_parquet\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_parquet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Exiting __main__ process\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Exiting __main__ process\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END PROGRAM\n",
      "Elapsed time: 0.9928143040015129\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # note that this design now deviates from previous methods.\n",
    "    # Implementation will assume a single execution of a single PIID folder, scanning results and\n",
    "    # appending metrics to a single ASCII file as the code proceeds thus ensuring multi-processor, *nix driven execution.\n",
    "\n",
    "    start_t = perf_counter()\n",
    "    print(\"BEGIN PROGRAM\")\n",
    "\n",
    "    ############################################\n",
    "    # CONSTANTS\n",
    "    ############################################\n",
    "\n",
    "    # Semantic Versioning\n",
    "    VERSION_NAME = \"MLDATAREADY\"\n",
    "    VERSION_MAJOR = 0\n",
    "    VERSION_MINOR = 0\n",
    "    VERSION_RELEASE = 1\n",
    "\n",
    "    DATA_VERSION_RELEASE = \"-\".join(\n",
    "        [\n",
    "            str(VERSION_NAME),\n",
    "            str(VERSION_MAJOR),\n",
    "            str(VERSION_MINOR),\n",
    "            str(VERSION_RELEASE),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # OUTPUT EXTENSIONS\n",
    "    OUTPUT_PANDAS_EXT = \"pkl\"\n",
    "    OUTPUT_NUMPY_EXT = \"npy\"\n",
    "    OUTPUT_TORCH_EXT = \"pt\"\n",
    "    OUTPUT_XARRAY_EXT = \"xr\"\n",
    "    OUTPUT_ZARR_EXT = \"zarr\"\n",
    "    OUTPUT_PARQUET_EXT = \"parquet\"\n",
    "    OUTPUT_TENSORFLOW_EXT = \"tf\"\n",
    "    OUTPUT_PYSTAC_EXT = \"psc\"\n",
    "    OUTPUT_DASK_EXT = \"dask\"\n",
    "    # location of our working files\n",
    "    # WORKING_FOLDER=\"/content/folderOnColab\"\n",
    "    WORKING_FOLDER = \"./folderOnColab\"\n",
    "    input_directory = \"./folderOnColab\"\n",
    "    output_directory = \"./folderOnColab\"\n",
    "\n",
    "    # Notebook Author details\n",
    "    AUTHOR_NAME = \"Christopher G Wood\"\n",
    "    GITHUB_USERNAME = \"christophergarthwood\"\n",
    "    AUTHOR_EMAIL = \"christopher.g.wood@gmail.com\"\n",
    "\n",
    "    # GEOSPATIAL NAMES\n",
    "    LAT_LNAME = \"latitude\"\n",
    "    LAT_SNAME = \"lat\"\n",
    "    LONG_LNAME = \"longitude\"\n",
    "    LONG_SNAME = \"lon\"\n",
    "    PRODUCT_LNAME = \"chlor_a\"\n",
    "    PRODUCT_SNAME = \"chlor_a\"\n",
    "\n",
    "    # PRODUCT_LNAME=\"salinity\"\n",
    "    # PRODUCT_SNAME=\"salinity\"\n",
    "\n",
    "    # Encoding\n",
    "    ENCODING = \"utf-8\"\n",
    "    os.environ[\"PYTHONIOENCODING\"] = ENCODING\n",
    "\n",
    "    BOLD_START = \"\\033[1m\"\n",
    "    BOLD_END = \"\\033[0;0m\"\n",
    "    TEXT_WIDTH = 77\n",
    "\n",
    "    # You can also adjust the verbosity by changing the value of TF_CPP_MIN_LOG_LEVEL:\n",
    "    #\n",
    "    # 0 = all messages are logged (default behavior)\n",
    "    # 1 = INFO messages are not printed\n",
    "    # 2 = INFO and WARNING messages are not printed\n",
    "    # 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "    TF_CPP_MIN_LOG_LEVEL_SETTING = 0\n",
    "\n",
    "    # Set the Seed for the experiment (ask me why?)\n",
    "    # seed the pseudorandom number generator\n",
    "    # THIS IS ESSENTIAL FOR CONSISTENT MODEL OUTPUT, remember these are random in nature.\n",
    "    # SEED_INIT = 7\n",
    "    # random.seed(SEED_INIT)\n",
    "    # tf.random.set_seed(SEED_INIT)\n",
    "    # np.random.seed(SEED_INIT)\n",
    "\n",
    "    DEBUG_STACKTRACE = 0\n",
    "    DEBUG_USING_GPU = 0   #no gpu utilization on 0, 1 is gpu utilization\n",
    "    NUM_PROCESSORS = 10\n",
    "    ITERATIONS = 20\n",
    "\n",
    "    # make comparisons lower case and include wild card character at the end of each to catch anomalous file extensions like xlsx, etc.\n",
    "    EXTENSIONS = [\".nc\"]\n",
    "    LOWER_EXTENSIONS = [x.lower() for x in EXTENSIONS]\n",
    "\n",
    "    THE_DEVICE_NAME = \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
    "    if DEBUG_USING_GPU == 1:\n",
    "        THE_DEVICE_NAME = \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    # GPU Setup (for multiple GPU devices)\n",
    "    device = torch.cuda.current_device()\n",
    "\n",
    "    # softare watermark\n",
    "    lib_diagnostics()\n",
    "\n",
    "    # hardware specs\n",
    "    get_hardware_stats()\n",
    "\n",
    "    # - Core workhorse routine\n",
    "    process(input_directory)\n",
    "    # - Save the results\n",
    "    # save_output(docs, output_directory, \"policy\")\n",
    "\n",
    "    end_t = perf_counter()\n",
    "    print(\"END PROGRAM\")\n",
    "    print(f\"Elapsed time: {end_t - start_t}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "STEM-004_ComputerVision.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "033cb5a8c21c4ca5a13b5d03e8b78fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebb9ca01cf52473699f21f15c5ec7d34",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_689fe090492947608518efc9f3bd6d5d",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n          "
     }
    },
    "0d8988d435644a928829ef71435a3e83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1015b1ce380943d48c7a6386444c768a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f5405303114146a28663cb1c46db7beb",
       "IPY_MODEL_bdc74a6d20a041819b6fc6d1c3f85a25",
       "IPY_MODEL_033cb5a8c21c4ca5a13b5d03e8b78fa0"
      ],
      "layout": "IPY_MODEL_2b447186de7a4571821ca59295744ce3"
     }
    },
    "202c8798fdda4778bf09a2124c37a6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "usfs-ai-bootcamp",
       "usfa-ai-advanced-training",
       "I will setup my own"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Set Your Project:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_dd3de71c792f4762af3b280ac9b7f5e6",
      "style": "IPY_MODEL_fc3fc48bdebe4501998819fbda36152c"
     }
    },
    "2b447186de7a4571821ca59295744ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "31b012d7d41a4d06933aa38e8e3bd74a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "479233081fbc44c8afe9a044ebb95faf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "689fe090492947608518efc9f3bd6d5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad3b9f4855d541e49303c03fb9f07db7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Accept",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_f196621910824a00a97d06e5433cb294",
      "style": "IPY_MODEL_31b012d7d41a4d06933aa38e8e3bd74a",
      "tooltip": ""
     }
    },
    "bdc74a6d20a041819b6fc6d1c3f85a25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_202c8798fdda4778bf09a2124c37a6c7",
       "IPY_MODEL_ad3b9f4855d541e49303c03fb9f07db7"
      ],
      "layout": "IPY_MODEL_0d8988d435644a928829ef71435a3e83"
     }
    },
    "cf363ab273bf4eeba7fbea4d79ed3a64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd3de71c792f4762af3b280ac9b7f5e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebb9ca01cf52473699f21f15c5ec7d34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f196621910824a00a97d06e5433cb294": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5405303114146a28663cb1c46db7beb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf363ab273bf4eeba7fbea4d79ed3a64",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_479233081fbc44c8afe9a044ebb95faf",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n\n        <table><tr><td>\n            <span style=\"font-family: Tahoma;font-size: 18\">\n              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n              Please verify that you are in the appropriate project and that the:</br>\n              <center><code><b>PROJECT_ID</b></code> </br></center>\n              aligns with the Project Id in the upper left corner of this browser and that the location:\n              <center><code><b>LOCATION</b></code> </br></center>\n              aligns with the instructions provided.\n            </span>\n          </td></tr></table></br></br>\n\n    "
     }
    },
    "fc3fc48bdebe4501998819fbda36152c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
