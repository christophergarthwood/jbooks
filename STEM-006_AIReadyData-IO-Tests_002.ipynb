{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "Jg3iJooMQjWA",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Artificial Intelligence\n",
    "## AI Ready Data - 006\n",
    "### Download, curate, and process weather and tree data.\n",
    "\n",
    "<center>\n",
    "<table align=\"center\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/christophergarthwood/jbooks/blob/main/STEM-006_AIReadyData.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/notebooks?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Link to Colab Enterprise\n",
    "    </a>\n",
    "  </td>   \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/christophergarthwood/jbooks/blob/main/STEM-006_AIReadyData.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Link to Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "</center>\n",
    "</br></br></br>\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Christopher G Wood](https://github.com/christophergarthwood)  |\n",
    "\n",
    "# Overview\n",
    "\n",
    "Using various data sources we will download, review, and package data in various formats exploring the options for \"AI Ready\" data and what that means.\n",
    "\n",
    "## What is a \"**AI Ready**\" Data?\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "### Python Tools in General (credit to: https://www.linkedin.com/in/milan-janosov/)\n",
    "\n",
    "#### ğƒğšğ­ğš ğŒğšğ§ğ¢ğ©ğ®ğ¥ğšğ­ğ¢ğ¨ğ§ \n",
    "- Polars: https://pola.rs \n",
    "- Modin: https://lnkd.in/d97bYx79 \n",
    "- Pandas: https://pandas.pydata.org \n",
    "- Vaex: https://vaex.io \n",
    "- Datatable: https://lnkd.in/dApHaBCT \n",
    "- CuPy: https://docs.cupy.dev \n",
    "- NumPy: https://numpy.org \n",
    "\n",
    "#### ğƒğšğ­ğš ğ•ğ¢ğ¬ğ®ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ \n",
    "- Plotly: https://plotly.com/python \n",
    "- Altair: https://lnkd.in/d8pN88j9 \n",
    "- Matplotlib: https://matplotlib.org \n",
    "- Seaborn: https://seaborn.pydata.org \n",
    "- Geoplotlib: https://lnkd.in/d8wtm5CN \n",
    "- Folium: https://lnkd.in/d-yMZhSf \n",
    "- Bokeh: https://docs.bokeh.org \n",
    "- Pygal: http://www.pygal.org \n",
    "\n",
    "#### ğ’ğ­ğšğ­ğ¢ğ¬ğ­ğ¢ğœğšğ¥ ğ€ğ§ğšğ¥ğ²ğ¬ğ¢ğ¬ \n",
    "- SciPy: https://scipy.org \n",
    "- PyMC3: https://docs.pymc.io \n",
    "- PyStan: https://lnkd.in/dyDq23S6 \n",
    "- Statsmodels: https://lnkd.in/dTAJ-sv9 \n",
    "- Lifelines: https://lnkd.in/drgZ54cj \n",
    "- Pingouin: https://pingouin-stats.org \n",
    "\n",
    "#### ğŒğšğœğ¡ğ¢ğ§ğ ğ‹ğğšğ«ğ§ğ¢ğ§ğ  \n",
    "- JAX: https://jax.readthedocs.io \n",
    "- Keras: https://keras.io \n",
    "- Theano: https://lnkd.in/d8Ga6xvV \n",
    "- XGBoost: https://lnkd.in/d7856MDc \n",
    "- Scikit-learn: https://scikit-learn.org \n",
    "- TensorFlow: https://tensorflow.org \n",
    "- PyTorch: https://pytorch.org \n",
    "\n",
    "#### ğğšğ­ğ®ğ«ğšğ¥ ğ‹ğšğ§ğ ğ®ğšğ ğ ğğ«ğ¨ğœğğ¬ğ¬ğ¢ğ§ğ  \n",
    "- NLTK: https://www.nltk.org \n",
    "- BERT: https://lnkd.in/d3DsJsyD \n",
    "- spaCy: https://spacy.io \n",
    "- TextBlob: https://lnkd.in/dNSdHsjC \n",
    "- Polyglot: https://lnkd.in/dWbhNJrn \n",
    "- Gensim: https://lnkd.in/d4bRCJTC \n",
    "- Pattern: https://lnkd.in/dCbSXzs6 \n",
    "\n",
    "#### ğƒğšğ­ğšğ›ğšğ¬ğ ğğ©ğğ«ğšğ­ğ¢ğ¨ğ§ğ¬ \n",
    "- Dask: https://dask.org \n",
    "- PySpark: https://lnkd.in/dqrKCvvu \n",
    "- Ray: https://docs.ray.io \n",
    "- Koalas: https://lnkd.in/dUwXiSWr \n",
    "- Kafka: https://kafka.apache.org \n",
    "- Hadoop: https://hadoop.apache.org \n",
    "\n",
    "#### ğ“ğ¢ğ¦ğ ğ’ğğ«ğ¢ğğ¬ ğ€ğ§ğšğ¥ğ²ğ¬ğ¢ğ¬ \n",
    "- Sktime: https://lnkd.in/dspnqaEH \n",
    "- Darts: https://lnkd.in/dD6kbSRw \n",
    "- AutoTS: https://lnkd.in/dpquYFRd \n",
    "- Prophet: https://lnkd.in/df8Xt5zG \n",
    "- Kats: https://lnkd.in/dPyQ8vpT \n",
    "- TSFresh: https://lnkd.in/dB8JDJF7 \n",
    "\n",
    "#### ğ–ğğ› ğ’ğœğ«ğšğ©ğ¢ğ§ğ  \n",
    "- Beautiful Soup: https://lnkd.in/drxaifkC \n",
    "- Scrapy: https://scrapy.org \n",
    "- Octoparse: https://www.octoparse.com \n",
    "- Selenium: https://www.selenium.dev\n",
    "\n",
    "![Python - Life is Short](./img/PythonLifeIsShort.jpg)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Additional References\n",
    "\n",
    "#### Data Formats\n",
    "+ [List of ML File Formats](https://github.com/trailofbits/ml-file-formats)\n",
    "+ [ML Guide to Data Formats](https://www.hopsworks.ai/post/guide-to-file-formats-for-machine-learning)\n",
    "+ [Why are ML Data Structures Different?](https://stackoverflow.blog/2023/01/04/getting-your-data-in-shape-for-machine-learning/)\n",
    "\n",
    "#### FAIR\n",
    "+ [FAIR and AI-Ready](https://repository.niddk.nih.gov/public/NIDDKCR_Office_Hours_AI-Readiness_and_Preparing_AI-Ready_+Datasets_12_2023.pdf)\n",
    "+ [AI-Ready-Data](https://www.rishabhsoft.com/blog/ai-ready-data)\n",
    "+ [AI-Ready FAIR Data](https://medium.com/@sean_hill/ai-ready-fair-data-accelerating-science-through-responsible-ai-and-data-stewardship-3b4f21c804fd)\n",
    "+ [AI-Ready Data ... Quality](https://www.elucidata.io/blog/building-ai-ready-data-why-quality-matters-more-than-quantity)\n",
    "+ [AI-Ready Data Explained](https://acodis.io/hubfs/pdfs/AI-ready%20data%20Explained%20Whitepaper%20(1).pdf)\n",
    "\n",
    "+ [GCP with BigQuery DataFrames](https://cloud.google.com/blog/products/data-analytics/building-aiml-apps-in-python-with-bigquery-dataframes)\n",
    "\n",
    "#### Format Libraries / Standards\n",
    "+ [Earth Science Information partners (ESIP)](https://www.esipfed.org/checklist-ai-ready-data/)\n",
    "+ [Zarr - Storage of N-dimensional arrays (tensors)](https://zarr.dev/#description)\n",
    "  + [Zarr explained](https://aijobs.net/insights/zarr-explained/)\n",
    "+ [Apache Parquet](https://parquet.apache.org/)\n",
    "  + [All about Parquet](https://medium.com/data-engineering-with-dremio/all-about-parquet-part-01-an-introduction-b62a5bcf70f8)\n",
    "+ [PySTAC - SpatioTemporal Asset Catalogs](https://pystac.readthedocs.io/en/stable/)\n",
    "  + [John Hogland's Spatial Modeling Tutorials](https://github.com/jshogland/SpatialModelingTutorials/blob/main/README.md)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stages of Data Interactivity in an AI/ML engagement\n",
    "\n",
    "\n",
    "Data preparation is critical in any machine learning project because it directly influences your model's performance and accuracy.\n",
    "\n",
    "1. Data wrangling\n",
    "2. Data curation\n",
    "3. Data utilization\n",
    "\n",
    "### Data Wrangling\n",
    "You gather the data, preferably ALL OF IT, thought necessary to produce the answer to your question / requirements.  Identify dependent and independent variables.\n",
    "\n",
    "### Data Curation\n",
    "Next, you clean the data by removing or correcting missing values, outliers, or inconsistencies.\n",
    "\n",
    "\n",
    "Then, you transform the data through processes like normalization and encoding to make it compatible with machine learning algorithms.\n",
    "\n",
    "\n",
    "### Data Utilization\n",
    "Finally, you reduce the data's complexity without losing the information it can provide to the machine learning model, often using techniques like dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1737665372909,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "M3qlCehNBu-_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's define some variables (information holders) for our project overall\n",
    "\n",
    "global PROJECT_ID, BUCKET_NAME, LOCATION\n",
    "BUCKET_NAME = \"cio-training-vertex-colab\"\n",
    "PROJECT_ID = \"usfs-ai-bootcamp\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "BOLD_START = \"\\033[1m\"\n",
    "BOLD_END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508,
     "referenced_widgets": [
      "1015b1ce380943d48c7a6386444c768a",
      "f5405303114146a28663cb1c46db7beb",
      "bdc74a6d20a041819b6fc6d1c3f85a25",
      "033cb5a8c21c4ca5a13b5d03e8b78fa0",
      "2b447186de7a4571821ca59295744ce3",
      "cf363ab273bf4eeba7fbea4d79ed3a64",
      "479233081fbc44c8afe9a044ebb95faf",
      "202c8798fdda4778bf09a2124c37a6c7",
      "ad3b9f4855d541e49303c03fb9f07db7",
      "0d8988d435644a928829ef71435a3e83",
      "ebb9ca01cf52473699f21f15c5ec7d34",
      "689fe090492947608518efc9f3bd6d5d",
      "dd3de71c792f4762af3b280ac9b7f5e6",
      "fc3fc48bdebe4501998819fbda36152c",
      "f196621910824a00a97d06e5433cb294",
      "31b012d7d41a4d06933aa38e8e3bd74a"
     ]
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1737665373417,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "3TrA1A5nIeCV",
    "outputId": "65ad0051-dd12-4511-e88f-62bf0f35300e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f076a7ae6e694130ba86390dd9cf4622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now create a means of enforcing project id selection\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def wait_for_button_press():\n",
    "\n",
    "    button_pressed = False\n",
    "\n",
    "    # Create widgets\n",
    "    html_widget = widgets.HTML(\n",
    "        value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "\n",
    "        <table><tr><td>\n",
    "            <span style=\"font-family: Tahoma;font-size: 18\">\n",
    "              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n",
    "              Please verify that you are in the appropriate project and that the:</br>\n",
    "              <center><code><b>PROJECT_ID</b></code> </br></center>\n",
    "              aligns with the Project Id in the upper left corner of this browser and that the location:\n",
    "              <center><code><b>LOCATION</b></code> </br></center>\n",
    "              aligns with the instructions provided.\n",
    "            </span>\n",
    "          </td></tr></table></br></br>\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    project_list = [\n",
    "        \"usfs-ai-bootcamp\",\n",
    "        \"usfa-ai-advanced-training\",\n",
    "        \"I will setup my own\",\n",
    "    ]\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=project_list,\n",
    "        value=project_list[0],\n",
    "        description=\"Set Your Project:\",\n",
    "    )\n",
    "\n",
    "    html_widget2 = widgets.HTML(\n",
    "        value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "          \"\"\"\n",
    "    )\n",
    "\n",
    "    button = widgets.Button(description=\"Accept\")\n",
    "\n",
    "    # Function to handle the selection change\n",
    "    def on_change(change):\n",
    "        global PROJECT_ID\n",
    "        if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
    "            # print(\"Selected option:\", change['new'])\n",
    "            PROJECT_ID = change[\"new\"]\n",
    "\n",
    "    # Observe the dropdown for changes\n",
    "    dropdown.observe(on_change)\n",
    "\n",
    "    def on_button_click(b):\n",
    "        nonlocal button_pressed\n",
    "        global PROJECT_ID\n",
    "        button_pressed = True\n",
    "        # button.disabled = True\n",
    "        button.close()  # Remove the button from display\n",
    "        with output:\n",
    "            # print(f\"Button pressed...continuing\")\n",
    "            # print(f\"Selected option: {dropdown.value}\")\n",
    "            PROJECT_ID = dropdown.value\n",
    "\n",
    "    button.on_click(on_button_click)\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Create centered layout\n",
    "    centered_layout = widgets.VBox(\n",
    "        [\n",
    "            html_widget,\n",
    "            widgets.HBox([dropdown, button]),\n",
    "            html_widget2,\n",
    "        ],\n",
    "        layout=widgets.Layout(\n",
    "            display=\"flex\", flex_flow=\"column\", align_items=\"center\", width=\"100%\"\n",
    "        ),\n",
    "    )\n",
    "    # Display the layout\n",
    "    display(centered_layout)\n",
    "\n",
    "\n",
    "wait_for_button_press()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zramkw-P93C-"
   },
   "source": [
    "## Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1737666212893,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "shY7a4DVQjWB",
    "outputId": "606fe1d2-f3de-47cf-f457-bb449cca5b52",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are likely running this notebook with Jupyter iPython runtime at 2025-03-22 12:07:43.493397 in the usfs-ai-bootcamp lab.\n"
     ]
    }
   ],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# - Google Colab Check\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "import datetime\n",
    "\n",
    "RunningInCOLAB = False\n",
    "RunningInCOLAB = \"google.colab\" in str(get_ipython())\n",
    "current_time = datetime.datetime.now()\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    print(\n",
    "        f\"You are running this notebook in Google Colab at {current_time} in the {BOLD_START}{PROJECT_ID}{BOLD_END}lab.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"You are likely running this notebook with Jupyter iPython runtime at {current_time} in the {PROJECT_ID} lab.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZVkISRuLURi"
   },
   "source": [
    "## Library Management\n",
    "### Load Libraries necessary for this operation via pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1737665373426,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "logDyNfnLURj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import key libraries necessary to support dynamic installation of additional libraries\n",
    "import sys\n",
    "\n",
    "# Use subprocess to support running operating system commands from the program, using the \"bang\" (!)\n",
    "# symbology is supported, however that does not translate to an actual python script, this is a more\n",
    "# agnostic approach.\n",
    "import subprocess\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74882,
     "status": "ok",
     "timestamp": 1737665448288,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "ldXG-5fhsV1e",
    "outputId": "5941b5ee-93f4-41df-ccc2-8915e0e73ed7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlibraries = [\\n    \"backoff\",\\n    \"python-dotenv\",\\n    \"seaborn\",\\n    \"piexif\",\\n    \"unidecode\",\\n    \"icecream\",\\n    \"watermark\",\\n    \"watermark[GPU]\",\\n    \"rich\",\\n    \"rich[jupyter]\",\\n    \"numpy\",\\n    \"pydot\",\\n    \"polars[all]\",\\n    \"dask[complete]\",\\n    \"pytables\",\\n    \"xarray\",\\n    \"pandas\",\\n    \"pystac\",\\n    \"pystac[jinja2]\",\\n    \"pystac[orjson]\",\\n    \"pystac[validation]\",\\n    \"fastparquet\",\\n    \"zarr\",\\n    \"gdown\",\\n    \"wget\",\\n]\\n\\n# Loop through each library and test for existence, if not present install quietly\\nfor library in libraries:\\n    if library == \"Pillow\":\\n        spec = importlib.util.find_spec(\"PIL\")\\n    else:\\n        spec = importlib.util.find_spec(library)\\n    if spec is None:\\n        print(\"Installing library \" + library)\\n        subprocess.run([\"pip\", \"install\", library, \"--quiet\"], check=True)\\n    else:\\n        print(\"Library \" + library + \" already installed.\")\\n\\n# Specialized install for GPU enabled capability with CUDF\\n# pip install --extra-index-url=https://pypi.nvidia.com \"cudf-cu12==25.2.*\" \"dask-cudf-cu12==25.2.*\" \"cuml-cu12==25.2.*\" \"cugraph-cu12==25.2.*\" \"nx-cugraph-cu12==25.2.*\" \"cuspatial-cu12==25.2.*\"     \"cuproj-cu12==25.2.*\" \"cuxfilter-cu12==25.2.*\" \"cucim-cu12==25.2.*\"\\ntry:\\n    library=\"cudf-cu12\"\\n    spec = importlib.util.find_spec(library)\\n    if spec is None:\\n        subprocess.run(\\n            [\\n                \"pip\",\\n                \"install\",\\n                \"--extra-index-url=https://pypi.nvidia.com\",\\n                library,\\n                \"--quiet\",\\n            ],\\n            check=True,\\n        )\\n    else:\\n        print(\"Library \" + library + \" already installed.\")\\n\\n    library=\"dask-cudf-cu12\"\\n    spec = importlib.util.find_spec(library)\\n    if spec is None:\\n        subprocess.run(\\n            [\\n                \"pip\",\\n                \"install\",\\n                \"--extra-index-url=https://pypi.nvidia.com\",\\n                library,\\n                \"--quiet\",\\n            ],\\n            check=True,\\n        )\\n    else:\\n        print(\"Library \" + library + \" already installed.\")\\n\\nexcept (subprocess.CalledProcessError, RuntimeError, Exception) as e:\\n    print(repr(e))\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the libraries you'd like to add to this Runtime environment.\n",
    "# Commented out as this adds time but is critical for initial run.\n",
    "\"\"\"\n",
    "libraries = [\n",
    "    \"backoff\",\n",
    "    \"python-dotenv\",\n",
    "    \"seaborn\",\n",
    "    \"piexif\",\n",
    "    \"unidecode\",\n",
    "    \"icecream\",\n",
    "    \"watermark\",\n",
    "    \"watermark[GPU]\",\n",
    "    \"rich\",\n",
    "    \"rich[jupyter]\",\n",
    "    \"numpy\",\n",
    "    \"pydot\",\n",
    "    \"polars[all]\",\n",
    "    \"dask[complete]\",\n",
    "    \"pytables\",\n",
    "    \"xarray\",\n",
    "    \"pandas\",\n",
    "    \"pystac\",\n",
    "    \"pystac[jinja2]\",\n",
    "    \"pystac[orjson]\",\n",
    "    \"pystac[validation]\",\n",
    "    \"fastparquet\",\n",
    "    \"zarr\",\n",
    "    \"gdown\",\n",
    "    \"wget\",\n",
    "]\n",
    "\n",
    "# Loop through each library and test for existence, if not present install quietly\n",
    "for library in libraries:\n",
    "    if library == \"Pillow\":\n",
    "        spec = importlib.util.find_spec(\"PIL\")\n",
    "    else:\n",
    "        spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "        print(\"Installing library \" + library)\n",
    "        subprocess.run([\"pip\", \"install\", library, \"--quiet\"], check=True)\n",
    "    else:\n",
    "        print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "# Specialized install for GPU enabled capability with CUDF\n",
    "# pip install --extra-index-url=https://pypi.nvidia.com \"cudf-cu12==25.2.*\" \"dask-cudf-cu12==25.2.*\" \"cuml-cu12==25.2.*\" \"cugraph-cu12==25.2.*\" \"nx-cugraph-cu12==25.2.*\" \"cuspatial-cu12==25.2.*\"     \"cuproj-cu12==25.2.*\" \"cuxfilter-cu12==25.2.*\" \"cucim-cu12==25.2.*\"\n",
    "try:\n",
    "    library=\"cudf-cu12\"\n",
    "    spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"pip\",\n",
    "                \"install\",\n",
    "                \"--extra-index-url=https://pypi.nvidia.com\",\n",
    "                library,\n",
    "                \"--quiet\",\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "    library=\"dask-cudf-cu12\"\n",
    "    spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"pip\",\n",
    "                \"install\",\n",
    "                \"--extra-index-url=https://pypi.nvidia.com\",\n",
    "                library,\n",
    "                \"--quiet\",\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "except (subprocess.CalledProcessError, RuntimeError, Exception) as e:\n",
    "    print(repr(e))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO_Hq5eq9joH"
   },
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 5041,
     "status": "ok",
     "timestamp": 1737665453315,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "PJuXEPlkSo9p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Import additional libraries that add value to the project related to NLP\n",
    "\n",
    "# - Set of libraries that perhaps should always be in Python source\n",
    "import backoff\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "import gc\n",
    "import getopt\n",
    "import glob\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "import socket\n",
    "import sys\n",
    "import textwrap\n",
    "import tqdm\n",
    "import traceback\n",
    "import warnings\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "#- Datastructures\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "#- Profiling\n",
    "from time import perf_counter\n",
    "import gc\n",
    "import io\n",
    "import tracemalloc\n",
    "import psutil\n",
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "#- Text formatting\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.traceback import install\n",
    "from tabulate import tabulate\n",
    "import locale\n",
    "\n",
    "# - Displays system info\n",
    "from watermark import watermark as the_watermark\n",
    "from py3nvml import py3nvml\n",
    "\n",
    "# - Additional libraries for this work\n",
    "import math\n",
    "from base64 import b64decode\n",
    "from IPython.display import Image, Markdown\n",
    "import pandas, IPython.display as display, io, jinja2, base64\n",
    "from IPython.display import clear_output  # used to support real-time plotting\n",
    "import requests\n",
    "import unidecode\n",
    "import pydot\n",
    "import wget\n",
    "\n",
    "# - Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import dask as da\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "import tables                   #Optional dependency required for dask to to_hdf, to_parquet used.\n",
    "import xarray as xr\n",
    "import cupy_xarray              # never actually invoked in source itself use ds=ds.cupy.as_cupy(), not valuable to I/O\n",
    "import pystac as pys\n",
    "import pystac\n",
    "from pystac.utils import datetime_to_str\n",
    "\n",
    "# from stacframes import df_from\n",
    "import fastparquet as fq\n",
    "import zarr\n",
    "from zarr import Group\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "try:\n",
    "    import cudf\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import cupy\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Tensorflow and related AI libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import data as tf_data\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "\n",
    "# - Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.cbook import get_sample_data\n",
    "from matplotlib.offsetbox import AnnotationBbox, DrawingArea, OffsetImage, TextArea\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.patches import Circle\n",
    "from PIL import Image as PIL_Image\n",
    "import PIL.ImageOps\n",
    "import matplotlib.image as mpimg\n",
    "from imageio import imread\n",
    "import seaborn as sns\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from pylab import *\n",
    "\n",
    "# - Image meta-data for Section 508 compliance\n",
    "import piexif\n",
    "from piexif.helper import UserComment\n",
    "\n",
    "# - Progress bar\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataclass used to represent each metric used during execution\n",
    "#\n",
    "@dataclass\n",
    "class runtime_metrics:\n",
    "    id: str\n",
    "\n",
    "    # see @Profile\n",
    "    runtime: float = field(init=False, default=0.0)\n",
    "\n",
    "    # reference: https://docs.python.org/3/library/profile.html\n",
    "    profile_data: cProfile.Profile = field(init=False)\n",
    "\n",
    "    # reference: https://www.geeksforgeeks.org/how-to-get-file-size-in-python/\n",
    "    file_size: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "    \n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_throughput: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_throughput: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_read_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_write_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_read_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_write_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # calculated in MBs, reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "    mem_current: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # calculated in MBs, reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "    mem_peak: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "                Id---------------------------------------------\n",
    "                                      Id: {self.id}\n",
    "                Runtime----------------------------------------\n",
    "                                 Runtime: {self.runtime:,.2f} milliseconds\n",
    "\n",
    "                I/O Size---------------------------------------\n",
    "                               File Size: {self.file_size:,.2f} bytes\n",
    "\n",
    "                I/O Counts-------------------------------------\n",
    "                      Targeted disk read: {self.io_disk_read_count:,.2f} counts\n",
    "                     Targeted disk write: {self.io_disk_write_count:,.2f} counts\n",
    "                       General disk read: {self.io_os_read_count:,.2f} counts\n",
    "                      General disk write: {self.io_os_write_count:,.2f} counts\n",
    "\n",
    "                I/O Time---------------------------------------\n",
    "                 Targeted disk read time: {self.io_disk_read_time:,.2f} milliseconds\n",
    "                Targeted disk write time: {self.io_disk_write_time:,.2f} milliseconds\n",
    "                  General disk read time: {self.io_os_read_time:,.2f} milliseconds\n",
    "                 General disk write time: {self.io_os_write_time:,.2f} milliseconds\n",
    "\n",
    "                Memory------------------------------------------\n",
    "                                 Current: {self.mem_current:,.2f} MB\n",
    "                                    Peak: {self.mem_peak:,.2f} MB\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "    #def __repr__(self):\n",
    "    #    return f'{self.__class__.__name__}(name={self.name!r}, unit_price={self.unit_price!r}, quantity={self.quantity_on_hand!r})'\n",
    "\n",
    "    # TODO - CGW\n",
    "    # def __post_init__(self):\n",
    "    #    self.id = f'{self.phrase}_{self.word_type.name.lower()}'\n",
    "\n",
    "    # worthy consideration - https://www.geeksforgeeks.org/psutil-module-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Profiling function custom created to track IO, memory, and runtme.\n",
    "# Reference: https://jiffyclub.github.io/snakeviz/\n",
    "# Reference: https://www.machinelearningplus.com/python/cprofile-how-to-profile-your-python-code/\n",
    "# Reference: https://cloud.google.com/stackdriver/docs/instrumentation/setup/python\n",
    "# Reference: https://www.turing.com/kb/python-code-with-cprofile\n",
    "\n",
    "def profile(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "\n",
    "        # custom metrics values\n",
    "        current_memories = 0.0\n",
    "        peak_memories = 0.0\n",
    "        current_metric = runtime_metrics(id=func.__name__)\n",
    "        disk = \"sdc\"\n",
    "\n",
    "        #####################################################################################################\n",
    "        # - Cprofiler startup\n",
    "        # Reference: https://www.google.com/search?client=firefox-b-1-d&q=python+example+use+of+cprofle+for+a+single+function#cobssid=s\n",
    "        #####################################################################################################\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        #####################################################################################################\n",
    "        # - Memory tracking\n",
    "        #  Reference: https://docs.python.org/3/library/tracemalloc.html\n",
    "        #  Reference: https://www.kdnuggets.com/how-to-trace-memory-allocation-in-python\n",
    "        #  Reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "        #####################################################################################################\n",
    "        tracemalloc.start()\n",
    "\n",
    "        #####################################################################################################\n",
    "        # - Disk tracking\n",
    "        # Reference: https://stackoverflow.com/questions/16945664/insight-needed-into-python-psutil-output#:~:text=1%20Answer%201%20%C2%B7%20read_count:%20number%20of,write_bytes:%20number%20of%20bytes%20written%20%C2%B7%20read_time:\n",
    "        #####################################################################################################\n",
    "        iocnt1 = psutil.disk_io_counters(perdisk=True)[disk]\n",
    "        disk_io_counters1 = psutil.disk_io_counters()\n",
    "        read_bytes_start = iocnt1.read_bytes\n",
    "        write_bytes_start = iocnt1.write_bytes\n",
    "        read_counters_start = iocnt1.read_count\n",
    "        write_counters_start = iocnt1.write_count\n",
    "        read_time_start = iocnt1.read_time\n",
    "        write_time_start = iocnt1.write_time\n",
    "        \n",
    "        read_os_bytes_start = disk_io_counters1.read_bytes\n",
    "        write_os_bytes_start = disk_io_counters1.write_bytes\n",
    "        read_os_counters_start = disk_io_counters1.read_count\n",
    "        write_os_counters_start = disk_io_counters1.write_count\n",
    "        read_os_time_start = disk_io_counters1.read_time\n",
    "        write_os_time_start = disk_io_counters1.write_time\n",
    "        \n",
    "        #####################################################################################################\n",
    "        # - Actual function call\n",
    "        #####################################################################################################\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        # disk close out\n",
    "        # targeted I/O\n",
    "        iocnt2 = psutil.disk_io_counters(perdisk=True)[\"sdc\"]\n",
    "        disk_io_counters2 = psutil.disk_io_counters()\n",
    "\n",
    "        #targeted disk\n",
    "        read_bytes_end = iocnt2.read_bytes\n",
    "        write_bytes_end = iocnt2.write_bytes\n",
    "        read_counters_end = iocnt2.read_count\n",
    "        write_counters_end = iocnt2.write_count\n",
    "        read_time_end = iocnt2.read_time\n",
    "        write_time_end = iocnt2.write_time\n",
    "        #general OS\n",
    "        read_os_bytes_end = disk_io_counters2.read_bytes\n",
    "        write_os_bytes_end = disk_io_counters2.write_bytes\n",
    "        read_os_counters_end = disk_io_counters2.read_count\n",
    "        write_os_counters_end = disk_io_counters2.write_count\n",
    "        read_os_time_end = disk_io_counters2.read_time\n",
    "        write_os_time_end = disk_io_counters2.write_time\n",
    "\n",
    "        #targeted disk\n",
    "        read_throughput = (read_bytes_end - read_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        write_throughput = (write_bytes_end - write_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        read_counters = (read_counters_end - read_counters_start)\n",
    "        write_counters = (read_counters_end - read_counters_start)\n",
    "        read_time =  (read_time_end - read_time_start)\n",
    "        write_time = (write_time_end - write_time_start)\n",
    "        current_metric.io_disk_read_throughput = read_throughput\n",
    "        current_metric.io_disk_write_throughput = write_throughput\n",
    "        current_metric.io_disk_read_count = read_counters\n",
    "        current_metric.io_disk_write_count = write_counters\n",
    "        current_metric.io_disk_read_time = read_time\n",
    "        current_metric.io_disk_write_time = write_time\n",
    "\n",
    "        #general OS\n",
    "        read_os_throughput = (read_os_bytes_end - read_os_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        write_os_throughput = (write_os_bytes_end - write_os_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        read_os_counters = (read_os_counters_end - read_os_counters_start)\n",
    "        write_os_counters = (read_os_counters_end - read_os_counters_start)\n",
    "        read_os_time =  (read_os_time_end - read_os_time_start)\n",
    "        write_os_time = (write_os_time_end - write_os_time_start)\n",
    "        current_metric.io_os_read_throughput = read_os_throughput\n",
    "        current_metric.io_os_write_throughput = write_os_throughput\n",
    "        current_metric.io_os_read_count = read_os_counters\n",
    "        current_metric.io_os_write_count = write_os_counters\n",
    "        current_metric.io_os_read_time = read_os_time\n",
    "        current_metric.io_os_write_time = write_os_time\n",
    "\n",
    "\n",
    "\n",
    "        # memory close\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()        \n",
    "        current_metric.mem_current = current / (1024 * 1024)\n",
    "        current_metric.mem_peak = peak / (1024 * 1024)\n",
    "        tracemalloc.clear_traces()\n",
    "\n",
    "\n",
    "        # CProfiler disabled\n",
    "        pr.disable()\n",
    "\n",
    "        #can't pickle this result\n",
    "        #current_metric.profile_data=pr\n",
    "        \n",
    "        # s = io.StringIO()\n",
    "        # sortby = SortKey.CUMULATIVE\n",
    "        # ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "        # ps.print_stats()\n",
    "        # print(s.getvalue())\n",
    "        end_time = time.perf_counter()\n",
    "        # other characteristics\n",
    "        current_metric.runtime = end_time - start_time\n",
    "\n",
    "        timestamp = datetime.datetime.now()\n",
    "        #timestamp_str = timestamp.strftime(\"%Y%m%d%H%M%S%f\") # Format as string\n",
    "        timestamp_str = timestamp.strftime(\"%Y%m%d%H%M%S\") # Format as string\n",
    "        unique_id = uuid.uuid4()\n",
    "        filename = f\"{output_directory}/{timestamp_str}_{unique_id}_{current_metric.id}_profiler.pkl\"\n",
    "        #print(filename)\n",
    "        #print(\"##########################################\")\n",
    "        #print(current_metric)\n",
    "\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(current_metric, file)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Complex Function to Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def complex_function():\n",
    "    # Define the size of the matrix\n",
    "    matrix_size = 2048\n",
    "    # Generate two random matrices\n",
    "    matrix_a = np.random.rand(matrix_size, matrix_size)\n",
    "    matrix_b = np.random.rand(matrix_size, matrix_size)\n",
    "    result_matrix = np.matmul(matrix_a, matrix_b)\n",
    "    np.savez(\n",
    "        \"./folderOnColab/data/local_test.npy\",\n",
    "        the_matrix=result_matrix,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FUa8QJT9tw_"
   },
   "source": [
    "## Function Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lib Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1737665453699,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "v_CqUVLZ98Mz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lib_diagnostics() -> None:\n",
    "\n",
    "    import pkg_resources\n",
    "\n",
    "    package_name_length = 20\n",
    "    package_version_length = 10\n",
    "\n",
    "    # Show notebook details\n",
    "    #%watermark?\n",
    "    #%watermark --github_username christophergwood --email christopher.g.wood@gmail.com --date --time --iso8601 --updated --python --conda --hostname --machine --githash --gitrepo --gitbranch --iversions --gpu\n",
    "    # Watermark\n",
    "    print(\n",
    "        the_watermark(\n",
    "            author=f\"{AUTHOR_NAME}\",\n",
    "            github_username=f\"GITHUB_USERNAME\",\n",
    "            email=f\"{AUTHOR_EMAIL}\",\n",
    "            iso8601=True,\n",
    "            datename=True,\n",
    "            current_time=True,\n",
    "            python=True,\n",
    "            updated=True,\n",
    "            hostname=True,\n",
    "            machine=True,\n",
    "            gitrepo=True,\n",
    "            gitbranch=True,\n",
    "            githash=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f\"{BOLD_START}Packages:{BOLD_END}\")\n",
    "    print(\"\")\n",
    "    # Get installed packages\n",
    "    the_packages = [\n",
    "        \"nltk\",\n",
    "        \"numpy\",\n",
    "        \"os\",\n",
    "        \"pandas\",\n",
    "        \"keras\",\n",
    "        \"seaborn\",\n",
    "        \"fastparquet\",\n",
    "        \"zarr\",\n",
    "        \"dask\",\n",
    "        \"pystac\",\n",
    "        \"polars\",\n",
    "        \"xarray\",\n",
    "    ]  # Functions are like legos that do one thing, this function outputs library version history of effort.\n",
    "\n",
    "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "    for package_idx, package_name in enumerate(installed):\n",
    "        if package_name in the_packages:\n",
    "            installed_version = installed[package_name]\n",
    "            print(\n",
    "                f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\"\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        print(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
    "        print(\n",
    "            f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        print(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
    "            print(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
    "            print(f\"{'     current':<40}#: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"No GPU available, using CPU.\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 508 Compliance Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1737665453948,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "Nl_kxpFUKKD5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Routines designed to support adding ALT text to an image generated through Matplotlib.\n",
    "\n",
    "\n",
    "def capture(figure):\n",
    "    buffer = io.BytesIO()\n",
    "    figure.savefig(buffer)\n",
    "    # return F\"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "    return f\"data:image/jpg;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "\n",
    "\n",
    "def make_accessible(figure, template, **kwargs):\n",
    "    return display.Markdown(\n",
    "        f\"\"\"![]({capture(figure)} \"{template.render(**globals(), **kwargs)}\")\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# requires JPG's or TIFFs\n",
    "def add_alt_text(image_path, alt_text):\n",
    "    try:\n",
    "        if os.path.isfile(image_path):\n",
    "            img = PIL_Image.open(image_path)\n",
    "            if \"exif\" in img.info:\n",
    "                exif_dict = piexif.load(img.info[\"exif\"])\n",
    "            else:\n",
    "                exif_dict = {}\n",
    "\n",
    "            w, h = img.size\n",
    "            if \"0th\" not in exif_dict:\n",
    "                exif_dict[\"0th\"] = {}\n",
    "            exif_dict[\"0th\"][piexif.ImageIFD.XResolution] = (w, 1)\n",
    "            exif_dict[\"0th\"][piexif.ImageIFD.YResolution] = (h, 1)\n",
    "\n",
    "            software_version = \" \".join(\n",
    "                [\"STEM-001 with Python v\", str(sys.version).split(\" \")[0]]\n",
    "            )\n",
    "            exif_dict[\"0th\"][piexif.ImageIFD.Software] = software_version.encode(\n",
    "                \"utf-8\"\n",
    "            )\n",
    "\n",
    "            if \"Exif\" not in exif_dict:\n",
    "                exif_dict[\"Exif\"] = {}\n",
    "            exif_dict[\"Exif\"][piexif.ExifIFD.UserComment] = UserComment.dump(\n",
    "                alt_text, encoding=\"unicode\"\n",
    "            )\n",
    "\n",
    "            exif_bytes = piexif.dump(exif_dict)\n",
    "            img.save(image_path, \"jpeg\", exif=exif_bytes)\n",
    "        else:\n",
    "            rprint(\n",
    "                f\"Cound not fine {image_path} for ALT text modification, please check your paths.\"\n",
    "            )\n",
    "\n",
    "    except (FileExistsError, FileNotFoundError, Exception) as e:\n",
    "        process_exception(e)\n",
    "\n",
    "\n",
    "# Appears to solve a problem associated with GPU use on Colab, see: https://github.com/explosion/spaCy/issues/11909\n",
    "def getpreferredencoding(do_setlocale=True):\n",
    "    return \"UTF-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libary Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_library_configuration() -> None:\n",
    "\n",
    "    ############################################\n",
    "    # - JUPYTER NOTEBOOK OUTPUT CONTROL / FORMATTING\n",
    "    ############################################\n",
    "    # pandas set floating point to 4 places to things don't run loose\n",
    "    debug.msg_info(\"Setting Pandas and Numpy library options.\")\n",
    "    pd.set_option(\n",
    "        \"display.max_colwidth\", 10\n",
    "    )  # None if you want to view the full json blob in the printed dataframe, use this\n",
    "    pd.options.display.float_format = \"{:,.4f}\".format\n",
    "    np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Exception Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this function displays the stack trace on errors from a central location making adjustments to the display on an error easier to manage\n",
    "# functions perform useful solutions for highly repetitive code\n",
    "def process_exception(inc_exception: Exception) -> None:\n",
    "    if DEBUG_STACKTRACE == 1:\n",
    "        traceback.print_exc()\n",
    "        console.print_exception(show_locals=True)\n",
    "    else:\n",
    "        rprint(repr(inc_exception))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Stats for a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quick_df_stats(\n",
    "    inc_df: pd.DataFrame,\n",
    "    inc_header_count: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load the data and return as a pd.DataFrame.\n",
    "\n",
    "            Parameters:\n",
    "                   inc_df (pd.DataFrame): Dataframe to be inspected, displayed\n",
    "                   inc_header_count (int): Anticipated number of columns to read in (validation check)\n",
    "\n",
    "            Returns:\n",
    "                    Printed output\n",
    "    \"\"\"\n",
    "    print(\"Data Resolution has: \" + str(inc_df.columns))\n",
    "    print(\"\\n\")\n",
    "    print(f\"\"\"{\"size\":20} : {inc_df.size:15,} \"\"\")\n",
    "    print(f\"\"\"{\"shape\":20} : {str(inc_df.shape):15} \"\"\")\n",
    "    print(f\"\"\"{\"ndim\":20} : {inc_df.ndim:15,} \"\"\")\n",
    "    print(f\"\"\"{\"column size\":20} : {inc_df.columns.size:15,} \"\"\")\n",
    "\n",
    "    # index added so you get an extra column\n",
    "    print(f\"\"\"{\"Read\":20} : {inc_df.columns.size:15,} \"\"\")\n",
    "    print(f\"\"\"{\"Expected\":20} : {inc_header_count:15,} \"\"\")\n",
    "    if (inc_df.columns.size) == inc_header_count:\n",
    "        print(f\"{BOLD_START}Expectations met{BOLD_END}.\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Expectations {BOLD_START}not met{BOLD_END}, check your datafile, columns don't match.\"\n",
    "        )\n",
    "    rprint(\"\\n\")\n",
    "    # rprint(str(inc_df.describe()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the FIADB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66496,
     "status": "ok",
     "timestamp": 1737665520749,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "AZcbq467sgrc",
    "outputId": "0a82f71c-4bcc-45bb-8681-8cce7803c49a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://research.fs.usda.gov/programs/fia#data-and-tools\n",
    "# Forest Inventory Asset Database (FIADB)\n",
    "\n",
    "\n",
    "def download_fiadb() -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    dataset_long_names = [\n",
    "        \"ALASKA_AK\",\n",
    "        \"CALIFORNIA_CA\",\n",
    "        \"HAWAII_HI\",\n",
    "        \"IDAHO_ID\",\n",
    "        \"NEVADA_NV\",\n",
    "        \"OREGON_OR\",\n",
    "        \"WASHINGTON_WA\",\n",
    "        \"ARIZONA_AZ\",\n",
    "        \"ARKANSAS_AR\",\n",
    "        \"COLORADO_CO\",\n",
    "        \"IOWA_IA\",\n",
    "        \"KANSAS_KS    \",\n",
    "        \"LOUISIANA_LA\",\n",
    "        \"MINNESOTA_MN\",\n",
    "        \"MISSOURI_MO\",\n",
    "        \"MONTANA_MT\",\n",
    "        \"NEBRASKA_NE\",\n",
    "        \"NEW_MEXICO_NM\",\n",
    "        \"NORTH_DAKOTA_ND\",\n",
    "        \"OKLAHOMA_OK\",\n",
    "        \"SOUTH_DAKOTA_SD\",\n",
    "        \"TEXAS_TX\",\n",
    "        \"U    TAH_UT\",\n",
    "        \"WYOMING_WY\",\n",
    "        \"ALABAMA_AL\",\n",
    "        \"CONNECTICUT_CT\",\n",
    "        \"DELAWARE_DE\",\n",
    "        \"FLORIDA_FL\",\n",
    "        \"GEORGIA_GA\",\n",
    "        \"ILLINOIS_IL\",\n",
    "        \"INDIANA_IN\",\n",
    "        \"KENTUCKY_KY\",\n",
    "        \"MAINE_ME\",\n",
    "        \"MARYLAND    _MD\",\n",
    "        \"MASSACHUSETTS_MA\",\n",
    "        \"MICHIGAN_MI\",\n",
    "        \"MISSISSIPPI_MS\",\n",
    "        \"NEW_HAMPSHIRE_NH\",\n",
    "        \"NEW_JERSEY_NJ\",\n",
    "        \"NEW_YORK_NY\",\n",
    "        \"NORTH_CAROLINA_NC\",\n",
    "        \"OHIO_OH\",\n",
    "        \"PENNSYLVANIA_PA\",\n",
    "        \"    RHODE_ISLAND_RI\",\n",
    "        \"SOUTH_CAROLINA_SC\",\n",
    "        \"TENNESSEE_TN\",\n",
    "        \"VERMONT_VT\",\n",
    "        \"VIRGINIA_VA\",\n",
    "        \"WEST_VIRGINIA_WV\",\n",
    "        \"WISCONSIN_WI\",\n",
    "        \"GUAM_GU\",\n",
    "        \"FEDERATED_STATES_OF_MICRONES_FM    \",\n",
    "        \"NORTHERN_MARIANA_ISLANDS_MP\",\n",
    "        \"PALAU_PW\",\n",
    "        \"AMERICAN_SAMOA_AS\",\n",
    "        \"PUERTO_RICO_PR\",\n",
    "        \"US_VIRGIN_ISLANDS_VI\",\n",
    "    ]\n",
    "    dataset_short_names = [\n",
    "        \"AK\",\n",
    "        \"AL\",\n",
    "        \"AR\",\n",
    "        \"AS\",\n",
    "        \"AZ\",\n",
    "        \"CA\",\n",
    "        \"CO\",\n",
    "        \"CT\",\n",
    "        \"DE\",\n",
    "        \"FL\",\n",
    "        \"GA\",\n",
    "        \"GU\",\n",
    "        \"HI\",\n",
    "        \"IA\",\n",
    "        \"ID\",\n",
    "        \"IL\",\n",
    "        \"IN\",\n",
    "        \"KS\",\n",
    "        \"KY\",\n",
    "        \"LA\",\n",
    "        \"MA\",\n",
    "        \"MD\",\n",
    "        \"ME\",\n",
    "        \"MI\",\n",
    "        \"MN\",\n",
    "        \"MO\",\n",
    "        \"MP\",\n",
    "        \"MS\",\n",
    "        \"MT\",\n",
    "        \"NC\",\n",
    "        \"ND\",\n",
    "        \"NE\",\n",
    "        \"NH\",\n",
    "        \"NJ\",\n",
    "        \"NM\",\n",
    "        \"NV\",\n",
    "        \"NY\",\n",
    "        \"OH\",\n",
    "        \"OK\",\n",
    "        \"OR\",\n",
    "        \"PA\",\n",
    "        \"PR\",\n",
    "        \"PW\",\n",
    "        \"RI\",\n",
    "        \"SC\",\n",
    "        \"SD\",\n",
    "        \"SFM\",\n",
    "        \"TN\",\n",
    "        \"TX\",\n",
    "        \"UT\",\n",
    "        \"VA\",\n",
    "        \"VI\",\n",
    "        \"VT\",\n",
    "        \"WA\",\n",
    "        \"WI\",\n",
    "        \"WV\",\n",
    "        \"WY\",\n",
    "    ]\n",
    "    # dataset_pattern=\"https://apps.fs.usda.gov/fia/datamart/CSV/MT_VEG_SUBPLOT.zip\"\n",
    "    dataset_pattern = \"https://apps.fs.usda.gov/fia/datamart/CSV/\"\n",
    "\n",
    "    rprint(\"Performing `wget` on target FIA records.\")\n",
    "    target_folder = WORKING_FOLDER\n",
    "    if os.path.isdir(target_folder):\n",
    "        target_directory = f\"{target_folder}{os.sep}downloads\"\n",
    "        for idx, filename in enumerate(dataset_short_names):\n",
    "            if os.path.isdir(target_directory):\n",
    "                target_filename = f\"{filename}_CSV.zip\"\n",
    "                target_url = f\"{dataset_pattern}{target_filename}\"\n",
    "                try:\n",
    "                    rprint(\n",
    "                        f\"...copying {dataset_long_names[idx]} to target folder: {target_directory}\"\n",
    "                    )\n",
    "                    subprocess.run(\n",
    "                        [\n",
    "                            \"/usr/bin/wget\",\n",
    "                            \"--show-progress\",\n",
    "                            f\"--directory-prefix={target_directory}\",\n",
    "                            f\"{target_url}\",\n",
    "                        ],\n",
    "                        check=True,\n",
    "                    )\n",
    "                    rprint(\"......completed\")\n",
    "                except (subprocess.CalledProcessError, Exception) as e:\n",
    "                    rprocess_exception(e)\n",
    "            else:\n",
    "                rprint(\n",
    "                    f\"...target folder: {target_directory} isn't present for {filename} download.\"\n",
    "                )\n",
    "    else:\n",
    "        rprint(\n",
    "            \"ERROR: Local downloads folder not found/created.  Check the output to ensure your folder is created.\"\n",
    "        )\n",
    "        rprint(f\"...target folder: {target_directory}\")\n",
    "        rprint(\"...if you can't find the problem contact the instructor.\")\n",
    "\n",
    "    # Process the downloaded data, open it up\n",
    "    rprint(\"Uncompressing the downloads...\")\n",
    "    if os.path.isdir(target_folder):\n",
    "        source_directory = f\"{target_folder}{os.sep}downloads\"\n",
    "        target_directory = f\"{target_folder}{os.sep}data\"\n",
    "        if os.path.isdir(target_directory) and os.path.isdir(source_directory):\n",
    "            for idx, filename in enumerate(dataset_short_names):\n",
    "                target_filename = f\"{filename}_CSV.zip\"\n",
    "                final_directory = f\"{target_directory}{os.sep}{filename}{os.sep}\"\n",
    "                try:\n",
    "                    if os.path.isfile(f\"{source_directory}{os.sep}{target_filename}\"):\n",
    "                        rprint(\n",
    "                            f\"...unzipping {dataset_long_names[idx]} to created target folder: {final_directory}\"\n",
    "                        )\n",
    "                        subprocess.run([\"mkdir\", \"-p\", final_directory], check=True)\n",
    "                        subprocess.run(\n",
    "                            [\n",
    "                                \"/usr/bin/unzip\",\n",
    "                                \"-o\",\n",
    "                                \"-qq\",\n",
    "                                \"-d\",\n",
    "                                f\"{final_directory}\",\n",
    "                                f\"{source_directory}{os.sep}{target_filename}\",\n",
    "                            ],\n",
    "                            check=True,\n",
    "                        )\n",
    "                        process1 = subprocess.Popen(\n",
    "                            [\n",
    "                                \"/usr/bin/find\",\n",
    "                                f\"{final_directory}\",\n",
    "                                \"-type\",\n",
    "                                \"f\",\n",
    "                                \"-print\",\n",
    "                            ],\n",
    "                            stdout=subprocess.PIPE,\n",
    "                        )\n",
    "                        process2 = subprocess.Popen(\n",
    "                            [\"wc\", \"-l\"], stdin=process1.stdout, stdout=subprocess.PIPE\n",
    "                        )\n",
    "\n",
    "                        # Close the output of process1 to allow process2 to receive EOF\n",
    "                        process1.stdout.close()\n",
    "                        output, error = process2.communicate()\n",
    "                        process2.stdout.close()\n",
    "                        number_files = output.decode().strip()\n",
    "                        rprint(f\"......completed, {number_files} files extracted.\")\n",
    "                    else:\n",
    "                        rprint(\n",
    "                            f\"......failed, unable to find ({source_directory}{os.sep}{target_filename}{os.sep})\"\n",
    "                        )\n",
    "                except (subprocess.CalledProcessError, Exception) as e:\n",
    "                    process_exception(e)\n",
    "                break\n",
    "        else:\n",
    "            rprint(\n",
    "                f\"...either the source directory ({source_directory})  or the ({target_directory}) isn't present for extraction.\"\n",
    "            )\n",
    "    else:\n",
    "        rprint(\n",
    "            \"ERROR: Local downloads folder not found/created.  Check the output to ensure your folder is created.\"\n",
    "        )\n",
    "        rprint(f\"...target folder: {target_directory}\")\n",
    "        rprint(\"...if you can't find the problem contact the instructor.\")\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDLoEWmVR9Sk"
   },
   "source": [
    "#### Download NOAA GDS 0.25 Degree Data for a Range of Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1737665520751,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "IZrJ8FhpR78m",
    "outputId": "111a3aec-6ad8-4889-c3ed-7f8e9ed4e737",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://polar.ncep.noaa.gov/global/data_access.shtml\n",
    "# Global Forecast System (GFS), 0.25 degree resolution\n",
    "def download_noaa() -> None:\n",
    "\n",
    "    print(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    dataset_url = \"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gfs.20250304/00/atmos/gfs.t00z.atmf000.nc\"\n",
    "    dataset_url_pattern_begin = (\n",
    "        f\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gfs.\"\n",
    "    )\n",
    "    dataset_filename_pattern = \"gfs.t00z.atmf000.nc\"\n",
    "    dataset_url_pattern_end = f\"/00/atmos/{dataset_filename_pattern}\"\n",
    "    dataset_date_month = \"03\"\n",
    "    dataset_day_start = int(18)\n",
    "    dataset_day_end = int(19)\n",
    "\n",
    "    print(\"...Performing `wget` on target GFS records.\")\n",
    "    target_folder = WORKING_FOLDER\n",
    "    if os.path.isdir(target_folder):\n",
    "        target_directory = f\"{target_folder}{os.sep}data\"\n",
    "        if os.path.isdir(target_directory):\n",
    "            for idx, day in enumerate(range(dataset_day_start, dataset_day_end)):\n",
    "                if day < 10:\n",
    "                    day = f\"0{day}\"\n",
    "                target_date = f\"2025{dataset_date_month}{day}\"\n",
    "                target_url = \"\".join(\n",
    "                    [dataset_url_pattern_begin, target_date, dataset_url_pattern_end]\n",
    "                )\n",
    "\n",
    "                # remove potentially partial downloads\n",
    "                target_partial_file = (\n",
    "                    f\"{target_folder}{os.sep}{dataset_filename_pattern}\"\n",
    "                )\n",
    "                if os.path.isfile(target_partial_file):\n",
    "                    print(\n",
    "                        f\"...removing {dataset_filename_pattern} as it is likely a partial download.\"\n",
    "                    )\n",
    "                    subprocess.run(\n",
    "                        [\"/usr/bin/rm\", \"-rf\", f\"{target_partial_file}\"], check=True\n",
    "                    )\n",
    "\n",
    "                exit_code=0\n",
    "                stdout=\"\"\n",
    "                stderr=\"\"\n",
    "                try:\n",
    "                    print(f\"......searching for download of day:{target_date}\")\n",
    "                    # subprocess.run([\"/usr/bin/wget\", \"--show-progress\", f\"--directory-prefix={target_directory}\", f\"{target_url}\"], check=True)\n",
    "                    #process = subprocess.run([\"/usr/bin/wget\", \"--quiet\", \" --server-response\", \"--no-check-certificate\", f\"--directory-prefix={target_directory}\", f\"{target_url}\"], check=True, text=True, capture_output=True, shell=False)\n",
    "                    process = subprocess.run([\"/usr/bin/wget\", \"--quiet\", \"--no-check-certificate\", f\"--directory-prefix={target_directory}\", f\"{target_url}\"], check=True)\n",
    "                    print(process)\n",
    "                    stdout = process.stdout\n",
    "                    stderr = process.stderr\n",
    "                    exit_code = process.check_returncode()\n",
    "                except (subprocess.CalledProcessError) as e:\n",
    "                    stdout = e.stdout\n",
    "                    stderr = e.stderr\n",
    "                    exit_code=e.returncode\n",
    "                    process_exception(e)\n",
    "                    pass\n",
    "\n",
    "                if exit_code == 0:\n",
    "                    try:\n",
    "                        if os.path.isfile(dataset_filename_pattern):\n",
    "                            print(f\"......completed download of day:{target_date}\")\n",
    "                        target_filename = \"_\".join([target_date, dataset_filename_pattern])\n",
    "                        os.rename(dataset_filename_pattern, target_filename)\n",
    "                        print(f\".........renamed file to {target_filename}\")\n",
    "                        if os.path.isfile(target_filename):\n",
    "                            print(f\".........{BOLD_START}SUCCESS{BOLD_END}.\")\n",
    "                        else:\n",
    "                            print(f\".........inspect download, there could be a problem.\")\n",
    "                    except (subprocess.CalledProcessError, Exception) as e:\n",
    "                        process_exception(e)\n",
    "                        pass\n",
    "                else:\n",
    "                        print(f\"......didn't complete download of day:{target_date}\")\n",
    "                        print(f\".........{BOLD_START}FAIL{BOLD_END}.\")\n",
    "                        print(\"\")\n",
    "                        print(\"\")\n",
    "                break\n",
    "        else:\n",
    "            print(\n",
    "                f\"ERROR: Target folder, {target_directory}, isn't present for {target_date} download.\"\n",
    "            )\n",
    "            raise SystemError\n",
    "    else:\n",
    "        print(\n",
    "            \"ERROR: Local downloads folder not found/created.  Check the output to ensure your folder is created.\"\n",
    "        )\n",
    "        print(f\"...target folder: {target_directory}\")\n",
    "        raise SystemError\n",
    "\n",
    "    print(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Single Specific NetCDF (MS Bight in Gulf of America) from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_test() -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    THE_FILE = \"ACS.txt\"\n",
    "    THE_ID = \"12L8VRY6J1Sj-B1vIf-ODh4kjHWHqIzm8\"\n",
    "\n",
    "    THE_FILE = \"MissBight_2020010900.nc\"\n",
    "    THE_ID = \"1uYMFrdeVD7_qvG2wRbyu6ir9C6b4wAZC\"\n",
    "\n",
    "    target_folder = f\"{WORKING_FOLDER}{os.sep}data\"\n",
    "\n",
    "    target_ids = [THE_ID]\n",
    "    target_filenames = [THE_FILE]\n",
    "\n",
    "    for idx, the_id in enumerate(target_ids):\n",
    "        try:\n",
    "            if os.path.isfile(f\"{target_folder}{os.sep}{target_filenames[idx]}\"):\n",
    "                rprint(f\"...no need to download {target_filenames[idx]} again.\")\n",
    "            else:\n",
    "                rprint(f\"...downloading {target_filenames[idx]}.\")\n",
    "                subprocess.run(\n",
    "                    [\n",
    "                        \"gdown\",\n",
    "                        f\"{the_id}\",\n",
    "                        \"--no-check-certificate\",\n",
    "                        \"--continue\",\n",
    "                        \"-O\",\n",
    "                        f\"{target_folder}{os.sep}{target_filenames[idx]}\",\n",
    "                    ],\n",
    "                    check=True,\n",
    "                )\n",
    "        except (subprocess.CalledProcessError, Exception) as e:\n",
    "            process_exception(e)\n",
    "            raise SystemError\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTrnAspo3CWR"
   },
   "source": [
    "#### Check your resources from a CPU/GPU perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1737665454280,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "hmGUvC7M3B0H",
    "outputId": "638ecead-d943-42f2-a2eb-99790d7fe3bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hardware_stats() -> None:\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    print(\n",
    "        f\"{BOLD_START}List Devices{BOLD_END} #########################################\"\n",
    "    )\n",
    "    try:\n",
    "        from tensorflow.python.client import device_lib\n",
    "\n",
    "        rprint(device_lib.list_local_devices())\n",
    "        print(\"\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    print(\n",
    "        f\"{BOLD_START}Devices Counts{BOLD_END} ########################################\"\n",
    "    )\n",
    "    try:\n",
    "        rprint(\n",
    "            f\"Num GPUs Available: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\"\n",
    "        )\n",
    "        rprint(\n",
    "            f\"Num CPUs Available: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\"\n",
    "        )\n",
    "        print(\"\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    print(\n",
    "        f\"{BOLD_START}Optional Enablement{BOLD_END} ####################################\"\n",
    "    )\n",
    "    try:\n",
    "        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    if gpus:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        try:\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "            rprint(\n",
    "                str(\n",
    "                    str(len(gpus))\n",
    "                    + \" Physical GPUs,\"\n",
    "                    + str(len(logical_gpus))\n",
    "                    + \" Logical GPU\"\n",
    "                )\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            # Visible devices must be set before GPUs have been initialized\n",
    "            rprint(str(repr(e)))\n",
    "        print(\"\")\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_house() -> None:\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    gc.collect()\n",
    "\n",
    "    # could leave the GPU unstable so holding off.\n",
    "    # torch.cuda.empty_cache()\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuke_file(target_filename: str) -> None:\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    if os.path.isfile(target_filename):\n",
    "        try:\n",
    "            # removing existing file, else you would append\n",
    "            subprocess.run([\"rm\", \"-rf\", f\"{target_filename}\"], check=True)\n",
    "        except (subprocess.CalledProcessError, Exception) as e:\n",
    "            process_exception(e)\n",
    "            raise SystemError\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46JMSTY2QjWD"
   },
   "source": [
    "## Input Sources\n",
    "### Create the storage locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1737665454282,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "pu8f4w5XK6i8",
    "outputId": "2aa1c04c-7c5a-4348-93a1-67ea14f79c83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the folder that will hold our content.\n",
    "def create_storage_locations(inc_directory: str) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    target_folder = inc_directory\n",
    "    sub_folders = [\"downloads\", \"data\"]\n",
    "    rprint(f\"Creating project infrastructure:\")\n",
    "    try:\n",
    "        for idx, subdir in enumerate(sub_folders):\n",
    "            target_directory = f\"{target_folder}{os.sep}{subdir}\"\n",
    "            rprint(f\"...creating ({target_directory}) to store project data.\")\n",
    "            if os.path.isfile(target_directory):\n",
    "                raise OSError(\n",
    "                    f\"Cannot create your folder ({target_directory}) a file of the same name already exists there, work with your instructor or remove it yourself.\"\n",
    "                )\n",
    "            elif os.path.isdir(target_directory):\n",
    "                print(\n",
    "                    f\"......folder named ({target_directory}) {BOLD_START}already exists{BOLD_END}, we won't try to create a new folder.\"\n",
    "                )\n",
    "            else:\n",
    "                subprocess.run([\"mkdir\", \"-p\", target_directory], check=True)\n",
    "    except (subprocess.CalledProcessError, Exception) as e:\n",
    "        process_exception(e)\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_basic_map(data) -> None:\n",
    "\n",
    "    # plt.figure()\n",
    "    lat = the_netcdf.variables[\"latitude\"][:]\n",
    "    lon = the_netcdf.variables[\"longitude\"][:]\n",
    "    data = the_netcdf.variables[\"water_temp\"][0, 0, :, :]\n",
    "\n",
    "    m = Basemap(\n",
    "        projection=\"mill\",\n",
    "        lat_ts=10,\n",
    "        llcrnrlon=lon.min(),\n",
    "        urcrnrlon=lon.max(),\n",
    "        llcrnrlat=lat.min(),\n",
    "        urcrnrlat=lat.max(),\n",
    "        resolution=\"c\",\n",
    "    )\n",
    "\n",
    "    Lon, Lat = meshgrid(lon, lat)\n",
    "    x, y = m(Lon, Lat)\n",
    "\n",
    "    # cs = m.pcolormesh(x,y,data,shading='flat', cmap=plt.cm.jet)\n",
    "    cs = m.pcolormesh(x, y, data, cmap=plt.cm.jet)\n",
    "\n",
    "    m.drawcoastlines()\n",
    "    m.fillcontinents()\n",
    "    m.drawmapboundary()\n",
    "    m.drawparallels(np.arange(-90.0, 120.0, 30.0), labels=[1, 0, 0, 0])\n",
    "    m.drawmeridians(np.arange(-180.0, 180.0, 60.0), labels=[0, 0, 0, 1])\n",
    "\n",
    "    colorbar(cs)\n",
    "    plt.title(\"Example 1: Global RTOFS SST from NOMADS\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate Various Data Storage Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Read\n",
    "\n",
    "Routine created to simulate large read, at once, of many large files for profiling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "#Profiling function call\n",
    "def initial_read(inc_source_filenames: []) -> {}:\n",
    "    the_netcdfs=read_netcdfs(inc_source_filenames)\n",
    "    the_payload=gather_variables(PRODUCT_LNAME, inc_source_filenames, the_netcdfs)\n",
    "\n",
    "    return the_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read NetCDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_netcdfs(inc_source_filenames: []) -> []:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    the_list = []\n",
    "\n",
    "    rprint(f\"...reading NetCDF4 from list of {len(inc_source_filenames)} files:\")\n",
    "    for target_filename in inc_source_filenames:\n",
    "        try:\n",
    "            rprint(f\"......reading NetCDF4 ({target_filename})\")\n",
    "            the_netcdf = Dataset(target_filename, \"r\", format=\"NETCDF4\")\n",
    "            the_list.append(the_netcdf)\n",
    "        except Exception as e:\n",
    "            process_exception(e)\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    return the_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ocean surface only for a single variable, entire world high resolution\n",
    "def gather_variables(inc_product_name:str, inc_file_list:[], inc_netcdfs:[]) -> {}:\n",
    "\n",
    "        rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "        geospatial_lat_nm=LAT_SNAME\n",
    "        geospatial_lon_nm=LONG_SNAME\n",
    "        product_nm=inc_product_name\n",
    "        local_payload={}\n",
    "\n",
    "\n",
    "        master_lat=[]\n",
    "        master_lon=[]\n",
    "        master_varAry=[]\n",
    "    \n",
    "        #single variable exercise\n",
    "        print(\"...pulling data for and stacking it to the core data structure:\")\n",
    "        for idx in range(len(inc_file_list)):\n",
    "            print(f\"......appending {inc_file_list[idx]}\")\n",
    "            lat_length=0\n",
    "            lon_length=0\n",
    "            varAry_length=0\n",
    "            if geospatial_lat_nm in inc_netcdfs[idx].variables:\n",
    "                lat_var = inc_netcdfs[idx].variables[geospatial_lat_nm]\n",
    "                lat_length = len(lat_var)\n",
    "            if geospatial_lon_nm in inc_netcdfs[idx].variables:\n",
    "                lon_var = inc_netcdfs[idx].variables[geospatial_lon_nm]\n",
    "                lon_length = len(lon_var)\n",
    "            if product_nm in inc_netcdfs[idx].variables:\n",
    "                product_var = inc_netcdfs[idx].variables[product_nm]\n",
    "                product_length = len(product_var)\n",
    "\n",
    "            if (lat_length > 0 and lon_length > 0 and product_length > 0):\n",
    "                master_lat.append(np.array(inc_netcdfs[idx].variables[geospatial_lat_nm][:][:].flatten(),dtype=AI_NUMPY_DATA_TYPE))\n",
    "                master_lon.append(np.array(inc_netcdfs[idx].variables[geospatial_lon_nm][:][:].flatten(),dtype=AI_NUMPY_DATA_TYPE))\n",
    "                master_varAry.append(np.array(inc_netcdfs[idx].variables[product_nm][0][0][:][:].flatten(),dtype=AI_NUMPY_DATA_TYPE))\n",
    "            else:\n",
    "                print(f\"FAILURE with processing {inc_file_list[idx]}\")\n",
    "\n",
    "        #Concatenate all in one time     \n",
    "        new_master_lat=np.concatenate(master_lat) \n",
    "        new_master_lon=np.concatenate(master_lon) \n",
    "        new_master_varAry=np.concatenate(master_varAry) \n",
    "\n",
    "        print(f\"...{BOLD_START}{geospatial_lat_nm:10}{\" data type:\":20}{BOLD_END}{str(type(new_master_lat)):20}\")\n",
    "        print(f\".......shape:{new_master_lat.shape}\")\n",
    "        print(f\"....datatype:{new_master_lat.dtype}\")\n",
    "\n",
    "        print(f\"...{BOLD_START}{geospatial_lon_nm:10}{\" data type:\":20}{BOLD_END}{str(type(new_master_lon)):20}\")\n",
    "        print(f\".......shape:{new_master_lon.shape}\")\n",
    "        print(f\"....datatype:{new_master_lon.dtype}\")\n",
    "\n",
    "        print(f\"...{BOLD_START}{\"Data type\":20}({product_nm:20}){BOLD_END}{str(type(new_master_varAry)):20}\")\n",
    "        print(f\".......shape:{new_master_varAry.shape}\")\n",
    "        print(f\"....datatype:{new_master_varAry.dtype}\")\n",
    "\n",
    "        local_payload[LAT_LNAME]=new_master_lat\n",
    "        local_payload[LONG_LNAME]=new_master_lon\n",
    "        local_payload[PRODUCT_LNAME]=new_master_varAry\n",
    "    \n",
    "        rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")\n",
    "    \n",
    "        return local_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pandas(inc_payload: {}) -> pd.DataFrame():\n",
    "\n",
    "    if DEBUG_USING_GPU == 1:\n",
    "        import cudf.pandas\n",
    "\n",
    "        cudf.pandas.install()\n",
    "        import pandas as pd\n",
    "    else:\n",
    "        import pandas as pd\n",
    "\n",
    "    latSeries = pd.Series(inc_payload[LAT_LNAME].flatten())\n",
    "    lonSeries = pd.Series(inc_payload[LONG_LNAME].flatten())\n",
    "    #varSeries = pd.Series(inc_payload[PRODUCT_LNAME][0, 0, :, :].flatten())\n",
    "    varSeries = pd.Series(inc_payload[PRODUCT_LNAME].flatten())\n",
    "\n",
    "    # define a Panda.DataFrame()\n",
    "    frame = {\n",
    "        LAT_LNAME: latSeries,\n",
    "        LONG_LNAME: lonSeries,\n",
    "        PRODUCT_LNAME: varSeries,\n",
    "    }\n",
    "\n",
    "    # instantiate a dataframe\n",
    "    df = pd.DataFrame(frame)\n",
    "\n",
    "    # ensure the data is cast as expected\n",
    "    df[LAT_LNAME].astype(AI_PANDAS_DATA_TYPE)\n",
    "    df[LONG_LNAME].astype(AI_PANDAS_DATA_TYPE)\n",
    "    df[PRODUCT_LNAME].astype(AI_PANDAS_DATA_TYPE)\n",
    "\n",
    "    # clean up behind yourself\n",
    "    del latSeries, lonSeries, varSeries, frame\n",
    "    clean_house()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pandas(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_PANDAS_EXT}\"\n",
    "\n",
    "    # create dataframe\n",
    "    df = build_pandas(inc_payload)\n",
    "    # quick stats\n",
    "    quick_df_stats(df, 3)\n",
    "\n",
    "    # Get memory usage of each column in bytes\n",
    "    memory_usage_per_column = df.memory_usage(deep=True)\n",
    "\n",
    "    # Get total memory usage of the DataFrame in bytes\n",
    "    total_memory_usage = df.memory_usage().sum()\n",
    "    print(f\"Original Dataframe memory use: {total_memory_usage:20,}\")\n",
    "\n",
    "    # WRITE\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_pandas(target_filename, df)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Pandas Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    # READ\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        start_time = time.perf_counter()\n",
    "        read_pandas(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Pandas Read Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_pandas(target_pandas_filename: str, df: pd.DataFrame) -> None:\n",
    "    df.to_pickle(target_pandas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_pandas(target_pandas_filename: str) -> None:\n",
    "    df = pd.read_pickle(target_pandas_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_numpy(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = (\n",
    "        f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_numpy.{OUTPUT_NUMPY_EXT}\"\n",
    "    )\n",
    "\n",
    "    # WRITE\n",
    "    for idx in range(0,ITERATIONS):\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_numpy_native(target_filename, inc_payload)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Numpy Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    df = build_pandas(inc_payload)\n",
    "    for idx in range(0,ITERATIONS):\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_numpy_as_df(target_filename, df)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Numpy pd.DataFrame Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    # READ\n",
    "    for idx in range(0,ITERATIONS):\n",
    "        start_time = time.perf_counter()\n",
    "        read_numpy(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "    print(f\"...Numpy Read Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_numpy_as_df(target_numpy_filename: str, df: pd.DataFrame) -> None:\n",
    "    if df is not None:\n",
    "        df_numpy = df.to_numpy()\n",
    "        np.savez(target_numpy_filename, df_numpy)\n",
    "    else:\n",
    "        print(f\"ERROR, {__name__} failed because the incoming array was None.\")\n",
    "        \n",
    "@profile\n",
    "def write_numpy_native(target_numpy_filename: str, inc_payload: {}) -> None:\n",
    "    # varying sized arrays\n",
    "    if inc_payload is not None:\n",
    "        np.savez(\n",
    "            target_numpy_filename,\n",
    "            LAT_LNAME=inc_payload[LAT_LNAME],\n",
    "            LONG_LNAME=inc_payload[LONG_LNAME],\n",
    "            PRODUCT_LNAME=inc_payload[PRODUCT_LNAME],\n",
    "        )\n",
    "    else:\n",
    "        print(f\"ERROR, {__name__} failed because the incoming array was None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_numpy(target_numpy_filename: str) -> None:\n",
    "    loaded_arr = np.load(target_numpy_filename + \".npz\")\n",
    "\n",
    "    # to unpack\n",
    "    # new_lat = loaded_arr[\"lat\"]\n",
    "    # new_lon = loaded_arr[\"lon\"]\n",
    "    # new_product = loaded_arr[\"product\"]\n",
    "    # del loaded_arr, new_lat, new_lon, new_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pytorch(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = (\n",
    "        f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_TORCH_EXT}\"\n",
    "    )\n",
    "\n",
    "    dev = \"cpu\"\n",
    "    # input_tensor = input_tensor.to(device)\n",
    "    lat_tensor = torch.tensor(inc_payload[LAT_LNAME].flatten(), dtype=AI_TORCH_DATA_TYPE).to(dev)\n",
    "    lon_tensor = torch.tensor(inc_payload[LONG_LNAME].flatten(), dtype=AI_TORCH_DATA_TYPE).to(dev)\n",
    "    var_tensor = torch.tensor(inc_payload[PRODUCT_LNAME].flatten(), dtype=AI_TORCH_DATA_TYPE).to(dev)\n",
    "    \n",
    "    for idx in range(0, ITERATIONS):\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_pytorch(target_filename, lat_tensor, lon_tensor, var_tensor)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...PyTorch Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        start_time = time.perf_counter()\n",
    "        read_pytorch(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...PyTorch Read Execution time: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_pytorch(target_pytorch_filename: str, lat_tensor: torch.tensor, lon_tensor: torch.tensor, var_tensor: torch.tensor) -> None:\n",
    "    # Save multiple tensors as a list\n",
    "    tensors_list = [lat_tensor, lon_tensor, var_tensor]\n",
    "    torch.save(tensors_list, target_pytorch_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_pytorch(target_pytorch_filename: str) -> None:\n",
    "    # Load the tensor from the file\n",
    "    tensor_loaded = torch.load(target_pytorch_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tensorflow(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_TENSORFLOW_EXT}\"\n",
    "    with tf.device(THE_DEVICE_NAME):\n",
    "        # create a TFRecord to store the data\n",
    "        lat_list = tf.train.FloatList(value=inc_payload[LAT_LNAME].flatten().tolist(), )\n",
    "        lon_list = tf.train.FloatList(value=inc_payload[LONG_LNAME].flatten().tolist(), )\n",
    "        varAry_list = tf.train.FloatList(\n",
    "            value=inc_payload[PRODUCT_LNAME].flatten().tolist(), \n",
    "        )\n",
    "        feature = {\n",
    "            LAT_LNAME: tf.train.Feature(float_list=lat_list),\n",
    "            LONG_LNAME: tf.train.Feature(float_list=lon_list),\n",
    "            PRODUCT_LNAME: tf.train.Feature(float_list=varAry_list),\n",
    "        }\n",
    "        tfRecord = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature)\n",
    "        ).SerializeToString()\n",
    "\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_tensorflow(target_filename, tfRecord)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...TensorFlow Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        start_time = time.perf_counter()\n",
    "        read_tensorflow(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...TensorFlow Read Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_tensorflow(target_tensorflow_filename: str, tfRecord) -> None:\n",
    "    with tf.io.TFRecordWriter(target_tensorflow_filename) as writer:\n",
    "        writer.write(tfRecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example_proto):\n",
    "    feature_description = {\n",
    "        \"latitude\": tf.io.FixedLenSequenceFeature(\n",
    "            [], dtype=tf.float32, allow_missing=True\n",
    "        ),\n",
    "        \"longitude\": tf.io.FixedLenSequenceFeature(\n",
    "            [], dtype=tf.float32, allow_missing=True\n",
    "        ),\n",
    "        \"product\": tf.io.FixedLenSequenceFeature(\n",
    "            [], dtype=tf.float32, allow_missing=True\n",
    "        ),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_tensorflow(target_tensorflow_filename: str) -> None:\n",
    "\n",
    "    with tf.device(THE_DEVICE_NAME):\n",
    "        dataset = tf.data.TFRecordDataset(target_tensorflow_filename)\n",
    "        tfRecord = dataset.map(parse_tfrecord_fn)\n",
    "        # for record in tfRecord:\n",
    "        #    lat = record[\"latitude\"]\n",
    "        #    lon = record[\"longitude\"]\n",
    "        #    varAry = record[\"product\"]\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_xarray(the_netcdf) -> None:\n",
    "def process_xarray(inc_netcdf_filenames: []) -> None:\n",
    "\n",
    "    # xarray on GPU: https://github.com/xarray-contrib/cupy-xarray\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "\n",
    "    ds_xr = xr.open_mfdataset(inc_netcdf_filenames, combine='nested', concat_dim='time', parallel=True)\n",
    "    ds_xr = ds_xr.drop_vars(AI_XARRAY_REMOVE_VARIABLES)\n",
    "    #ds_xr = xr.combine_nested(inc_netcdf_filenames, concat_dim=['time', ])\n",
    "    #ds_list = [xr.open_dataset(file) for file in inc_netcdf_filenames]\n",
    "    #for idx,dataset in enumerate(ds_list):\n",
    "    #    ds_list[idx]=dataset.drop_vars(AI_XARRAY_REMOVE_VARIABLES)\n",
    "    #    ds_list[idx].variables[\"lat\"].astype(AI_NUMPY_DATA_TYPE)\n",
    "    #    ds_list[idx].variables[\"lon\"].astype(AI_NUMPY_DATA_TYPE)\n",
    "    #print(ds_list[0])\n",
    "    # Replace 'time' with the appropriate dimension if needed\n",
    "    #dsxr = xr.concat(ds_list[0:len(ds_list)-3], dim='time') \n",
    "    # WRITE - NetCDF\n",
    "\n",
    "    try:\n",
    "        print(\"Starting Xarray NetCDF Write routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_xarray.{OUTPUT_XARRAY_EXT}\"\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_xarray_netcdf(target_filename, ds_xr)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Write (NetCDF) Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        print(\"Starting Xarray NetCDF Read routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            read_xarray_netcdf(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Read (NetCDF) Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    # WRITE - HDF5\n",
    "    try:\n",
    "        print(\"Starting Xarray HDF Write routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            target_filename = (\n",
    "                f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_xarray.h5\"\n",
    "            )\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_xarray_hdf5(target_filename, ds_xr)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Write (HDF5) Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        print(\"Starting Xarray HDF Read routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            read_xarray_hdf5(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Read (HDF5) Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "    # WRITE - ZARR, acting up...not sure why\n",
    "    try:\n",
    "        print(\"Starting Xarray Zarr Write routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_xarray.{OUTPUT_ZARR_EXT}\"\n",
    "            print(f\"delete {target_filename}\")\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_xarray_zarr(target_filename, ds_xr)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Write (Zarr) Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        print(\"Starting Xarray Zarr Read routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            read_xarray_zarr(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Read (Zarr) Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    # WRITE - PICKLE, cannot pickle xarray very easily at all\n",
    "    try:\n",
    "        print(\"Starting Xarray Pickle Write routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_xarray.{OUTPUT_PANDAS_EXT}\"\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_xarray_pickle(target_filename, ds_xr)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Write (Pickle) Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        print(\"Starting Xarray Pickle Read routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            read_xarray_pickle(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Read (Pickle) Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_house()\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_xarray_netcdf(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    if ds_xr is not None:\n",
    "        ds_xr.to_netcdf(target_xarray_filename)\n",
    "    else:\n",
    "        print(\"ERROR saving incoming ds_xr as pickled file because there is no content.\")\n",
    "\n",
    "@profile\n",
    "def write_xarray_hdf5(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    if ds_xr is not None:\n",
    "        ds_xr.to_netcdf(target_xarray_filename)\n",
    "    else:\n",
    "        print(\"ERROR saving incoming ds_xr as pickled file because there is no content.\")\n",
    "\n",
    "\n",
    "def write_xarray_zarr(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    if ds_xr is not None:\n",
    "        ds_xr.to_zarr(target_xarray_filename, mode='w')\n",
    "    else:\n",
    "        print(\"ERROR saving incoming ds_xr as pickled file because there is no content.\")\n",
    "    \n",
    "def write_xarray_pickle(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    if ds_xr is not None:\n",
    "        pkl = pickle.dumps(ds_xr, protocol=1)\n",
    "        with open(target_xarray_filename, \"wb\") as file:\n",
    "            # Use pickle.dump() to serialize and write the data to the file\n",
    "            pickle.dump(pkl, target_xarray_filename)\n",
    "    else:\n",
    "        print(\"ERROR saving incoming ds_xr as pickled file because there is no content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_xarray_netcdf(target_xarray_filename: str) -> None:\n",
    "    ds_xr_loaded = xr.open_dataset(target_xarray_filename, engine=\"netcdf4\")\n",
    "\n",
    "@profile\n",
    "def read_xarray_hdf5(target_xarray_filename: str) -> None:\n",
    "    ds_xr_loaded = xr.open_dataset(target_xarray_filename, engine=\"netcdf4\")\n",
    "\n",
    "@profile\n",
    "def read_xarray_zarr(target_xarray_filename: str) -> None:\n",
    "    ds_xr_loaded = xr.open_zarr(target_xarray_filename)\n",
    "\n",
    "@profile\n",
    "def read_xarray_pickle(target_xarray_filename: str) -> None:\n",
    "    try:\n",
    "        with open(target_xarray_filename, \"rb\") as file:\n",
    "            #ds_xr = pickle.load(file, protocol=-1)\n",
    "            #changed protocol because of saving issues\n",
    "            ds_xr = pickle.load(file, protocol=1)\n",
    "    except (FileNotFoundError, Exception) as e:\n",
    "        process_exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apache Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_parquet(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    # build the xarray dataset from scratch\n",
    "    df = build_pandas(inc_payload)\n",
    "\n",
    "    try:\n",
    "        # WRITE - NetCDF\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_pandas.{OUTPUT_PARQUET_EXT}\"\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_parquet(target_filename, df)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Parquet Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        # READ\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            read_parquet(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Parquet Read Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    del df\n",
    "    clean_house()\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_parquet(target_parquet_filename: str, df: pd.DataFrame) -> None:\n",
    "    if df is not None:\n",
    "        df.to_parquet(target_parquet_filename, compression=\"gzip\")\n",
    "    else:\n",
    "        print(f\"ERROR: {__name__} experienced an error where the incoming Pandas DataFrame was non-existent.\")\n",
    "\n",
    "@profile\n",
    "def read_parquet(target_parquet_filename: str) -> None:\n",
    "    df_parquet = pd.read_parquet(target_parquet_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_zarr(the_payload:{}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    # Zarr\n",
    "    # Save xarray Dataset to Zarr\n",
    "    # ds_zarr=ds_xr.to_zarr('data.zarr')\n",
    "    import zarr\n",
    "        \n",
    "    # ds_xr = xr.open_dataset(target_filename)\n",
    "    geospatial_lat_nm = LAT_SNAME\n",
    "    geospatial_lon_nm = LONG_SNAME\n",
    "    product_nm = PRODUCT_LNAME\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_zarr.{OUTPUT_ZARR_EXT}\"\n",
    "\n",
    "    lat=np.array(the_payload[LAT_LNAME].flatten(), dtype=AI_NUMPY_DATA_TYPE)\n",
    "    lon=np.array(the_payload[LONG_LNAME].flatten(), dtype=AI_NUMPY_DATA_TYPE)\n",
    "    varAry=np.array(the_payload[PRODUCT_LNAME].flatten(), dtype=AI_NUMPY_DATA_TYPE)\n",
    "\n",
    "    try:\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            # WRITE\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_zarr(target_filename, lat,lon,varAry)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Zarr Write Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            # READ\n",
    "            start_time = time.perf_counter()\n",
    "            read_zarr(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Zarr Read Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_zarr(target_zarr_filename: str, lat, lon, varAry) -> None:\n",
    "        root = zarr.group(store=target_zarr_filename, overwrite=True)\n",
    "        z_lat_grp = root.create_group(LAT_LNAME)\n",
    "        z_lon_grp = root.create_group(LONG_LNAME)\n",
    "        z_product_grp = root.create_group(PRODUCT_LNAME)\n",
    "        z_lat_ary = z_lat_grp.create_array( name=LAT_LNAME,shape=lat.shape, chunks=\"auto\", dtype=AI_NUMPY_DATA_TYPE)\n",
    "        z_lon_ary = z_lon_grp.create_array( name=LONG_LNAME,shape=lon.shape, chunks=\"auto\", dtype=AI_NUMPY_DATA_TYPE)\n",
    "        z_product_ary = z_product_grp.create_array( name=PRODUCT_LNAME,shape=varAry.shape, chunks=\"auto\", dtype=AI_NUMPY_DATA_TYPE)\n",
    "    \n",
    "        z_lat_ary[:] = lat\n",
    "        z_lon_ary[:] = lon\n",
    "        z_product_ary[:] = varAry\n",
    "    \n",
    "        print(f\"{BOLD_START}Zarr Node Tree:{BOLD_END}\")\n",
    "        print(root.tree())\n",
    "        #zarr.save(target_zarr_filename, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_zarr(target_zarr_filename: str) -> None:\n",
    "    # Load the group from the directory\n",
    "    z = zarr.open_group(target_zarr_filename, mode='r')\n",
    "    print(z.info)\n",
    "    print(z.tree())\n",
    "    print(z.groups())\n",
    "    for the_group in z.groups():\n",
    "        print(the_group)\n",
    "    \"\"\"\n",
    "    # Verify the structure and data\n",
    "    assert isinstance(loaded_group, zarr.Group)\n",
    "    assert 'root' in loaded_group\n",
    "    assert 'bar' in loaded_group['foo']\n",
    "    assert 'baz' in loaded_group['foo']\n",
    "    np.testing.assert_array_equal(bar, loaded_group['foo']['bar'])\n",
    "    np.testing.assert_array_equal(baz, loaded_group['foo']['baz'])\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dask(inc_payload:{}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_DASK_EXT}\"\n",
    "    df = build_pandas(inc_payload)\n",
    "    df_dask = dd.from_pandas(df, npartitions=1)\n",
    "    # Convert DataFrame rows to dictionaries and create a Dask bag\n",
    "    #df_bag = db.from_sequence(df.to_dict(orient=\"records\"))\n",
    "\n",
    "    # Print the Dask bag (computation is lazy, so compute() is needed to see the result)\n",
    "    #print(df_bag.compute())\n",
    "    try:\n",
    "        for idx in range(0,ITERATIONS):\n",
    "            # WRITE\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_dask.hdf\"\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_dask_hdf(target_filename, df_dask)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Dask Write HDF Execution time: {execution_time:.4f} seconds\")\n",
    "            \n",
    "        for idx in range(0,ITERATIONS): \n",
    "            # READ\n",
    "            start_time = time.perf_counter()\n",
    "            read_dask_hdf(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Dask Read parquet Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        for idx in range(0,ITERATIONS): \n",
    "            #WRITE\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_dask.parquet\"\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_dask_parquet(target_filename, df_dask)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Dask Write parquet Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        for idx in range(0,ITERATIONS): \n",
    "            # READ\n",
    "            start_time = time.perf_counter()\n",
    "            read_dask_parquet(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Dask Read parquet Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_dask_hdf(target_dask_filename: str, inc_dataframe: dd) -> None:\n",
    "    if dd is not None:\n",
    "        inc_dataframe.to_hdf(target_dask_filename, key=PRODUCT_LNAME,) \n",
    "    else:\n",
    "        print(f\"ERROR: {__name__} had an error because the Dask DataFrame was non-existent.\")\n",
    "    \n",
    "@profile\n",
    "def write_dask_parquet(target_dask_filename: str, inc_dataframe: dd) -> None:\n",
    "    if inc_dataframe is not None:\n",
    "        inc_dataframe.to_parquet(target_dask_filename, ) \n",
    "    else:\n",
    "        print(f\"ERROR: {__name__} had an error because the Dask DataFrame was non-existent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_dask_hdf(target_dask_filename: str) -> None:\n",
    "    # Load the group from the directory\n",
    "    dask_dataframe=dd.read_hdf(target_dask_filename, key=PRODUCT_LNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_dask_parquet(target_dask_filename: str) -> None:\n",
    "    # Load the group from the directory\n",
    "    dask_dataframe=dd.read_parquet(target_dask_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySTAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def process_pystac(inc_payload:{}) -> None:\n",
    "\n",
    "        rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample pandas DataFrame\n",
    "        data = {\n",
    "            \"id\": [\"item1\", \"item2\"],\n",
    "            \"geometry\": [\n",
    "                {\"type\": \"Point\", \"coordinates\": [1, 1]},\n",
    "                {\"type\": \"Point\", \"coordinates\": [2, 2]},\n",
    "            ],\n",
    "            \"datetime\": [pd.Timestamp(\"2023-01-01\"), pd.Timestamp(\"2023-01-02\")],\n",
    "            \"properties\": [{\"prop1\": \"value1\"}, {\"prop1\": \"value2\"}],\n",
    "        }\n",
    "        \n",
    "        # Create a STAC Catalog\n",
    "        catalog = pystac.Catalog.from_dict(\n",
    "            {\"type\": \"Catalog\", \"id\": \"acs\", \"stac_version\": \"1.0.0\"}\n",
    "        )\n",
    "        \n",
    "        # Convert DataFrame to STAC Items and add to Catalog\n",
    "        for index, row in df.iterrows():\n",
    "            item = pystac.Item(\n",
    "                id=row[\"id\"],\n",
    "                geometry=row[\"geometry\"],\n",
    "                datetime=row[\"datetime\"].to_pydatetime(),\n",
    "                properties=row[\"properties\"],\n",
    "            )\n",
    "            catalog.add_item(item)\n",
    "        \n",
    "        # Write the catalog to a file\n",
    "        catalog.normalize_hrefs(\"./pystac_data\")\n",
    "        catalog.save_object(pystac.Catalog, \"acs.json\")\n",
    "        \n",
    "        # Read STAC catalog into a DataFrame\n",
    "        # df_from_stac = df_from(catalog)\n",
    "        # print(df_from_stac)\n",
    "        \"\"\"\n",
    "        rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main routine that executes all code, does return a data frame of data for further analysis if desired.\n",
    "#\n",
    "#  @param (None)\n",
    "def process(inc_input_directory: str,\n",
    "            inc_input_pattern: str) -> {}:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    # variables\n",
    "    source_nc_list = []\n",
    "    source_filenames_list = []\n",
    "\n",
    "    # setup storage solution\n",
    "    create_storage_locations(inc_input_directory)\n",
    "\n",
    "\n",
    "    # identify target files\n",
    "    print(\"...marshaling data files:\")\n",
    "    target_directory = f\"{inc_input_directory}{os.sep}data\"\n",
    "    if os.path.isdir(target_directory):\n",
    "        for file in os.listdir(target_directory):\n",
    "            print(f\"......processing {file} from {target_directory}\")\n",
    "            filename, file_extension = os.path.splitext(file)\n",
    "            if file_extension.lower() in LOWER_EXTENSIONS:\n",
    "                if inc_input_pattern in file:\n",
    "                    source_filenames_list.append(os.path.join(target_directory, file))\n",
    "    else:\n",
    "        print(\n",
    "            \"Target directory ({target_directory}) does not exist, cannot continue execution.  Check your paths.\"\n",
    "        )\n",
    "        raise SystemError\n",
    "\n",
    "    source_filenames_list = sorted(source_filenames_list)\n",
    "    #for idx in range(0,ITERATIONS):\n",
    "    the_payload = initial_read(source_filenames_list)\n",
    "    \n",
    "    process_numpy(the_payload)\n",
    "    process_pandas(the_payload)\n",
    "    #process_xarray(source_filenames_list)\n",
    "    process_pytorch(the_payload)\n",
    "    process_tensorflow(the_payload)\n",
    "    process_zarr(the_payload)\n",
    "    process_dask(the_payload)\n",
    "    process_parquet(the_payload)\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Routine (call all other routines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN PROGRAM\n",
      "Author: Christopher G Wood\n",
      "\n",
      "Github username: GITHUB_USERNAME\n",
      "\n",
      "Email: christopher.g.wood@gmail.com\n",
      "\n",
      "Last updated: 2025-03-22T12:07:52.372578-05:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.12.9\n",
      "IPython version      : 8.30.0\n",
      "\n",
      "Compiler    : GCC 13.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.167.4-microsoft-standard-WSL2\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Hostname: ThulsaDoom\n",
      "\n",
      "Git hash: f824406166462e63ed44e15a1c1f57341698d566\n",
      "\n",
      "Git repo: git@github.com:christophergarthwood/jbooks.git\n",
      "\n",
      "Git branch: Updates\n",
      "\n",
      "\u001b[1mPackages:\u001b[0;0m\n",
      "\n",
      "dask                                    #: 2024.12.1           \n",
      "fastparquet                             #: 2024.11.0           \n",
      "keras                                   #: 3.9.0               \n",
      "numpy                                   #: 1.26.4              \n",
      "pandas                                  #: 2.2.3               \n",
      "polars                                  #: 1.24.0              \n",
      "pystac                                  #: 1.12.1              \n",
      "seaborn                                 #: 0.13.2              \n",
      "xarray                                  #: 2024.11.0           \n",
      "zarr                                    #: 3.0.5               \n",
      "TensorFlow version                      #: 2.18.0              \n",
      "     gpu.count:                         #: 1\n",
      "     cpu.count:                         #: 1\n",
      "Torch version                           #: 2.6.0+cu124         \n",
      "     GPUs available?                    #: True\n",
      "     count                              #: 1\n",
      "     current                            #: NVIDIA GeForce RTX 2060\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ get_hardware_stats\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ get_hardware_stats\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mList Devices\u001b[0;0m #########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742663272.446695   10899 service.cc:148] XLA service 0x5617aa072870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742663272.446794   10899 service.cc:156]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1742663272.562366   10899 service.cc:148] XLA service 0x5617aa05f470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742663272.562432   10899 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 2060, Compute Capability 7.5\n",
      "I0000 00:00:1742663272.579413   10899 gpu_device.cc:2022] Created device /device:GPU:0 with 4056 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    name: <span style=\"color: #008000; text-decoration-color: #008000\">\"/device:CPU:0\"</span>\n",
       "device_type: <span style=\"color: #008000; text-decoration-color: #008000\">\"CPU\"</span>\n",
       "memory_limit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">268435456</span>\n",
       "locality <span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "incarnation: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15425302335628098080</span>\n",
       "xla_global_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>\n",
       ",\n",
       "    name: <span style=\"color: #008000; text-decoration-color: #008000\">\"/device:XLA_CPU:0\"</span>\n",
       "device_type: <span style=\"color: #008000; text-decoration-color: #008000\">\"XLA_CPU\"</span>\n",
       "memory_limit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17179869184</span>\n",
       "locality <span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "incarnation: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11933995060937895729</span>\n",
       "physical_device_desc: <span style=\"color: #008000; text-decoration-color: #008000\">\"device: XLA_CPU device\"</span>\n",
       "xla_global_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>\n",
       ",\n",
       "    name: <span style=\"color: #008000; text-decoration-color: #008000\">\"/device:XLA_GPU:0\"</span>\n",
       "device_type: <span style=\"color: #008000; text-decoration-color: #008000\">\"XLA_GPU\"</span>\n",
       "memory_limit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17179869184</span>\n",
       "locality <span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "incarnation: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5620745224192386560</span>\n",
       "physical_device_desc: <span style=\"color: #008000; text-decoration-color: #008000\">\"device: XLA_GPU device\"</span>\n",
       "xla_global_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>\n",
       ",\n",
       "    name: <span style=\"color: #008000; text-decoration-color: #008000\">\"/device:GPU:0\"</span>\n",
       "device_type: <span style=\"color: #008000; text-decoration-color: #008000\">\"GPU\"</span>\n",
       "memory_limit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4253024256</span>\n",
       "locality <span style=\"font-weight: bold\">{</span>\n",
       "  bus_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "  links <span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "incarnation: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6278347137955457797</span>\n",
       "physical_device_desc: <span style=\"color: #008000; text-decoration-color: #008000\">\"device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"</span>\n",
       "xla_global_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">416903419</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    name: \u001b[32m\"/device:CPU:0\"\u001b[0m\n",
       "device_type: \u001b[32m\"CPU\"\u001b[0m\n",
       "memory_limit: \u001b[1;36m268435456\u001b[0m\n",
       "locality \u001b[1m{\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "incarnation: \u001b[1;36m15425302335628098080\u001b[0m\n",
       "xla_global_id: \u001b[1;36m-1\u001b[0m\n",
       ",\n",
       "    name: \u001b[32m\"/device:XLA_CPU:0\"\u001b[0m\n",
       "device_type: \u001b[32m\"XLA_CPU\"\u001b[0m\n",
       "memory_limit: \u001b[1;36m17179869184\u001b[0m\n",
       "locality \u001b[1m{\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "incarnation: \u001b[1;36m11933995060937895729\u001b[0m\n",
       "physical_device_desc: \u001b[32m\"device: XLA_CPU device\"\u001b[0m\n",
       "xla_global_id: \u001b[1;36m-1\u001b[0m\n",
       ",\n",
       "    name: \u001b[32m\"/device:XLA_GPU:0\"\u001b[0m\n",
       "device_type: \u001b[32m\"XLA_GPU\"\u001b[0m\n",
       "memory_limit: \u001b[1;36m17179869184\u001b[0m\n",
       "locality \u001b[1m{\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "incarnation: \u001b[1;36m5620745224192386560\u001b[0m\n",
       "physical_device_desc: \u001b[32m\"device: XLA_GPU device\"\u001b[0m\n",
       "xla_global_id: \u001b[1;36m-1\u001b[0m\n",
       ",\n",
       "    name: \u001b[32m\"/device:GPU:0\"\u001b[0m\n",
       "device_type: \u001b[32m\"GPU\"\u001b[0m\n",
       "memory_limit: \u001b[1;36m4253024256\u001b[0m\n",
       "locality \u001b[1m{\u001b[0m\n",
       "  bus_id: \u001b[1;36m1\u001b[0m\n",
       "  links \u001b[1m{\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "incarnation: \u001b[1;36m6278347137955457797\u001b[0m\n",
       "physical_device_desc: \u001b[32m\"device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\u001b[0m\n",
       "xla_global_id: \u001b[1;36m416903419\u001b[0m\n",
       "\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mDevices Counts\u001b[0;0m ########################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num GPUs Available: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num GPUs Available: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num CPUs Available: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num CPUs Available: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mOptional Enablement\u001b[0;0m ####################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742663272.590062   10899 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4056 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> Physical GPUs,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> Logical GPU\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m Physical GPUs,\u001b[1;36m1\u001b[0m Logical GPU\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ get_hardware_stats\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ get_hardware_stats\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ create_storage_locations\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ create_storage_locations\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating project infrastructure:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating project infrastructure:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span>creating <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">downloads</span><span style=\"font-weight: bold\">)</span> to store project data.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0mcreating \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/\u001b[0m\u001b[95mdownloads\u001b[0m\u001b[1m)\u001b[0m to store project data.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......folder named (./folderOnColab/downloads) \u001b[1malready exists\u001b[0;0m, we won't try to create a new folder.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span>creating <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">data</span><span style=\"font-weight: bold\">)</span> to store project data.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0mcreating \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/\u001b[0m\u001b[95mdata\u001b[0m\u001b[1m)\u001b[0m to store project data.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......folder named (./folderOnColab/data) \u001b[1malready exists\u001b[0;0m, we won't try to create a new folder.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Exiting __main__ create_storage_locations\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Exiting __main__ create_storage_locations\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...marshaling data files:\n",
      "......processing 20250308_gfs.t00z.atmf000.nc from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1_xarray.h5 from ./folderOnColab/data\n",
      "......processing 20250314_gfs.t00z.atmf000.nc from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1_dask.parquet from ./folderOnColab/data\n",
      "......processing 20250312_gfs.t00z.atmf000.nc from ./folderOnColab/data\n",
      "......processing 20250317_gfs.t00z.atmf000.nc from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1.pt from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1.pkl from ./folderOnColab/data\n",
      "......processing 20250313_gfs.t00z.atmf000.nc from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1_xarray.pkl from ./folderOnColab/data\n",
      "......processing 20250309_gfs.t00z.atmf000.nc from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1_zarr.zarr from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1_numpy.npy.npz from ./folderOnColab/data\n",
      "......processing 20250310_gfs.t00z.atmf000.nc from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1_xarray.nc from ./folderOnColab/data\n",
      "......processing 20250315_gfs.t00z.atmf000.nc from ./folderOnColab/data\n",
      "......processing 20250316_gfs.t00z.atmf000.nc from ./folderOnColab/data\n",
      "......processing 20250311_gfs.t00z.atmf000.nc from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1.tf from ./folderOnColab/data\n",
      "......processing MLDATAREADY-0-0-1_dask.hdf from ./folderOnColab/data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ read_netcdfs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ read_netcdfs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span>reading NetCDF4 from list of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> files:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0mreading NetCDF4 from list of \u001b[1;36m10\u001b[0m files:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">......</span>reading NetCDF4 <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">20250308_gfs.t00z.atmf000.nc</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0mreading NetCDF4 \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/data/\u001b[0m\u001b[95m20250308_gfs.t00z.atmf000.nc\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">......</span>reading NetCDF4 <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">20250309_gfs.t00z.atmf000.nc</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0mreading NetCDF4 \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/data/\u001b[0m\u001b[95m20250309_gfs.t00z.atmf000.nc\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">......</span>reading NetCDF4 <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">20250310_gfs.t00z.atmf000.nc</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0mreading NetCDF4 \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/data/\u001b[0m\u001b[95m20250310_gfs.t00z.atmf000.nc\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">......</span>reading NetCDF4 <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">20250311_gfs.t00z.atmf000.nc</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0mreading NetCDF4 \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/data/\u001b[0m\u001b[95m20250311_gfs.t00z.atmf000.nc\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">......</span>reading NetCDF4 <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">20250312_gfs.t00z.atmf000.nc</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0mreading NetCDF4 \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/data/\u001b[0m\u001b[95m20250312_gfs.t00z.atmf000.nc\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">......</span>reading NetCDF4 <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">20250313_gfs.t00z.atmf000.nc</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0mreading NetCDF4 \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/data/\u001b[0m\u001b[95m20250313_gfs.t00z.atmf000.nc\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">......</span>reading NetCDF4 <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">20250314_gfs.t00z.atmf000.nc</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0mreading NetCDF4 \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/data/\u001b[0m\u001b[95m20250314_gfs.t00z.atmf000.nc\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">......</span>reading NetCDF4 <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">20250315_gfs.t00z.atmf000.nc</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0mreading NetCDF4 \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/data/\u001b[0m\u001b[95m20250315_gfs.t00z.atmf000.nc\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">......</span>reading NetCDF4 <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">20250316_gfs.t00z.atmf000.nc</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0mreading NetCDF4 \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/data/\u001b[0m\u001b[95m20250316_gfs.t00z.atmf000.nc\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">......</span>reading NetCDF4 <span style=\"font-weight: bold\">(</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/folderOnColab/data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">20250317_gfs.t00z.atmf000.nc</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0mreading NetCDF4 \u001b[1m(\u001b[0m.\u001b[35m/folderOnColab/data/\u001b[0m\u001b[95m20250317_gfs.t00z.atmf000.nc\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ read_netcdfs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ read_netcdfs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ gather_variables\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ gather_variables\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...pulling data for and stacking it to the core data structure:\n",
      "......appending ./folderOnColab/data/20250308_gfs.t00z.atmf000.nc\n",
      "......appending ./folderOnColab/data/20250309_gfs.t00z.atmf000.nc\n",
      "......appending ./folderOnColab/data/20250310_gfs.t00z.atmf000.nc\n",
      "......appending ./folderOnColab/data/20250311_gfs.t00z.atmf000.nc\n",
      "......appending ./folderOnColab/data/20250312_gfs.t00z.atmf000.nc\n",
      "......appending ./folderOnColab/data/20250313_gfs.t00z.atmf000.nc\n",
      "......appending ./folderOnColab/data/20250314_gfs.t00z.atmf000.nc\n",
      "......appending ./folderOnColab/data/20250315_gfs.t00z.atmf000.nc\n",
      "......appending ./folderOnColab/data/20250316_gfs.t00z.atmf000.nc\n",
      "......appending ./folderOnColab/data/20250317_gfs.t00z.atmf000.nc\n",
      "...\u001b[1mlat        data type:         \u001b[0;0m<class 'numpy.ndarray'>\n",
      ".......shape:(47185920,)\n",
      "....datatype:float32\n",
      "...\u001b[1mlon        data type:         \u001b[0;0m<class 'numpy.ndarray'>\n",
      ".......shape:(47185920,)\n",
      "....datatype:float32\n",
      "...\u001b[1mData type           (cld_amt             )\u001b[0;0m<class 'numpy.ndarray'>\n",
      ".......shape:(47185920,)\n",
      "....datatype:float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Exiting __main__ gather_variables\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Exiting __main__ gather_variables\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_numpy\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_numpy\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Numpy Write Execution time: 0.5537 seconds\n",
      "...Numpy Write Execution time: 1.0238 seconds\n",
      "...Numpy Write Execution time: 1.9193 seconds\n",
      "...Numpy Write Execution time: 5.8136 seconds\n",
      "...Numpy Write Execution time: 0.8341 seconds\n",
      "...Numpy Write Execution time: 0.8073 seconds\n",
      "...Numpy Write Execution time: 0.7207 seconds\n",
      "...Numpy Write Execution time: 0.7694 seconds\n",
      "...Numpy Write Execution time: 0.8100 seconds\n",
      "...Numpy Write Execution time: 0.8515 seconds\n",
      "...Numpy Write Execution time: 1.7578 seconds\n",
      "...Numpy Write Execution time: 4.6824 seconds\n",
      "...Numpy Write Execution time: 0.9371 seconds\n",
      "...Numpy Write Execution time: 0.8166 seconds\n",
      "...Numpy Write Execution time: 0.8827 seconds\n",
      "...Numpy Write Execution time: 0.8122 seconds\n",
      "...Numpy Write Execution time: 0.8194 seconds\n",
      "...Numpy Write Execution time: 1.3460 seconds\n",
      "...Numpy Write Execution time: 4.5388 seconds\n",
      "...Numpy Write Execution time: 0.7281 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.5729 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.8125 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 1.4994 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 5.6902 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 1.7652 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.8771 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.8006 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.8089 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.8499 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 1.1472 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 1.9381 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 2.2622 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.8144 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.9033 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.9593 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 1.8292 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 4.4184 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.9007 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.8169 seconds\n",
      "...Numpy pd.DataFrame Write Execution time: 0.8117 seconds\n",
      "...Numpy Read Execution time: 0.0116 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Exiting __main__ process_numpy\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Exiting __main__ process_numpy\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_pandas\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_pandas\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Resolution has: Index(['latitude', 'longitude', 'cld_amt'], dtype='object')\n",
      "\n",
      "\n",
      "size                 :     141,557,760 \n",
      "shape                : (47185920, 3)   \n",
      "ndim                 :               2 \n",
      "column size          :               3 \n",
      "Read                 :               3 \n",
      "Expected             :               3 \n",
      "\u001b[1mExpectations met\u001b[0;0m.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataframe memory use:          566,231,172\n",
      "...Pandas Write Execution time: 0.2161 seconds\n",
      "...Pandas Write Execution time: 0.2092 seconds\n",
      "...Pandas Write Execution time: 0.2099 seconds\n",
      "...Pandas Write Execution time: 0.2091 seconds\n",
      "...Pandas Write Execution time: 0.2087 seconds\n",
      "...Pandas Write Execution time: 0.2093 seconds\n",
      "...Pandas Write Execution time: 0.2118 seconds\n",
      "...Pandas Write Execution time: 0.3750 seconds\n",
      "...Pandas Write Execution time: 0.2109 seconds\n",
      "...Pandas Write Execution time: 0.2076 seconds\n",
      "...Pandas Write Execution time: 0.2115 seconds\n",
      "...Pandas Write Execution time: 0.2075 seconds\n",
      "...Pandas Write Execution time: 0.2167 seconds\n",
      "...Pandas Write Execution time: 0.2076 seconds\n",
      "...Pandas Write Execution time: 0.2111 seconds\n",
      "...Pandas Write Execution time: 0.2952 seconds\n",
      "...Pandas Write Execution time: 0.2132 seconds\n",
      "...Pandas Write Execution time: 0.2146 seconds\n",
      "...Pandas Write Execution time: 0.2190 seconds\n",
      "...Pandas Write Execution time: 0.2069 seconds\n",
      "...Pandas Read Execution time: 0.2576 seconds\n",
      "...Pandas Read Execution time: 0.1001 seconds\n",
      "...Pandas Read Execution time: 0.0880 seconds\n",
      "...Pandas Read Execution time: 0.0931 seconds\n",
      "...Pandas Read Execution time: 0.0950 seconds\n",
      "...Pandas Read Execution time: 0.0858 seconds\n",
      "...Pandas Read Execution time: 0.1819 seconds\n",
      "...Pandas Read Execution time: 0.0866 seconds\n",
      "...Pandas Read Execution time: 0.0867 seconds\n",
      "...Pandas Read Execution time: 0.0896 seconds\n",
      "...Pandas Read Execution time: 0.0870 seconds\n",
      "...Pandas Read Execution time: 0.0861 seconds\n",
      "...Pandas Read Execution time: 0.0888 seconds\n",
      "...Pandas Read Execution time: 0.0879 seconds\n",
      "...Pandas Read Execution time: 0.0953 seconds\n",
      "...Pandas Read Execution time: 0.0873 seconds\n",
      "...Pandas Read Execution time: 0.0865 seconds\n",
      "...Pandas Read Execution time: 0.0861 seconds\n",
      "...Pandas Read Execution time: 0.0879 seconds\n",
      "...Pandas Read Execution time: 0.0857 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_pandas\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_pandas\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_pytorch\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_pytorch\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...PyTorch Write Execution time: 0.3391 seconds\n",
      "...PyTorch Write Execution time: 0.3466 seconds\n",
      "...PyTorch Write Execution time: 0.3339 seconds\n",
      "...PyTorch Write Execution time: 0.3299 seconds\n",
      "...PyTorch Write Execution time: 0.3279 seconds\n",
      "...PyTorch Write Execution time: 0.3296 seconds\n",
      "...PyTorch Write Execution time: 0.3317 seconds\n",
      "...PyTorch Write Execution time: 0.4954 seconds\n",
      "...PyTorch Write Execution time: 0.3320 seconds\n",
      "...PyTorch Write Execution time: 0.3303 seconds\n",
      "...PyTorch Write Execution time: 0.3266 seconds\n",
      "...PyTorch Write Execution time: 0.3652 seconds\n",
      "...PyTorch Write Execution time: 0.3275 seconds\n",
      "...PyTorch Write Execution time: 0.5129 seconds\n",
      "...PyTorch Write Execution time: 0.3292 seconds\n",
      "...PyTorch Write Execution time: 0.3272 seconds\n",
      "...PyTorch Write Execution time: 0.3372 seconds\n",
      "...PyTorch Write Execution time: 0.3327 seconds\n",
      "...PyTorch Write Execution time: 0.4301 seconds\n",
      "...PyTorch Write Execution time: 0.3298 seconds\n",
      "...PyTorch Read Execution time: 0.2087 seconds\n",
      "...PyTorch Read Execution time: 0.0995 seconds\n",
      "...PyTorch Read Execution time: 0.0878 seconds\n",
      "...PyTorch Read Execution time: 0.0923 seconds\n",
      "...PyTorch Read Execution time: 0.0994 seconds\n",
      "...PyTorch Read Execution time: 0.1544 seconds\n",
      "...PyTorch Read Execution time: 0.0976 seconds\n",
      "...PyTorch Read Execution time: 0.0856 seconds\n",
      "...PyTorch Read Execution time: 0.0882 seconds\n",
      "...PyTorch Read Execution time: 0.0957 seconds\n",
      "...PyTorch Read Execution time: 0.0858 seconds\n",
      "...PyTorch Read Execution time: 0.0923 seconds\n",
      "...PyTorch Read Execution time: 0.0902 seconds\n",
      "...PyTorch Read Execution time: 0.0920 seconds\n",
      "...PyTorch Read Execution time: 0.0973 seconds\n",
      "...PyTorch Read Execution time: 0.0868 seconds\n",
      "...PyTorch Read Execution time: 0.2731 seconds\n",
      "...PyTorch Read Execution time: 0.0879 seconds\n",
      "...PyTorch Read Execution time: 0.0863 seconds\n",
      "...PyTorch Read Execution time: 0.0902 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_tensorflow\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_tensorflow\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...TensorFlow Write Execution time: 0.3271 seconds\n",
      "...TensorFlow Write Execution time: 0.3113 seconds\n",
      "...TensorFlow Write Execution time: 0.3206 seconds\n",
      "...TensorFlow Write Execution time: 0.3012 seconds\n",
      "...TensorFlow Write Execution time: 0.4001 seconds\n",
      "...TensorFlow Write Execution time: 0.2968 seconds\n",
      "...TensorFlow Write Execution time: 0.3015 seconds\n",
      "...TensorFlow Write Execution time: 0.3011 seconds\n",
      "...TensorFlow Write Execution time: 0.2996 seconds\n",
      "...TensorFlow Write Execution time: 0.3001 seconds\n",
      "...TensorFlow Write Execution time: 0.6827 seconds\n",
      "...TensorFlow Write Execution time: 0.3028 seconds\n",
      "...TensorFlow Write Execution time: 0.3033 seconds\n",
      "...TensorFlow Write Execution time: 0.3086 seconds\n",
      "...TensorFlow Write Execution time: 0.3018 seconds\n",
      "...TensorFlow Write Execution time: 0.3603 seconds\n",
      "...TensorFlow Write Execution time: 0.3030 seconds\n",
      "...TensorFlow Write Execution time: 0.3021 seconds\n",
      "...TensorFlow Write Execution time: 0.3087 seconds\n",
      "...TensorFlow Write Execution time: 0.3061 seconds\n",
      "...TensorFlow Read Execution time: 0.3330 seconds\n",
      "...TensorFlow Read Execution time: 0.0558 seconds\n",
      "...TensorFlow Read Execution time: 0.0549 seconds\n",
      "...TensorFlow Read Execution time: 0.0560 seconds\n",
      "...TensorFlow Read Execution time: 0.0577 seconds\n",
      "...TensorFlow Read Execution time: 0.0568 seconds\n",
      "...TensorFlow Read Execution time: 0.0626 seconds\n",
      "...TensorFlow Read Execution time: 0.0631 seconds\n",
      "...TensorFlow Read Execution time: 0.0589 seconds\n",
      "...TensorFlow Read Execution time: 0.0583 seconds\n",
      "...TensorFlow Read Execution time: 0.0591 seconds\n",
      "...TensorFlow Read Execution time: 0.0565 seconds\n",
      "...TensorFlow Read Execution time: 0.0664 seconds\n",
      "...TensorFlow Read Execution time: 0.0576 seconds\n",
      "...TensorFlow Read Execution time: 0.0570 seconds\n",
      "...TensorFlow Read Execution time: 0.0558 seconds\n",
      "...TensorFlow Read Execution time: 0.0588 seconds\n",
      "...TensorFlow Read Execution time: 0.0565 seconds\n",
      "...TensorFlow Read Execution time: 0.0556 seconds\n",
      "...TensorFlow Read Execution time: 0.0564 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Exiting __main__ process_tensorflow\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Exiting __main__ process_tensorflow\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_zarr\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_zarr\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.5513 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.5038 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.5174 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.4799 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.4818 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.5218 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.4956 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.4903 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.4740 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.4669 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.4852 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.5048 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.5101 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.5062 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.5890 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.5063 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.4865 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.5011 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.5082 seconds\n",
      "\u001b[1mZarr Node Tree:\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Zarr Write Execution time: 1.4929 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a6960>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "...Zarr Read Execution time: 0.0502 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a6960>\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "...Zarr Read Execution time: 0.0510 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a6960>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0515 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a7e60>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0569 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a7e60>\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "...Zarr Read Execution time: 0.0498 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a7e60>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0504 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a7e60>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0502 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a7e60>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0506 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a6960>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0501 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a6960>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0494 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a6960>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0501 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a6960>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0508 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a6960>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0529 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a7e60>\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0522 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a7e60>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0491 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a7e60>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0510 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a6960>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "...Zarr Read Execution time: 0.0547 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a7e60>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0513 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a7e60>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0496 seconds\n",
      "Name        : \n",
      "Type        : Group\n",
      "Zarr format : 3\n",
      "Read-only   : True\n",
      "Store type  : LocalStore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">cld_amt</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">cld_amt</span> (47185920,) float32\n",
       "â”œâ”€â”€ <span style=\"font-weight: bold\">latitude</span>\n",
       "â”‚   â””â”€â”€ <span style=\"font-weight: bold\">latitude</span> (47185920,) float32\n",
       "â””â”€â”€ <span style=\"font-weight: bold\">longitude</span>\n",
       "    â””â”€â”€ <span style=\"font-weight: bold\">longitude</span> (47185920,) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1mcld_amt\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mcld_amt\u001b[0m (47185920,) float32\n",
       "â”œâ”€â”€ \u001b[1mlatitude\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[1mlatitude\u001b[0m (47185920,) float32\n",
       "â””â”€â”€ \u001b[1mlongitude\u001b[0m\n",
       "    â””â”€â”€ \u001b[1mlongitude\u001b[0m (47185920,) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<generator object Group.groups at 0x7f6f271a7e60>\n",
      "('latitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/latitude>)\n",
      "('cld_amt', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/cld_amt>)\n",
      "('longitude', <Group file://folderOnColab/data/MLDATAREADY-0-0-1_zarr.zarr/longitude>)\n",
      "...Zarr Read Execution time: 0.0519 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Exiting __main__ process_zarr\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Exiting __main__ process_zarr\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_dask\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_dask\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Dask Write HDF Execution time: 15.5905 seconds\n",
      "...Dask Write HDF Execution time: 14.2255 seconds\n",
      "...Dask Write HDF Execution time: 14.3129 seconds\n",
      "...Dask Write HDF Execution time: 14.2411 seconds\n",
      "...Dask Write HDF Execution time: 14.3099 seconds\n",
      "...Dask Write HDF Execution time: 14.2658 seconds\n",
      "...Dask Write HDF Execution time: 14.4153 seconds\n",
      "...Dask Write HDF Execution time: 14.2926 seconds\n",
      "...Dask Write HDF Execution time: 14.2479 seconds\n",
      "...Dask Write HDF Execution time: 14.2793 seconds\n",
      "...Dask Write HDF Execution time: 14.3304 seconds\n",
      "...Dask Write HDF Execution time: 14.4141 seconds\n",
      "...Dask Write HDF Execution time: 14.2742 seconds\n",
      "...Dask Write HDF Execution time: 14.3054 seconds\n",
      "...Dask Write HDF Execution time: 14.2685 seconds\n",
      "...Dask Write HDF Execution time: 14.4239 seconds\n",
      "...Dask Write HDF Execution time: 14.2462 seconds\n",
      "...Dask Write HDF Execution time: 14.2809 seconds\n",
      "...Dask Write HDF Execution time: 14.2450 seconds\n",
      "...Dask Write HDF Execution time: 14.2981 seconds\n",
      "...Dask Read parquet Execution time: 0.0503 seconds\n",
      "...Dask Read parquet Execution time: 0.0510 seconds\n",
      "...Dask Read parquet Execution time: 0.0513 seconds\n",
      "...Dask Read parquet Execution time: 0.0506 seconds\n",
      "...Dask Read parquet Execution time: 0.0511 seconds\n",
      "...Dask Read parquet Execution time: 0.0522 seconds\n",
      "...Dask Read parquet Execution time: 0.0530 seconds\n",
      "...Dask Read parquet Execution time: 0.0603 seconds\n",
      "...Dask Read parquet Execution time: 0.0525 seconds\n",
      "...Dask Read parquet Execution time: 0.0506 seconds\n",
      "...Dask Read parquet Execution time: 0.0509 seconds\n",
      "...Dask Read parquet Execution time: 0.0489 seconds\n",
      "...Dask Read parquet Execution time: 0.0497 seconds\n",
      "...Dask Read parquet Execution time: 0.0476 seconds\n",
      "...Dask Read parquet Execution time: 0.0476 seconds\n",
      "...Dask Read parquet Execution time: 0.0476 seconds\n",
      "...Dask Read parquet Execution time: 0.0519 seconds\n",
      "...Dask Read parquet Execution time: 0.0487 seconds\n",
      "...Dask Read parquet Execution time: 0.0476 seconds\n",
      "...Dask Read parquet Execution time: 0.0475 seconds\n",
      "...Dask Write parquet Execution time: 3.9104 seconds\n",
      "...Dask Write parquet Execution time: 3.6860 seconds\n",
      "...Dask Write parquet Execution time: 3.8046 seconds\n",
      "...Dask Write parquet Execution time: 3.6900 seconds\n",
      "...Dask Write parquet Execution time: 3.7086 seconds\n",
      "...Dask Write parquet Execution time: 4.1660 seconds\n",
      "...Dask Write parquet Execution time: 5.0711 seconds\n",
      "...Dask Write parquet Execution time: 3.7438 seconds\n",
      "...Dask Write parquet Execution time: 3.6939 seconds\n",
      "...Dask Write parquet Execution time: 3.6836 seconds\n",
      "...Dask Write parquet Execution time: 3.6647 seconds\n",
      "...Dask Write parquet Execution time: 3.6778 seconds\n",
      "...Dask Write parquet Execution time: 3.6677 seconds\n",
      "...Dask Write parquet Execution time: 3.6829 seconds\n",
      "...Dask Write parquet Execution time: 3.7276 seconds\n",
      "...Dask Write parquet Execution time: 3.6661 seconds\n",
      "...Dask Write parquet Execution time: 3.8825 seconds\n",
      "...Dask Write parquet Execution time: 3.7095 seconds\n",
      "...Dask Write parquet Execution time: 3.6702 seconds\n",
      "...Dask Write parquet Execution time: 3.6491 seconds\n",
      "...Dask Read parquet Execution time: 0.0563 seconds\n",
      "...Dask Read parquet Execution time: 0.0248 seconds\n",
      "...Dask Read parquet Execution time: 0.0245 seconds\n",
      "...Dask Read parquet Execution time: 0.0238 seconds\n",
      "...Dask Read parquet Execution time: 0.0238 seconds\n",
      "...Dask Read parquet Execution time: 0.0239 seconds\n",
      "...Dask Read parquet Execution time: 0.0246 seconds\n",
      "...Dask Read parquet Execution time: 0.0279 seconds\n",
      "...Dask Read parquet Execution time: 0.0300 seconds\n",
      "...Dask Read parquet Execution time: 0.0263 seconds\n",
      "...Dask Read parquet Execution time: 0.0261 seconds\n",
      "...Dask Read parquet Execution time: 0.0259 seconds\n",
      "...Dask Read parquet Execution time: 0.0261 seconds\n",
      "...Dask Read parquet Execution time: 0.0270 seconds\n",
      "...Dask Read parquet Execution time: 0.0291 seconds\n",
      "...Dask Read parquet Execution time: 0.0277 seconds\n",
      "...Dask Read parquet Execution time: 0.0286 seconds\n",
      "...Dask Read parquet Execution time: 0.0290 seconds\n",
      "...Dask Read parquet Execution time: 0.0280 seconds\n",
      "...Dask Read parquet Execution time: 0.0276 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_dask\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_dask\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_parquet\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_parquet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Parquet Write Execution time: 1.9485 seconds\n",
      "...Parquet Write Execution time: 1.9756 seconds\n",
      "...Parquet Write Execution time: 1.9113 seconds\n",
      "...Parquet Write Execution time: 1.9761 seconds\n",
      "...Parquet Write Execution time: 1.9108 seconds\n",
      "...Parquet Write Execution time: 1.9239 seconds\n",
      "...Parquet Write Execution time: 1.9275 seconds\n",
      "...Parquet Write Execution time: 1.8977 seconds\n",
      "...Parquet Write Execution time: 1.9220 seconds\n",
      "...Parquet Write Execution time: 1.9384 seconds\n",
      "...Parquet Write Execution time: 2.0253 seconds\n",
      "...Parquet Write Execution time: 1.8946 seconds\n",
      "...Parquet Write Execution time: 1.9293 seconds\n",
      "...Parquet Write Execution time: 1.9330 seconds\n",
      "...Parquet Write Execution time: 1.9035 seconds\n",
      "...Parquet Write Execution time: 1.9243 seconds\n",
      "...Parquet Write Execution time: 1.9512 seconds\n",
      "...Parquet Write Execution time: 1.9084 seconds\n",
      "...Parquet Write Execution time: 1.9404 seconds\n",
      "...Parquet Write Execution time: 1.9412 seconds\n",
      "...Parquet Read Execution time: 0.4240 seconds\n",
      "...Parquet Read Execution time: 0.2438 seconds\n",
      "...Parquet Read Execution time: 0.2499 seconds\n",
      "...Parquet Read Execution time: 0.2795 seconds\n",
      "...Parquet Read Execution time: 0.2426 seconds\n",
      "...Parquet Read Execution time: 0.2488 seconds\n",
      "...Parquet Read Execution time: 0.2748 seconds\n",
      "...Parquet Read Execution time: 0.2728 seconds\n",
      "...Parquet Read Execution time: 0.2881 seconds\n",
      "...Parquet Read Execution time: 0.2269 seconds\n",
      "...Parquet Read Execution time: 0.2482 seconds\n",
      "...Parquet Read Execution time: 0.2335 seconds\n",
      "...Parquet Read Execution time: 0.2238 seconds\n",
      "...Parquet Read Execution time: 0.2327 seconds\n",
      "...Parquet Read Execution time: 0.2373 seconds\n",
      "...Parquet Read Execution time: 0.2539 seconds\n",
      "...Parquet Read Execution time: 0.2264 seconds\n",
      "...Parquet Read Execution time: 0.2516 seconds\n",
      "...Parquet Read Execution time: 0.2290 seconds\n",
      "...Parquet Read Execution time: 0.2466 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entering __main__ process_parquet\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Entering __main__ process_parquet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Exiting __main__ process\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Exiting __main__ process\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END PROGRAM\n",
      "Elapsed time: 612.2456484419527\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # note that this design now deviates from previous methods.\n",
    "    # Implementation will assume a single execution of a single PIID folder, scanning results and\n",
    "    # appending metrics to a single ASCII file as the code proceeds thus ensuring multi-processor, *nix driven execution.\n",
    "\n",
    "    start_t = perf_counter()\n",
    "    print(\"BEGIN PROGRAM\")\n",
    "\n",
    "    ############################################\n",
    "    # CONSTANTS\n",
    "    ############################################\n",
    "\n",
    "    # Semantic Versioning\n",
    "    VERSION_NAME = \"MLDATAREADY\"\n",
    "    VERSION_MAJOR = 0\n",
    "    VERSION_MINOR = 0\n",
    "    VERSION_RELEASE = 1\n",
    "\n",
    "    DATA_VERSION_RELEASE = \"-\".join(\n",
    "        [\n",
    "            str(VERSION_NAME),\n",
    "            str(VERSION_MAJOR),\n",
    "            str(VERSION_MINOR),\n",
    "            str(VERSION_RELEASE),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # OUTPUT EXTENSIONS\n",
    "    OUTPUT_PANDAS_EXT = \"pkl\"\n",
    "    OUTPUT_NUMPY_EXT = \"npy\"\n",
    "    OUTPUT_TORCH_EXT = \"pt\"\n",
    "    OUTPUT_XARRAY_EXT = \"nc\"\n",
    "    OUTPUT_ZARR_EXT = \"zarr\"\n",
    "    OUTPUT_PARQUET_EXT = \"parquet\"\n",
    "    OUTPUT_TENSORFLOW_EXT = \"tf\"\n",
    "    OUTPUT_PYSTAC_EXT = \"psc\"\n",
    "    OUTPUT_DASK_EXT = \"dask\"\n",
    "    # location of our working files\n",
    "    # WORKING_FOLDER=\"/content/folderOnColab\"\n",
    "    WORKING_FOLDER = \"./folderOnColab\"\n",
    "    input_directory = \"./folderOnColab\"\n",
    "    output_directory = \"./folderOnColab\"\n",
    "\n",
    "    # Notebook Author details\n",
    "    AUTHOR_NAME = \"Christopher G Wood\"\n",
    "    GITHUB_USERNAME = \"christophergarthwood\"\n",
    "    AUTHOR_EMAIL = \"christopher.g.wood@gmail.com\"\n",
    "\n",
    "    # GEOSPATIAL NAMES\n",
    "    LAT_LNAME = \"latitude\"\n",
    "    LAT_SNAME = \"lat\"\n",
    "    LONG_LNAME = \"longitude\"\n",
    "    LONG_SNAME = \"lon\"\n",
    "    #PRODUCT_LNAME = \"chlor_a\"\n",
    "    #PRODUCT_SNAME = \"chlor_a\"\n",
    "    PRODUCT_LNAME = \"cld_amt\"\n",
    "    PRODUCT_SNAME = \"cld_amt\"\n",
    "\n",
    "    # PRODUCT_LNAME=\"salinity\"\n",
    "    # PRODUCT_SNAME=\"salinity\"\n",
    "\n",
    "    # Encoding\n",
    "    ENCODING = \"utf-8\"\n",
    "    os.environ[\"PYTHONIOENCODING\"] = ENCODING\n",
    "\n",
    "    BOLD_START = \"\\033[1m\"\n",
    "    BOLD_END = \"\\033[0;0m\"\n",
    "    TEXT_WIDTH = 77\n",
    "    AI_NUMPY_DATA_TYPE  = np.float32\n",
    "    AI_PANDAS_DATA_TYPE = \"float32\"\n",
    "    AI_TORCH_DATA_TYPE = torch.float32\n",
    "    AI_XARRAY_REMOVE_VARIABLES= [\"clwmr\", \"delz\", \"dpres\", \"dzdt\", \"grle\", \"hgtsfc\", \"icmr\", \"o3mr\", \"pressfc\", \"rwmr\", \"snmr\", \"spfh\", \"tmp\", \"ugrd\", \"vgrd\",]\n",
    "    # You can also adjust the verbosity by changing the value of TF_CPP_MIN_LOG_LEVEL:\n",
    "    #\n",
    "    # 0 = all messages are logged (default behavior)\n",
    "    # 1 = INFO messages are not printed\n",
    "    # 2 = INFO and WARNING messages are not printed\n",
    "    # 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "    TF_CPP_MIN_LOG_LEVEL_SETTING = 0\n",
    "\n",
    "    # Set the Seed for the experiment (ask me why?)\n",
    "    # seed the pseudorandom number generator\n",
    "    # THIS IS ESSENTIAL FOR CONSISTENT MODEL OUTPUT, remember these are random in nature.\n",
    "    # SEED_INIT = 7\n",
    "    # random.seed(SEED_INIT)\n",
    "    # tf.random.set_seed(SEED_INIT)\n",
    "    # np.random.seed(SEED_INIT)\n",
    "\n",
    "    DEBUG_STACKTRACE = 0\n",
    "    DEBUG_USING_GPU = 0   #no gpu utilization on 0, 1 is gpu utilization\n",
    "    NUM_PROCESSORS = 10\n",
    "    ITERATIONS = 20\n",
    "\n",
    "    # make comparisons lower case and include wild card character at the end of each to catch anomalous file extensions like xlsx, etc.\n",
    "    EXTENSIONS = [\".nc\"]\n",
    "    LOWER_EXTENSIONS = [x.lower() for x in EXTENSIONS]\n",
    "\n",
    "    THE_DEVICE_NAME = \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
    "    if DEBUG_USING_GPU == 1:\n",
    "        THE_DEVICE_NAME = \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    # GPU Setup (for multiple GPU devices)\n",
    "    device = torch.cuda.current_device()\n",
    "\n",
    "    # softare watermark\n",
    "    lib_diagnostics()\n",
    "\n",
    "    # hardware specs\n",
    "    get_hardware_stats()\n",
    "\n",
    "    # download the data\n",
    "    #download_noaa()\n",
    "    #download_test()\n",
    "    \n",
    "    # - Core workhorse routine\n",
    "    process(input_directory, \"2025\")\n",
    "\n",
    "    # - Save the results\n",
    "    # save_output()\n",
    "\n",
    "    end_t = perf_counter()\n",
    "    print(\"END PROGRAM\")\n",
    "    print(f\"Elapsed time: {end_t - start_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "STEM-004_ComputerVision.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "033cb5a8c21c4ca5a13b5d03e8b78fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebb9ca01cf52473699f21f15c5ec7d34",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_689fe090492947608518efc9f3bd6d5d",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n          "
     }
    },
    "0d8988d435644a928829ef71435a3e83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1015b1ce380943d48c7a6386444c768a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f5405303114146a28663cb1c46db7beb",
       "IPY_MODEL_bdc74a6d20a041819b6fc6d1c3f85a25",
       "IPY_MODEL_033cb5a8c21c4ca5a13b5d03e8b78fa0"
      ],
      "layout": "IPY_MODEL_2b447186de7a4571821ca59295744ce3"
     }
    },
    "202c8798fdda4778bf09a2124c37a6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "usfs-ai-bootcamp",
       "usfa-ai-advanced-training",
       "I will setup my own"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Set Your Project:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_dd3de71c792f4762af3b280ac9b7f5e6",
      "style": "IPY_MODEL_fc3fc48bdebe4501998819fbda36152c"
     }
    },
    "2b447186de7a4571821ca59295744ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "31b012d7d41a4d06933aa38e8e3bd74a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "479233081fbc44c8afe9a044ebb95faf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "689fe090492947608518efc9f3bd6d5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad3b9f4855d541e49303c03fb9f07db7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Accept",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_f196621910824a00a97d06e5433cb294",
      "style": "IPY_MODEL_31b012d7d41a4d06933aa38e8e3bd74a",
      "tooltip": ""
     }
    },
    "bdc74a6d20a041819b6fc6d1c3f85a25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_202c8798fdda4778bf09a2124c37a6c7",
       "IPY_MODEL_ad3b9f4855d541e49303c03fb9f07db7"
      ],
      "layout": "IPY_MODEL_0d8988d435644a928829ef71435a3e83"
     }
    },
    "cf363ab273bf4eeba7fbea4d79ed3a64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd3de71c792f4762af3b280ac9b7f5e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebb9ca01cf52473699f21f15c5ec7d34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f196621910824a00a97d06e5433cb294": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5405303114146a28663cb1c46db7beb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf363ab273bf4eeba7fbea4d79ed3a64",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_479233081fbc44c8afe9a044ebb95faf",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n\n        <table><tr><td>\n            <span style=\"font-family: Tahoma;font-size: 18\">\n              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n              Please verify that you are in the appropriate project and that the:</br>\n              <center><code><b>PROJECT_ID</b></code> </br></center>\n              aligns with the Project Id in the upper left corner of this browser and that the location:\n              <center><code><b>LOCATION</b></code> </br></center>\n              aligns with the instructions provided.\n            </span>\n          </td></tr></table></br></br>\n\n    "
     }
    },
    "fc3fc48bdebe4501998819fbda36152c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
