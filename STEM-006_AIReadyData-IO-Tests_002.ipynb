{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "Jg3iJooMQjWA",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Artificial Intelligence\n",
    "## AI Ready Data - 006\n",
    "### Download, curate, and process weather and tree data.\n",
    "\n",
    "<center>\n",
    "<table align=\"center\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/christophergarthwood/jbooks/blob/main/STEM-006_AIReadyData-IO-Tests_002.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/notebooks?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Link to Colab Enterprise\n",
    "    </a>\n",
    "  </td>   \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/christophergarthwood/jbooks/blob/main/STEM-006_AIReadyData-IO-Tests_002.ipynb\">\n",
    "      <img width=32 src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Link to Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "</center>\n",
    "</br></br></br>\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Christopher G Wood](https://github.com/christophergarthwood)  |\n",
    "\n",
    "# Overview\n",
    "\n",
    "Using various data sources we will download, review, and package data in various formats exploring the options for \"AI Ready\" data and what that means.\n",
    "\n",
    "## What is a \"**AI Ready**\" Data?\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "### Python Tools in General (credit to: https://www.linkedin.com/in/milan-janosov/)\n",
    "\n",
    "#### 𝐃𝐚𝐭𝐚 𝐌𝐚𝐧𝐢𝐩𝐮𝐥𝐚𝐭𝐢𝐨𝐧 \n",
    "- Polars: https://pola.rs \n",
    "- Modin: https://lnkd.in/d97bYx79 \n",
    "- Pandas: https://pandas.pydata.org \n",
    "- Vaex: https://vaex.io \n",
    "- Datatable: https://lnkd.in/dApHaBCT \n",
    "- CuPy: https://docs.cupy.dev \n",
    "- NumPy: https://numpy.org \n",
    "\n",
    "#### 𝐃𝐚𝐭𝐚 𝐕𝐢𝐬𝐮𝐚𝐥𝐢𝐳𝐚𝐭𝐢𝐨𝐧 \n",
    "- Plotly: https://plotly.com/python \n",
    "- Altair: https://lnkd.in/d8pN88j9 \n",
    "- Matplotlib: https://matplotlib.org \n",
    "- Seaborn: https://seaborn.pydata.org \n",
    "- Geoplotlib: https://lnkd.in/d8wtm5CN \n",
    "- Folium: https://lnkd.in/d-yMZhSf \n",
    "- Bokeh: https://docs.bokeh.org \n",
    "- Pygal: http://www.pygal.org \n",
    "\n",
    "#### 𝐒𝐭𝐚𝐭𝐢𝐬𝐭𝐢𝐜𝐚𝐥 𝐀𝐧𝐚𝐥𝐲𝐬𝐢𝐬 \n",
    "- SciPy: https://scipy.org \n",
    "- PyMC3: https://docs.pymc.io \n",
    "- PyStan: https://lnkd.in/dyDq23S6 \n",
    "- Statsmodels: https://lnkd.in/dTAJ-sv9 \n",
    "- Lifelines: https://lnkd.in/drgZ54cj \n",
    "- Pingouin: https://pingouin-stats.org \n",
    "\n",
    "#### 𝐌𝐚𝐜𝐡𝐢𝐧𝐞 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠 \n",
    "- JAX: https://jax.readthedocs.io \n",
    "- Keras: https://keras.io \n",
    "- Theano: https://lnkd.in/d8Ga6xvV \n",
    "- XGBoost: https://lnkd.in/d7856MDc \n",
    "- Scikit-learn: https://scikit-learn.org \n",
    "- TensorFlow: https://tensorflow.org \n",
    "- PyTorch: https://pytorch.org \n",
    "\n",
    "#### 𝐍𝐚𝐭𝐮𝐫𝐚𝐥 𝐋𝐚𝐧𝐠𝐮𝐚𝐠𝐞 𝐏𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠 \n",
    "- NLTK: https://www.nltk.org \n",
    "- BERT: https://lnkd.in/d3DsJsyD \n",
    "- spaCy: https://spacy.io \n",
    "- TextBlob: https://lnkd.in/dNSdHsjC \n",
    "- Polyglot: https://lnkd.in/dWbhNJrn \n",
    "- Gensim: https://lnkd.in/d4bRCJTC \n",
    "- Pattern: https://lnkd.in/dCbSXzs6 \n",
    "\n",
    "#### 𝐃𝐚𝐭𝐚𝐛𝐚𝐬𝐞 𝐎𝐩𝐞𝐫𝐚𝐭𝐢𝐨𝐧𝐬 \n",
    "- Dask: https://dask.org \n",
    "- PySpark: https://lnkd.in/dqrKCvvu \n",
    "- Ray: https://docs.ray.io \n",
    "- Koalas: https://lnkd.in/dUwXiSWr \n",
    "- Kafka: https://kafka.apache.org \n",
    "- Hadoop: https://hadoop.apache.org \n",
    "\n",
    "#### 𝐓𝐢𝐦𝐞 𝐒𝐞𝐫𝐢𝐞𝐬 𝐀𝐧𝐚𝐥𝐲𝐬𝐢𝐬 \n",
    "- Sktime: https://lnkd.in/dspnqaEH \n",
    "- Darts: https://lnkd.in/dD6kbSRw \n",
    "- AutoTS: https://lnkd.in/dpquYFRd \n",
    "- Prophet: https://lnkd.in/df8Xt5zG \n",
    "- Kats: https://lnkd.in/dPyQ8vpT \n",
    "- TSFresh: https://lnkd.in/dB8JDJF7 \n",
    "\n",
    "#### 𝐖𝐞𝐛 𝐒𝐜𝐫𝐚𝐩𝐢𝐧𝐠 \n",
    "- Beautiful Soup: https://lnkd.in/drxaifkC \n",
    "- Scrapy: https://scrapy.org \n",
    "- Octoparse: https://www.octoparse.com \n",
    "- Selenium: https://www.selenium.dev\n",
    "\n",
    "![Python - Life is Short](./img/PythonLifeIsShort.jpg)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Additional References\n",
    "\n",
    "#### Data Formats\n",
    "+ [List of ML File Formats](https://github.com/trailofbits/ml-file-formats)\n",
    "+ [ML Guide to Data Formats](https://www.hopsworks.ai/post/guide-to-file-formats-for-machine-learning)\n",
    "+ [Why are ML Data Structures Different?](https://stackoverflow.blog/2023/01/04/getting-your-data-in-shape-for-machine-learning/)\n",
    "\n",
    "#### FAIR\n",
    "+ [FAIR and AI-Ready](https://repository.niddk.nih.gov/public/NIDDKCR_Office_Hours_AI-Readiness_and_Preparing_AI-Ready_+Datasets_12_2023.pdf)\n",
    "+ [AI-Ready-Data](https://www.rishabhsoft.com/blog/ai-ready-data)\n",
    "+ [AI-Ready FAIR Data](https://medium.com/@sean_hill/ai-ready-fair-data-accelerating-science-through-responsible-ai-and-data-stewardship-3b4f21c804fd)\n",
    "+ [AI-Ready Data ... Quality](https://www.elucidata.io/blog/building-ai-ready-data-why-quality-matters-more-than-quantity)\n",
    "+ [AI-Ready Data Explained](https://acodis.io/hubfs/pdfs/AI-ready%20data%20Explained%20Whitepaper%20(1).pdf)\n",
    "\n",
    "+ [GCP with BigQuery DataFrames](https://cloud.google.com/blog/products/data-analytics/building-aiml-apps-in-python-with-bigquery-dataframes)\n",
    "\n",
    "#### Format Libraries / Standards\n",
    "+ [Earth Science Information partners (ESIP)](https://www.esipfed.org/checklist-ai-ready-data/)\n",
    "+ [Zarr - Storage of N-dimensional arrays (tensors)](https://zarr.dev/#description)\n",
    "  + [Zarr explained](https://aijobs.net/insights/zarr-explained/)\n",
    "+ [Apache Parquet](https://parquet.apache.org/)\n",
    "  + [All about Parquet](https://medium.com/data-engineering-with-dremio/all-about-parquet-part-01-an-introduction-b62a5bcf70f8)\n",
    "+ [PySTAC - SpatioTemporal Asset Catalogs](https://pystac.readthedocs.io/en/stable/)\n",
    "  + [John Hogland's Spatial Modeling Tutorials](https://github.com/jshogland/SpatialModelingTutorials/blob/main/README.md)\n",
    "\n",
    "#### Additional Analysis Tools\n",
    "+ [SHapley Additive exPlanations](https://shap.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types / Iterations Schema\n",
    "\n",
    "There are various schema appropriate for data when engaged in an AI/ML effort, however the following descriptions represent a minimal configuration of data iterations that should exist.\n",
    "\n",
    "1. Original / RAW data\n",
    "2. Cleansed / Scientifically Prepared data\n",
    "3. AI Ready Data\n",
    "\n",
    "### Original / RAW data\n",
    "\n",
    "This data is from the original source, has documentation and is unaltered sans a calibration applied which should be referenced in the documentation.  These values could potentially be used as-is but represent unclansed and unregulated data and should be used with care.  There will only ever be a single version of RAW data.\n",
    "\n",
    "### Cleansed / Scientifically Prepared data\n",
    "\n",
    "Cleansed data has calibration coefficients applied, is in standardized units in accordance with an established standard, is formatted and normalized in a consistent manner, has missing values fixed in accordance with the data standard, and is generally useful for review with a high degree of confidence that it is worthy of use and additional work towards solving a problem.  It is possible to have multiple versions of cleansed data, however it is recommended data governance specify a single version that is well documented regarding the modifications performed (to include the algorithms utilized in any transformations or unit conversions).\n",
    "\n",
    "### AI Ready Data\n",
    "\n",
    "Has a lineage of meta-data sufficient to identify it's origination and the changes applied up this this point.  Is versioned and could contain 1 to N versions of the data derived from the Cleansed / Scientifically Preparred data.  Explicit details regarding what \"AI Ready\" means and how it can be evaluated are cited below.\n",
    "\n",
    "\n",
    "All three forms of the aforementioned schema of data should exist thereby allowing an organization to roll-back, recalibrate or re-assess it's data holdings without losing any fidelity or quality control.\n",
    "\n",
    "***Why is AI data different from traditional means of data storage / capture?*** Performance matters a lot in Artificial Intelligence (AI), especially in Machine Learning.  Training data is inferred hundreds and thousands of times by a model during training and models can have a multitude of **Hyper-parameters** suitable for investigation. Machine learning applications are trained and used by (multiple) GPUs synchronizing data via high performance internal networks and pipes. All this requires an optimized data format that can handle different types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasons to Prioritize AI-Ready Data\n",
    "\n",
    "AI and Machine Learning models certainly perform better based on their ability to learn from data, enabling them to react, predict, and provide broadly applicable information. In addition, the overall effectiveness of these capabilities hinges on the quality of the data being used during the process & execution time of AI. Ensuring AI-ready data allows your enterprise to uncover gaps, erroneous information, and discrepancies, creating a solid foundation for reliable AI performance.\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "title: Key Reasons to Prioritize AI-Ready Data\n",
    "---\n",
    "flowchart LR\n",
    "    AA[Improves AI Model Accuracy];\n",
    "    ROI[Maximizes the Return on Investment];\n",
    "    SU[Speeds Up Prototyping and Deployment];\n",
    "    SA[Strategic Agility];\n",
    "\n",
    "    AA--->ROI;\n",
    "    ROI--->SU;\n",
    "    SU--->SA;\n",
    "```    \n",
    "\n",
    "## How to make your data AI-Ready\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "#title: How to make your data AI-Ready\n",
    "config:\n",
    "  look: handDrawn\n",
    "  theme: neutral\n",
    "  layout: dagre\n",
    "---\n",
    "flowchart LR\n",
    "    A(((Assess Your Current Data State)))\n",
    "    B(((Break Down Data Silos)))\n",
    "    P(((Cleanse and Prepare Your Data)))\n",
    "    M(((Make Unstructure Data Accessible)))\n",
    "    I(((Implement Data Governance Frameworks)))\n",
    "    E(((Ensure Data Literacy Across the Organization)))\n",
    "\n",
    "    A-->B;\n",
    "    B-->P;\n",
    "    P-->M;\n",
    "    M-->I;\n",
    "    I-->E;\n",
    "    E-->A;\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stages of Data Interactivity in an AI/ML engagement\n",
    "\n",
    "\n",
    "Data preparation is critical in any machine learning project because it directly influences your model's performance and accuracy.\n",
    "\n",
    "1. Data gathering\n",
    "2. Data wrangling\n",
    "3. Data utilization\n",
    "\n",
    "### Data Gathering\n",
    "\n",
    "You gather the data, preferably ALL OF IT, thought necessary to produce the answer to your question / requirements. \n",
    "\n",
    "Your data might be encapsulated within a Relational Database, be a directory of various file formats, or hand written log sheets.\n",
    "\n",
    "Gaining access to that data can be it's own challenge due to disparate storage mechanisms, rights and control schema, or even lack of a digital footprint.\n",
    "\n",
    "Identify dependent and independent variables, limit the use of independent variables to the smallest size first and pay close attention to the importance of each feature through analysis like SHAP and correlation analysis with dependent and independent variables.\n",
    "\n",
    "### Data Wrangling\n",
    "\n",
    "Data wrangling, also known as data munging, is the process of transforming raw data into a usable and organized format for analysis, visualization, or machine learning. It involves cleaning, structuring, and enriching data to ensure accuracy, completeness, and relevance for downstream tasks. \n",
    "\n",
    "+ Clean the data by removing or correcting missing values, outliers, or inconsistencies.\n",
    "+ Structure the data into a format that supports ease of use and readability.\n",
    "+ Enrich the data based on need such as resolution of imbalanced classifications.\n",
    "+ Transform the data by merging datasets, converting units, etc...\n",
    "+ Quality analysis ensures the data is accurate, reliable, and relevant for use.\n",
    "\n",
    "Then, you transform the data through processes like normalization and encoding to make it compatible with machine learning algorithms.\n",
    "\n",
    "\n",
    "### Data Utilization\n",
    "Finally, you reduce the data's complexity without losing the information it can provide to the machine learning model, often using techniques like dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is \"AI Ready\" Data?\n",
    "\n",
    "Data often needs significant reformatting for machine learning frameworks, slowing down research processes. Researchers need data that is already formatted and ready for machine learning applications to truly transform science. Additionally, the lack of semantic enrichment for machine-readable context limits the usability of data in AI applications. Semantic enrichment is crucial for making data truly AI-ready. Inefficient data preparation tools and processes remain a major bottleneck, and limited integration capabilities with various machine learning frameworks hinder the efficiency and scope of AI-driven research.\n",
    "\n",
    "There are two stages of data preparedness appropriate for consideration within an AI engagement.  The first stage is focused on the data pipeline and business considerations appropos to the business in question and is not readily consumed for model processing.  A data pipeline is a structured process for moving data from one or more source systems to one or more destination systems, often with transformations along the way. It ensures data flows efficiently and reliably, allowing organizations to break down information silos and leverage their data for various purposes, such as analytics, machine learning, or business intelligence. Typically data pipelines take on a real time or batch model of processing.  A data pipeline does not automatically prepare data for model integration; a data pipeline prepares data to be more readily avaialble across a spectrum of business lines.\n",
    "\n",
    "This first stage of data readiness should occur with a set periodicity and has very long downstream influence over all business operations.  Overall technical debt, interoperability and faster model integration is achieved by establishing data pipelines.\n",
    "\n",
    "The second stage of data for an AI engagement is \"AI Ready\" data.  AI Ready data has been processed, typically by a skilled individual with a data science background, with compute and storage already allocated ready for model use.  By enabling the \"AI Ready\" data concept you ensure repeatable model results, ready addition of new data, and faster time to model training without waste of resources for human capital and compute.\n",
    "\n",
    "AI Ready data is:\n",
    "\n",
    "+ 1. Versioned\n",
    "+ 2. Discoverable (easily found, timeliness, contains meta-data)\n",
    "+ 3. Security (controlled)\n",
    "+ 4. Peer reviewed\n",
    "+ 5. Accuracy (curated, timliness)\n",
    "  + 5.1 Data should be reflective of the intended population and be statistically relevant\n",
    "  + 5.2 Missing data should be accounted for.\n",
    "  + 5.3 Analysis of data and correlations between variables is a minimal requirement.\n",
    "  + 5.4 Identification of Dependent and Independent variables must be established.\n",
    "  + 5.5 Min/Max threshoulds should exist and be considered against physics.\n",
    "  + 5.6 Variance should be reviewed.\n",
    "  + 5.7 Standardized formatting, conversion of units, recoding, and categorization (one-shot) values established.\n",
    "+ 7. Consumable (easily read, timeliness)\n",
    "+ 8. Requires minimal modification prior to model enagement (timeliness)\n",
    "+ 9. Findable, Accessible, Interoperable, Reusable (FAIR)\n",
    " \n",
    "For insight into methods of data analysis and curation see: 002 through 003a of this notebook series for Pandas manipulation of data.  Plotting packages are found in 011 and data Anomaly Detection and Cleanup can be found in [015 Anomaly Detection & Clean-Up](https://github.com/christophergarthwood/jbooks/blob/main/015_AnomalyDetection_Cleanup.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Provenance (Meta-Data and history)\n",
    "\n",
    "Data provenance, which refers to the detailed history of the data’s origins, movements, and transformations, is essential in the context of Responsible AI. Understanding data provenance allows for greater transparency, accountability, and trust in AI systems. It ensures that AI models are built on reliable and well-documented data, mitigating risks associated with biases and errors.\n",
    "\n",
    "Standardized metadata provides context to the raw data, offering crucial details such as sample origin, experimental conditions, and collection methods. These annotations make the data more interpretable and ready for downstream processing. Without harmonization, even the most extensive datasets risk being unusable due to inconsistencies that hinder integration and reproducibility.\n",
    "\n",
    "By maintaining a clear record of data provenance, users can trace back the lineage of data, verify its integrity, and understand the context in which it was collected and processed. This transparency is crucial for ensuring that AI systems are fair and unbiased, as it allows for the identification and correction of any issues that may arise during the data lifecycle.\n",
    "\n",
    "Data provenance also supports compliance with ethical guidelines and regulatory requirements, ensuring that data is used responsibly and in accordance with established standards. This is particularly important in sensitive fields such as healthcare, finance, and social sciences, where the implications of data misuse can be significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAIR (Findable, Accessible, Interoperable, Reusable)\n",
    "\n",
    "#### Findable\n",
    "\n",
    "The first step in (re)using data is to find them. Metadata and data should be easy to find for both humans and computers. Machine-readable metadata are essential for automatic discovery of datasets and services, so this is an essential component of the FAIRification process.\n",
    "\n",
    "+ F1. (Meta)data are assigned a globally unique and persistent identifier\n",
    "\n",
    "+ F2. Data are described with rich metadata (defined by R1 below)\n",
    "\n",
    "+ F3. Metadata clearly and explicitly include the identifier of the data they describe\n",
    "\n",
    "+ F4. (Meta)data are registered or indexed in a searchable resource\n",
    "\n",
    "#### Accessible\n",
    "\n",
    "Once the user finds the required data, they need to know how they can be accessed, possibly including authentication and authorisation.\n",
    "\n",
    "+ A1. (Meta)data are retrievable by their identifier using a standardised communications protocol\n",
    "\n",
    "  + A1.1 The protocol is open, free, and universally implementable\n",
    "\n",
    "  + A1.2 The protocol allows for an authentication and authorisation procedure, where necessary\n",
    "\n",
    "+ A2. Metadata are accessible, even when the data are no longer available\n",
    "\n",
    "#### Interoperable\n",
    "\n",
    "The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.\n",
    "\n",
    "+ I1. (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.\n",
    "\n",
    "+ I2. (Meta)data use vocabularies that follow FAIR principles\n",
    "\n",
    "+ I3. (Meta)data include qualified references to other (meta)data\n",
    "\n",
    "#### Reusable\n",
    "\n",
    "The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.\n",
    "\n",
    "+ R1. (Meta)data are richly described with a plurality of accurate and relevant attributes\n",
    "\n",
    "+ R1.1. (Meta)data are released with a clear and accessible data usage license\n",
    "\n",
    "+ R1.2. (Meta)data are associated with detailed provenance\n",
    "\n",
    "+ R1.3. (Meta)data meet domain-relevant community standards\n",
    "\n",
    "The principles refer to three types of entities: data (or any digital object), metadata (information about that digital object), and infrastructure. For instance, principle F4 defines that both metadata and data are registered or indexed in a searchable resource (the infrastructure component).\n",
    "\n",
    "Reference: [Wikipedia FAIR](https://en.wikipedia.org/wiki/FAIR_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Relevance\n",
    "\n",
    "#### General Guidelines and Common Recommendations:\n",
    "\n",
    "\n",
    "+ ***Central Limit Theorem:*** A sample size of 30 or more is often considered sufficient for the Central Limit Theorem to hold, allowing for the use of normal distribution approximations. \n",
    "\n",
    "+ ***Basic inferential statistics:*** A sample size of at least 30 is widely recommended for t-tests and confidence intervals. \n",
    "\n",
    "+ ***Regression analysis:*** Some researchers suggest at least 10 observations per variable. \n",
    "\n",
    "+ ***Surveys:*** For surveys aiming for good accuracy, 200 responses might be a good starting point, and 100 might be considered marginally acceptable. \n",
    "\n",
    "+ ***Margin of error:*** A larger sample size is needed for a smaller margin of error. \n",
    "    \n",
    "While 30 is a commonly mentioned minimum, it is important to consider the specific context of your research and use appropriate statistical methods and formulas to determine the necessary sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1737665372909,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "M3qlCehNBu-_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's define some variables (information holders) for our project overall\n",
    "\n",
    "global PROJECT_ID, BUCKET_NAME, LOCATION\n",
    "BUCKET_NAME = \"cio-training-vertex-colab\"\n",
    "PROJECT_ID = \"usfs-ai-bootcamp\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "BOLD_START = \"\\033[1m\"\n",
    "BOLD_END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508,
     "referenced_widgets": [
      "1015b1ce380943d48c7a6386444c768a",
      "f5405303114146a28663cb1c46db7beb",
      "bdc74a6d20a041819b6fc6d1c3f85a25",
      "033cb5a8c21c4ca5a13b5d03e8b78fa0",
      "2b447186de7a4571821ca59295744ce3",
      "cf363ab273bf4eeba7fbea4d79ed3a64",
      "479233081fbc44c8afe9a044ebb95faf",
      "202c8798fdda4778bf09a2124c37a6c7",
      "ad3b9f4855d541e49303c03fb9f07db7",
      "0d8988d435644a928829ef71435a3e83",
      "ebb9ca01cf52473699f21f15c5ec7d34",
      "689fe090492947608518efc9f3bd6d5d",
      "dd3de71c792f4762af3b280ac9b7f5e6",
      "fc3fc48bdebe4501998819fbda36152c",
      "f196621910824a00a97d06e5433cb294",
      "31b012d7d41a4d06933aa38e8e3bd74a"
     ]
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1737665373417,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "3TrA1A5nIeCV",
    "outputId": "65ad0051-dd12-4511-e88f-62bf0f35300e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now create a means of enforcing project id selection\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def wait_for_button_press():\n",
    "\n",
    "    button_pressed = False\n",
    "\n",
    "    # Create widgets\n",
    "    html_widget = widgets.HTML(\n",
    "        value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "\n",
    "        <table><tr><td>\n",
    "            <span style=\"font-family: Tahoma;font-size: 18\">\n",
    "              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n",
    "              Please verify that you are in the appropriate project and that the:</br>\n",
    "              <center><code><b>PROJECT_ID</b></code> </br></center>\n",
    "              aligns with the Project Id in the upper left corner of this browser and that the location:\n",
    "              <center><code><b>LOCATION</b></code> </br></center>\n",
    "              aligns with the instructions provided.\n",
    "            </span>\n",
    "          </td></tr></table></br></br>\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    project_list = [\n",
    "        \"usfs-ai-bootcamp\",\n",
    "        \"usfa-ai-advanced-training\",\n",
    "        \"I will setup my own\",\n",
    "    ]\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=project_list,\n",
    "        value=project_list[0],\n",
    "        description=\"Set Your Project:\",\n",
    "    )\n",
    "\n",
    "    html_widget2 = widgets.HTML(\n",
    "        value=\"\"\"\n",
    "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
    "          \"\"\"\n",
    "    )\n",
    "\n",
    "    button = widgets.Button(description=\"Accept\")\n",
    "\n",
    "    # Function to handle the selection change\n",
    "    def on_change(change):\n",
    "        global PROJECT_ID\n",
    "        if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
    "            # print(\"Selected option:\", change['new'])\n",
    "            PROJECT_ID = change[\"new\"]\n",
    "\n",
    "    # Observe the dropdown for changes\n",
    "    dropdown.observe(on_change)\n",
    "\n",
    "    def on_button_click(b):\n",
    "        nonlocal button_pressed\n",
    "        global PROJECT_ID\n",
    "        button_pressed = True\n",
    "        # button.disabled = True\n",
    "        button.close()  # Remove the button from display\n",
    "        with output:\n",
    "            # print(f\"Button pressed...continuing\")\n",
    "            # print(f\"Selected option: {dropdown.value}\")\n",
    "            PROJECT_ID = dropdown.value\n",
    "\n",
    "    button.on_click(on_button_click)\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Create centered layout\n",
    "    centered_layout = widgets.VBox(\n",
    "        [\n",
    "            html_widget,\n",
    "            widgets.HBox([dropdown, button]),\n",
    "            html_widget2,\n",
    "        ],\n",
    "        layout=widgets.Layout(\n",
    "            display=\"flex\", flex_flow=\"column\", align_items=\"center\", width=\"100%\"\n",
    "        ),\n",
    "    )\n",
    "    # Display the layout\n",
    "    display(centered_layout)\n",
    "\n",
    "\n",
    "wait_for_button_press()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zramkw-P93C-"
   },
   "source": [
    "## Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1737666212893,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "shY7a4DVQjWB",
    "outputId": "606fe1d2-f3de-47cf-f457-bb449cca5b52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# - Google Colab Check\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "import datetime\n",
    "\n",
    "RunningInCOLAB = False\n",
    "RunningInCOLAB = \"google.colab\" in str(get_ipython())\n",
    "current_time = datetime.datetime.now()\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    print(\n",
    "        f\"You are running this notebook in Google Colab at {current_time} in the {BOLD_START}{PROJECT_ID}{BOLD_END}lab.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"You are likely running this notebook with Jupyter iPython runtime at {current_time} in the {PROJECT_ID} lab.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZVkISRuLURi"
   },
   "source": [
    "## Library Management\n",
    "### Load Libraries necessary for this operation via pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1737665373426,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "logDyNfnLURj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import key libraries necessary to support dynamic installation of additional libraries\n",
    "import sys\n",
    "\n",
    "# Use subprocess to support running operating system commands from the program, using the \"bang\" (!)\n",
    "# symbology is supported, however that does not translate to an actual python script, this is a more\n",
    "# agnostic approach.\n",
    "import subprocess\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74882,
     "status": "ok",
     "timestamp": 1737665448288,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "ldXG-5fhsV1e",
    "outputId": "5941b5ee-93f4-41df-ccc2-8915e0e73ed7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the libraries you'd like to add to this Runtime environment.\n",
    "# Commented out as this adds time but is critical for initial run.\n",
    "\"\"\"\n",
    "libraries = [\n",
    "    \"backoff\",\n",
    "    \"python-dotenv\",\n",
    "    \"seaborn\",\n",
    "    \"piexif\",\n",
    "    \"unidecode\",\n",
    "    \"icecream\",\n",
    "    \"watermark\",\n",
    "    \"watermark[GPU]\",\n",
    "    \"rich\",\n",
    "    \"rich[jupyter]\",\n",
    "    \"numpy\",\n",
    "    \"pydot\",\n",
    "    \"polars[all]\",\n",
    "    \"dask[complete]\",\n",
    "    \"pytables\",\n",
    "    \"xarray\",\n",
    "    \"pandas\",\n",
    "    \"pystac\",\n",
    "    \"pystac[jinja2]\",\n",
    "    \"pystac[orjson]\",\n",
    "    \"pystac[validation]\",\n",
    "    \"fastparquet\",\n",
    "    \"zarr\",\n",
    "    \"gdown\",\n",
    "    \"wget\",\n",
    "]\n",
    "\n",
    "# Loop through each library and test for existence, if not present install quietly\n",
    "for library in libraries:\n",
    "    if library == \"Pillow\":\n",
    "        spec = importlib.util.find_spec(\"PIL\")\n",
    "    else:\n",
    "        spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "        print(\"Installing library \" + library)\n",
    "        subprocess.run([\"pip\", \"install\", library, \"--quiet\"], check=True)\n",
    "    else:\n",
    "        print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "# Specialized install for GPU enabled capability with CUDF\n",
    "# pip install --extra-index-url=https://pypi.nvidia.com \"cudf-cu12==25.2.*\" \"dask-cudf-cu12==25.2.*\" \"cuml-cu12==25.2.*\" \"cugraph-cu12==25.2.*\" \"nx-cugraph-cu12==25.2.*\" \"cuspatial-cu12==25.2.*\"     \"cuproj-cu12==25.2.*\" \"cuxfilter-cu12==25.2.*\" \"cucim-cu12==25.2.*\"\n",
    "try:\n",
    "    library=\"cudf-cu12\"\n",
    "    spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"pip\",\n",
    "                \"install\",\n",
    "                \"--extra-index-url=https://pypi.nvidia.com\",\n",
    "                library,\n",
    "                \"--quiet\",\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "    library=\"dask-cudf-cu12\"\n",
    "    spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"pip\",\n",
    "                \"install\",\n",
    "                \"--extra-index-url=https://pypi.nvidia.com\",\n",
    "                library,\n",
    "                \"--quiet\",\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "except (subprocess.CalledProcessError, RuntimeError, Exception) as e:\n",
    "    print(repr(e))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO_Hq5eq9joH"
   },
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5041,
     "status": "ok",
     "timestamp": 1737665453315,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "PJuXEPlkSo9p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Import additional libraries that add value to the project related to NLP\n",
    "\n",
    "# - Set of libraries that perhaps should always be in Python source\n",
    "import backoff\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "import gc\n",
    "import getopt\n",
    "import glob\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "import socket\n",
    "import sys\n",
    "import textwrap\n",
    "import tqdm\n",
    "import traceback\n",
    "import warnings\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "#- Datastructures\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "#- Profiling\n",
    "from time import perf_counter\n",
    "import gc\n",
    "import io\n",
    "import tracemalloc\n",
    "import psutil\n",
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "#- Text formatting\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.traceback import install\n",
    "from tabulate import tabulate\n",
    "import locale\n",
    "\n",
    "# - Displays system info\n",
    "from watermark import watermark as the_watermark\n",
    "from py3nvml import py3nvml\n",
    "\n",
    "# - Additional libraries for this work\n",
    "import math\n",
    "from base64 import b64decode\n",
    "from IPython.display import Image, Markdown\n",
    "import pandas, IPython.display as display, io, jinja2, base64\n",
    "from IPython.display import clear_output  # used to support real-time plotting\n",
    "import requests\n",
    "import unidecode\n",
    "import pydot\n",
    "import wget\n",
    "\n",
    "# - Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import dask as da\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "import tables                   #Optional dependency required for dask to to_hdf, to_parquet used.\n",
    "import xarray as xr\n",
    "import cupy_xarray              # never actually invoked in source itself use ds=ds.cupy.as_cupy(), not valuable to I/O\n",
    "import pystac as pys\n",
    "import pystac\n",
    "from pystac.utils import datetime_to_str\n",
    "\n",
    "# from stacframes import df_from\n",
    "import fastparquet as fq\n",
    "import zarr\n",
    "from zarr import Group\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "try:\n",
    "    import cudf\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import cupy\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Tensorflow and related AI libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import data as tf_data\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "\n",
    "# - Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.cbook import get_sample_data\n",
    "from matplotlib.offsetbox import AnnotationBbox, DrawingArea, OffsetImage, TextArea\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.patches import Circle\n",
    "from PIL import Image as PIL_Image\n",
    "import PIL.ImageOps\n",
    "import matplotlib.image as mpimg\n",
    "from imageio import imread\n",
    "import seaborn as sns\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from pylab import *\n",
    "\n",
    "# - Image meta-data for Section 508 compliance\n",
    "import piexif\n",
    "from piexif.helper import UserComment\n",
    "\n",
    "# - Progress bar\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataclass used to represent each metric used during execution\n",
    "#\n",
    "@dataclass\n",
    "class runtime_metrics:\n",
    "    id: str\n",
    "\n",
    "    # see @Profile, in seconds\n",
    "    runtime: float = field(init=False, default=0.0)\n",
    "\n",
    "    # reference: https://docs.python.org/3/library/profile.html\n",
    "    profile_data: cProfile.Profile = field(init=False)\n",
    "\n",
    "    # reference: https://www.geeksforgeeks.org/how-to-get-file-size-in-python/\n",
    "    file_size: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "    \n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_throughput: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_throughput: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_read_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_write_count: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_read_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_write_time: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # calculated in MBs, reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "    mem_current: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # calculated in MBs, reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "    mem_peak: float = field(\n",
    "        init=False,\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "                Id---------------------------------------------\n",
    "                                      Id: {self.id}\n",
    "                Runtime----------------------------------------\n",
    "                                 Runtime: {self.runtime:,.2f} milliseconds\n",
    "\n",
    "                I/O Size---------------------------------------\n",
    "                               File Size: {self.file_size:,.2f} bytes\n",
    "\n",
    "                I/O Counts-------------------------------------\n",
    "                      Targeted disk read: {self.io_disk_read_count:,.2f} counts\n",
    "                     Targeted disk write: {self.io_disk_write_count:,.2f} counts\n",
    "                       General disk read: {self.io_os_read_count:,.2f} counts\n",
    "                      General disk write: {self.io_os_write_count:,.2f} counts\n",
    "\n",
    "                I/O Time---------------------------------------\n",
    "                 Targeted disk read time: {self.io_disk_read_time:,.2f} milliseconds\n",
    "                Targeted disk write time: {self.io_disk_write_time:,.2f} milliseconds\n",
    "                  General disk read time: {self.io_os_read_time:,.2f} milliseconds\n",
    "                 General disk write time: {self.io_os_write_time:,.2f} milliseconds\n",
    "\n",
    "                Memory------------------------------------------\n",
    "                                 Current: {self.mem_current:,.2f} MB\n",
    "                                    Peak: {self.mem_peak:,.2f} MB\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "    #def __repr__(self):\n",
    "    #    return f'{self.__class__.__name__}(name={self.name!r}, unit_price={self.unit_price!r}, quantity={self.quantity_on_hand!r})'\n",
    "\n",
    "    # TODO - CGW\n",
    "    # def __post_init__(self):\n",
    "    #    self.id = f'{self.phrase}_{self.word_type.name.lower()}'\n",
    "\n",
    "    # worthy consideration - https://www.geeksforgeeks.org/psutil-module-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Profiling function custom created to track IO, memory, and runtme.\n",
    "# Reference: https://code.google.com/archive/p/psutil/wikis/Documentation.wiki#Disks\n",
    "# Reference: https://jiffyclub.github.io/snakeviz/\n",
    "# Reference: https://www.machinelearningplus.com/python/cprofile-how-to-profile-your-python-code/\n",
    "# Reference: https://cloud.google.com/stackdriver/docs/instrumentation/setup/python\n",
    "# Reference: https://www.turing.com/kb/python-code-with-cprofile\n",
    "\n",
    "def profile(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "\n",
    "        # custom metrics values\n",
    "        current_memories = 0.0\n",
    "        peak_memories = 0.0\n",
    "        current_metric = runtime_metrics(id=func.__name__)\n",
    "        disk = \"sdc\"\n",
    "\n",
    "        #####################################################################################################\n",
    "        # - Cprofiler startup\n",
    "        # Reference: https://www.google.com/search?client=firefox-b-1-d&q=python+example+use+of+cprofle+for+a+single+function#cobssid=s\n",
    "        #####################################################################################################\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        #####################################################################################################\n",
    "        # - Memory tracking\n",
    "        #  Reference: https://docs.python.org/3/library/tracemalloc.html\n",
    "        #  Reference: https://www.kdnuggets.com/how-to-trace-memory-allocation-in-python\n",
    "        #  Reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "        #####################################################################################################\n",
    "        tracemalloc.start()\n",
    "\n",
    "        #####################################################################################################\n",
    "        # - Disk tracking\n",
    "        # Reference: https://stackoverflow.com/questions/16945664/insight-needed-into-python-psutil-output#:~:text=1%20Answer%201%20%C2%B7%20read_count:%20number%20of,write_bytes:%20number%20of%20bytes%20written%20%C2%B7%20read_time:\n",
    "        #####################################################################################################\n",
    "        iocnt1 = psutil.disk_io_counters(perdisk=True)[disk]\n",
    "        disk_io_counters1 = psutil.disk_io_counters()\n",
    "        read_bytes_start = iocnt1.read_bytes\n",
    "        write_bytes_start = iocnt1.write_bytes\n",
    "        read_counters_start = iocnt1.read_count\n",
    "        write_counters_start = iocnt1.write_count\n",
    "        read_time_start = iocnt1.read_time\n",
    "        write_time_start = iocnt1.write_time\n",
    "        \n",
    "        read_os_bytes_start = disk_io_counters1.read_bytes\n",
    "        write_os_bytes_start = disk_io_counters1.write_bytes\n",
    "        read_os_counters_start = disk_io_counters1.read_count\n",
    "        write_os_counters_start = disk_io_counters1.write_count\n",
    "        read_os_time_start = disk_io_counters1.read_time\n",
    "        write_os_time_start = disk_io_counters1.write_time\n",
    "        \n",
    "        #####################################################################################################\n",
    "        # - Actual function call\n",
    "        #####################################################################################################\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        # disk close out\n",
    "        # targeted I/O\n",
    "        iocnt2 = psutil.disk_io_counters(perdisk=True)[\"sdc\"]\n",
    "        disk_io_counters2 = psutil.disk_io_counters()\n",
    "\n",
    "        #targeted disk\n",
    "        read_bytes_end = iocnt2.read_bytes\n",
    "        write_bytes_end = iocnt2.write_bytes\n",
    "        read_counters_end = iocnt2.read_count\n",
    "        write_counters_end = iocnt2.write_count\n",
    "        read_time_end = iocnt2.read_time\n",
    "        write_time_end = iocnt2.write_time\n",
    "        #general OS\n",
    "        read_os_bytes_end = disk_io_counters2.read_bytes\n",
    "        write_os_bytes_end = disk_io_counters2.write_bytes\n",
    "        read_os_counters_end = disk_io_counters2.read_count\n",
    "        write_os_counters_end = disk_io_counters2.write_count\n",
    "        read_os_time_end = disk_io_counters2.read_time\n",
    "        write_os_time_end = disk_io_counters2.write_time\n",
    "\n",
    "        #targeted disk\n",
    "        read_throughput = (read_bytes_end - read_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        write_throughput = (write_bytes_end - write_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        read_counters = (read_counters_end - read_counters_start)\n",
    "        write_counters = (read_counters_end - read_counters_start)\n",
    "        read_time =  (read_time_end - read_time_start)\n",
    "        write_time = (write_time_end - write_time_start)\n",
    "        current_metric.io_disk_read_throughput = read_throughput\n",
    "        current_metric.io_disk_write_throughput = write_throughput\n",
    "        current_metric.io_disk_read_count = read_counters\n",
    "        current_metric.io_disk_write_count = write_counters\n",
    "        current_metric.io_disk_read_time = read_time\n",
    "        current_metric.io_disk_write_time = write_time\n",
    "\n",
    "        #general OS\n",
    "        read_os_throughput = (read_os_bytes_end - read_os_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        write_os_throughput = (write_os_bytes_end - write_os_bytes_start) / (1024 * 1024)  # MB/s\n",
    "        read_os_counters = (read_os_counters_end - read_os_counters_start)\n",
    "        write_os_counters = (read_os_counters_end - read_os_counters_start)\n",
    "        read_os_time =  (read_os_time_end - read_os_time_start)\n",
    "        write_os_time = (write_os_time_end - write_os_time_start)\n",
    "        current_metric.io_os_read_throughput = read_os_throughput\n",
    "        current_metric.io_os_write_throughput = write_os_throughput\n",
    "        current_metric.io_os_read_count = read_os_counters\n",
    "        current_metric.io_os_write_count = write_os_counters\n",
    "        current_metric.io_os_read_time = read_os_time\n",
    "        current_metric.io_os_write_time = write_os_time\n",
    "\n",
    "\n",
    "\n",
    "        # memory close\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()        \n",
    "        current_metric.mem_current = current / (1024 * 1024)\n",
    "        current_metric.mem_peak = peak / (1024 * 1024)\n",
    "        tracemalloc.clear_traces()\n",
    "\n",
    "\n",
    "        # CProfiler disabled\n",
    "        pr.disable()\n",
    "\n",
    "        #can't pickle this result\n",
    "        #current_metric.profile_data=pr\n",
    "        \n",
    "        # s = io.StringIO()\n",
    "        # sortby = SortKey.CUMULATIVE\n",
    "        # ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "        # ps.print_stats()\n",
    "        # print(s.getvalue())\n",
    "        end_time = time.perf_counter()\n",
    "        # other characteristics\n",
    "        current_metric.runtime = end_time - start_time\n",
    "\n",
    "        timestamp = datetime.datetime.now()\n",
    "        #timestamp_str = timestamp.strftime(\"%Y%m%d%H%M%S%f\") # Format as string\n",
    "        timestamp_str = timestamp.strftime(\"%Y%m%d%H%M%S\") # Format as string\n",
    "        unique_id = uuid.uuid4()\n",
    "        filename = f\"{output_directory}/{timestamp_str}_{unique_id}_{current_metric.id}_profiler.pkl\"\n",
    "        #print(filename)\n",
    "        #print(\"##########################################\")\n",
    "        #print(current_metric)\n",
    "\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(current_metric, file)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Complex Function to Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def complex_function():\n",
    "    # Define the size of the matrix\n",
    "    matrix_size = 2048\n",
    "    # Generate two random matrices\n",
    "    matrix_a = np.random.rand(matrix_size, matrix_size)\n",
    "    matrix_b = np.random.rand(matrix_size, matrix_size)\n",
    "    result_matrix = np.matmul(matrix_a, matrix_b)\n",
    "    np.savez(\n",
    "        \"./folderOnColab/data/local_test.npy\",\n",
    "        the_matrix=result_matrix,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FUa8QJT9tw_"
   },
   "source": [
    "## Function Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lib Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1737665453699,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "v_CqUVLZ98Mz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lib_diagnostics() -> None:\n",
    "\n",
    "    import pkg_resources\n",
    "\n",
    "    package_name_length = 20\n",
    "    package_version_length = 10\n",
    "\n",
    "    # Show notebook details\n",
    "    #%watermark?\n",
    "    #%watermark --github_username christophergwood --email christopher.g.wood@gmail.com --date --time --iso8601 --updated --python --conda --hostname --machine --githash --gitrepo --gitbranch --iversions --gpu\n",
    "    # Watermark\n",
    "    print(\n",
    "        the_watermark(\n",
    "            author=f\"{AUTHOR_NAME}\",\n",
    "            github_username=f\"GITHUB_USERNAME\",\n",
    "            email=f\"{AUTHOR_EMAIL}\",\n",
    "            iso8601=True,\n",
    "            datename=True,\n",
    "            current_time=True,\n",
    "            python=True,\n",
    "            updated=True,\n",
    "            hostname=True,\n",
    "            machine=True,\n",
    "            gitrepo=True,\n",
    "            gitbranch=True,\n",
    "            githash=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f\"{BOLD_START}Packages:{BOLD_END}\")\n",
    "    print(\"\")\n",
    "    # Get installed packages\n",
    "    the_packages = [\n",
    "        \"nltk\",\n",
    "        \"numpy\",\n",
    "        \"os\",\n",
    "        \"pandas\",\n",
    "        \"keras\",\n",
    "        \"seaborn\",\n",
    "        \"fastparquet\",\n",
    "        \"zarr\",\n",
    "        \"dask\",\n",
    "        \"pystac\",\n",
    "        \"polars\",\n",
    "        \"xarray\",\n",
    "    ]  # Functions are like legos that do one thing, this function outputs library version history of effort.\n",
    "\n",
    "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "    for package_idx, package_name in enumerate(installed):\n",
    "        if package_name in the_packages:\n",
    "            installed_version = installed[package_name]\n",
    "            print(\n",
    "                f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\"\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        print(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
    "        print(\n",
    "            f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        print(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
    "            print(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
    "            print(f\"{'     current':<40}#: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"No GPU available, using CPU.\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 508 Compliance Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1737665453948,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "Nl_kxpFUKKD5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Routines designed to support adding ALT text to an image generated through Matplotlib.\n",
    "\n",
    "\n",
    "def capture(figure):\n",
    "    buffer = io.BytesIO()\n",
    "    figure.savefig(buffer)\n",
    "    # return F\"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "    return f\"data:image/jpg;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
    "\n",
    "\n",
    "def make_accessible(figure, template, **kwargs):\n",
    "    return display.Markdown(\n",
    "        f\"\"\"![]({capture(figure)} \"{template.render(**globals(), **kwargs)}\")\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# requires JPG's or TIFFs\n",
    "def add_alt_text(image_path, alt_text):\n",
    "    try:\n",
    "        if os.path.isfile(image_path):\n",
    "            img = PIL_Image.open(image_path)\n",
    "            if \"exif\" in img.info:\n",
    "                exif_dict = piexif.load(img.info[\"exif\"])\n",
    "            else:\n",
    "                exif_dict = {}\n",
    "\n",
    "            w, h = img.size\n",
    "            if \"0th\" not in exif_dict:\n",
    "                exif_dict[\"0th\"] = {}\n",
    "            exif_dict[\"0th\"][piexif.ImageIFD.XResolution] = (w, 1)\n",
    "            exif_dict[\"0th\"][piexif.ImageIFD.YResolution] = (h, 1)\n",
    "\n",
    "            software_version = \" \".join(\n",
    "                [\"STEM-001 with Python v\", str(sys.version).split(\" \")[0]]\n",
    "            )\n",
    "            exif_dict[\"0th\"][piexif.ImageIFD.Software] = software_version.encode(\n",
    "                \"utf-8\"\n",
    "            )\n",
    "\n",
    "            if \"Exif\" not in exif_dict:\n",
    "                exif_dict[\"Exif\"] = {}\n",
    "            exif_dict[\"Exif\"][piexif.ExifIFD.UserComment] = UserComment.dump(\n",
    "                alt_text, encoding=\"unicode\"\n",
    "            )\n",
    "\n",
    "            exif_bytes = piexif.dump(exif_dict)\n",
    "            img.save(image_path, \"jpeg\", exif=exif_bytes)\n",
    "        else:\n",
    "            rprint(\n",
    "                f\"Cound not fine {image_path} for ALT text modification, please check your paths.\"\n",
    "            )\n",
    "\n",
    "    except (FileExistsError, FileNotFoundError, Exception) as e:\n",
    "        process_exception(e)\n",
    "\n",
    "\n",
    "# Appears to solve a problem associated with GPU use on Colab, see: https://github.com/explosion/spaCy/issues/11909\n",
    "def getpreferredencoding(do_setlocale=True):\n",
    "    return \"UTF-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libary Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_library_configuration() -> None:\n",
    "\n",
    "    ############################################\n",
    "    # - JUPYTER NOTEBOOK OUTPUT CONTROL / FORMATTING\n",
    "    ############################################\n",
    "    # pandas set floating point to 4 places to things don't run loose\n",
    "    debug.msg_info(\"Setting Pandas and Numpy library options.\")\n",
    "    pd.set_option(\n",
    "        \"display.max_colwidth\", 10\n",
    "    )  # None if you want to view the full json blob in the printed dataframe, use this\n",
    "    pd.options.display.float_format = \"{:,.4f}\".format\n",
    "    np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Exception Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this function displays the stack trace on errors from a central location making adjustments to the display on an error easier to manage\n",
    "# functions perform useful solutions for highly repetitive code\n",
    "def process_exception(inc_exception: Exception) -> None:\n",
    "    if DEBUG_STACKTRACE == 1:\n",
    "        traceback.print_exc()\n",
    "        console.print_exception(show_locals=True)\n",
    "    else:\n",
    "        rprint(repr(inc_exception))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Stats for a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quick_df_stats(\n",
    "    inc_df: pd.DataFrame,\n",
    "    inc_header_count: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load the data and return as a pd.DataFrame.\n",
    "\n",
    "            Parameters:\n",
    "                   inc_df (pd.DataFrame): Dataframe to be inspected, displayed\n",
    "                   inc_header_count (int): Anticipated number of columns to read in (validation check)\n",
    "\n",
    "            Returns:\n",
    "                    Printed output\n",
    "    \"\"\"\n",
    "    print(\"Data Resolution has: \" + str(inc_df.columns))\n",
    "    print(\"\\n\")\n",
    "    print(f\"\"\"{\"size\":20} : {inc_df.size:15,} \"\"\")\n",
    "    print(f\"\"\"{\"shape\":20} : {str(inc_df.shape):15} \"\"\")\n",
    "    print(f\"\"\"{\"ndim\":20} : {inc_df.ndim:15,} \"\"\")\n",
    "    print(f\"\"\"{\"column size\":20} : {inc_df.columns.size:15,} \"\"\")\n",
    "\n",
    "    # index added so you get an extra column\n",
    "    print(f\"\"\"{\"Read\":20} : {inc_df.columns.size:15,} \"\"\")\n",
    "    print(f\"\"\"{\"Expected\":20} : {inc_header_count:15,} \"\"\")\n",
    "    if (inc_df.columns.size) == inc_header_count:\n",
    "        print(f\"{BOLD_START}Expectations met{BOLD_END}.\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Expectations {BOLD_START}not met{BOLD_END}, check your datafile, columns don't match.\"\n",
    "        )\n",
    "    rprint(\"\\n\")\n",
    "    # rprint(str(inc_df.describe()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the FIADB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66496,
     "status": "ok",
     "timestamp": 1737665520749,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "AZcbq467sgrc",
    "outputId": "0a82f71c-4bcc-45bb-8681-8cce7803c49a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://research.fs.usda.gov/programs/fia#data-and-tools\n",
    "# Forest Inventory Asset Database (FIADB)\n",
    "\n",
    "\n",
    "def download_fiadb() -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    dataset_long_names = [\n",
    "        \"ALASKA_AK\",\n",
    "        \"CALIFORNIA_CA\",\n",
    "        \"HAWAII_HI\",\n",
    "        \"IDAHO_ID\",\n",
    "        \"NEVADA_NV\",\n",
    "        \"OREGON_OR\",\n",
    "        \"WASHINGTON_WA\",\n",
    "        \"ARIZONA_AZ\",\n",
    "        \"ARKANSAS_AR\",\n",
    "        \"COLORADO_CO\",\n",
    "        \"IOWA_IA\",\n",
    "        \"KANSAS_KS    \",\n",
    "        \"LOUISIANA_LA\",\n",
    "        \"MINNESOTA_MN\",\n",
    "        \"MISSOURI_MO\",\n",
    "        \"MONTANA_MT\",\n",
    "        \"NEBRASKA_NE\",\n",
    "        \"NEW_MEXICO_NM\",\n",
    "        \"NORTH_DAKOTA_ND\",\n",
    "        \"OKLAHOMA_OK\",\n",
    "        \"SOUTH_DAKOTA_SD\",\n",
    "        \"TEXAS_TX\",\n",
    "        \"U    TAH_UT\",\n",
    "        \"WYOMING_WY\",\n",
    "        \"ALABAMA_AL\",\n",
    "        \"CONNECTICUT_CT\",\n",
    "        \"DELAWARE_DE\",\n",
    "        \"FLORIDA_FL\",\n",
    "        \"GEORGIA_GA\",\n",
    "        \"ILLINOIS_IL\",\n",
    "        \"INDIANA_IN\",\n",
    "        \"KENTUCKY_KY\",\n",
    "        \"MAINE_ME\",\n",
    "        \"MARYLAND    _MD\",\n",
    "        \"MASSACHUSETTS_MA\",\n",
    "        \"MICHIGAN_MI\",\n",
    "        \"MISSISSIPPI_MS\",\n",
    "        \"NEW_HAMPSHIRE_NH\",\n",
    "        \"NEW_JERSEY_NJ\",\n",
    "        \"NEW_YORK_NY\",\n",
    "        \"NORTH_CAROLINA_NC\",\n",
    "        \"OHIO_OH\",\n",
    "        \"PENNSYLVANIA_PA\",\n",
    "        \"    RHODE_ISLAND_RI\",\n",
    "        \"SOUTH_CAROLINA_SC\",\n",
    "        \"TENNESSEE_TN\",\n",
    "        \"VERMONT_VT\",\n",
    "        \"VIRGINIA_VA\",\n",
    "        \"WEST_VIRGINIA_WV\",\n",
    "        \"WISCONSIN_WI\",\n",
    "        \"GUAM_GU\",\n",
    "        \"FEDERATED_STATES_OF_MICRONES_FM    \",\n",
    "        \"NORTHERN_MARIANA_ISLANDS_MP\",\n",
    "        \"PALAU_PW\",\n",
    "        \"AMERICAN_SAMOA_AS\",\n",
    "        \"PUERTO_RICO_PR\",\n",
    "        \"US_VIRGIN_ISLANDS_VI\",\n",
    "    ]\n",
    "    dataset_short_names = [\n",
    "        \"AK\",\n",
    "        \"AL\",\n",
    "        \"AR\",\n",
    "        \"AS\",\n",
    "        \"AZ\",\n",
    "        \"CA\",\n",
    "        \"CO\",\n",
    "        \"CT\",\n",
    "        \"DE\",\n",
    "        \"FL\",\n",
    "        \"GA\",\n",
    "        \"GU\",\n",
    "        \"HI\",\n",
    "        \"IA\",\n",
    "        \"ID\",\n",
    "        \"IL\",\n",
    "        \"IN\",\n",
    "        \"KS\",\n",
    "        \"KY\",\n",
    "        \"LA\",\n",
    "        \"MA\",\n",
    "        \"MD\",\n",
    "        \"ME\",\n",
    "        \"MI\",\n",
    "        \"MN\",\n",
    "        \"MO\",\n",
    "        \"MP\",\n",
    "        \"MS\",\n",
    "        \"MT\",\n",
    "        \"NC\",\n",
    "        \"ND\",\n",
    "        \"NE\",\n",
    "        \"NH\",\n",
    "        \"NJ\",\n",
    "        \"NM\",\n",
    "        \"NV\",\n",
    "        \"NY\",\n",
    "        \"OH\",\n",
    "        \"OK\",\n",
    "        \"OR\",\n",
    "        \"PA\",\n",
    "        \"PR\",\n",
    "        \"PW\",\n",
    "        \"RI\",\n",
    "        \"SC\",\n",
    "        \"SD\",\n",
    "        \"SFM\",\n",
    "        \"TN\",\n",
    "        \"TX\",\n",
    "        \"UT\",\n",
    "        \"VA\",\n",
    "        \"VI\",\n",
    "        \"VT\",\n",
    "        \"WA\",\n",
    "        \"WI\",\n",
    "        \"WV\",\n",
    "        \"WY\",\n",
    "    ]\n",
    "    # dataset_pattern=\"https://apps.fs.usda.gov/fia/datamart/CSV/MT_VEG_SUBPLOT.zip\"\n",
    "    dataset_pattern = \"https://apps.fs.usda.gov/fia/datamart/CSV/\"\n",
    "\n",
    "    rprint(\"Performing `wget` on target FIA records.\")\n",
    "    target_folder = WORKING_FOLDER\n",
    "    if os.path.isdir(target_folder):\n",
    "        target_directory = f\"{target_folder}{os.sep}downloads\"\n",
    "        for idx, filename in enumerate(dataset_short_names):\n",
    "            if os.path.isdir(target_directory):\n",
    "                target_filename = f\"{filename}_CSV.zip\"\n",
    "                target_url = f\"{dataset_pattern}{target_filename}\"\n",
    "                try:\n",
    "                    rprint(\n",
    "                        f\"...copying {dataset_long_names[idx]} to target folder: {target_directory}\"\n",
    "                    )\n",
    "                    subprocess.run(\n",
    "                        [\n",
    "                            \"/usr/bin/wget\",\n",
    "                            \"--show-progress\",\n",
    "                            f\"--directory-prefix={target_directory}\",\n",
    "                            f\"{target_url}\",\n",
    "                        ],\n",
    "                        check=True,\n",
    "                    )\n",
    "                    rprint(\"......completed\")\n",
    "                except (subprocess.CalledProcessError, Exception) as e:\n",
    "                    rprocess_exception(e)\n",
    "            else:\n",
    "                rprint(\n",
    "                    f\"...target folder: {target_directory} isn't present for {filename} download.\"\n",
    "                )\n",
    "    else:\n",
    "        rprint(\n",
    "            \"ERROR: Local downloads folder not found/created.  Check the output to ensure your folder is created.\"\n",
    "        )\n",
    "        rprint(f\"...target folder: {target_directory}\")\n",
    "        rprint(\"...if you can't find the problem contact the instructor.\")\n",
    "\n",
    "    # Process the downloaded data, open it up\n",
    "    rprint(\"Uncompressing the downloads...\")\n",
    "    if os.path.isdir(target_folder):\n",
    "        source_directory = f\"{target_folder}{os.sep}downloads\"\n",
    "        target_directory = f\"{target_folder}{os.sep}data\"\n",
    "        if os.path.isdir(target_directory) and os.path.isdir(source_directory):\n",
    "            for idx, filename in enumerate(dataset_short_names):\n",
    "                target_filename = f\"{filename}_CSV.zip\"\n",
    "                final_directory = f\"{target_directory}{os.sep}{filename}{os.sep}\"\n",
    "                try:\n",
    "                    if os.path.isfile(f\"{source_directory}{os.sep}{target_filename}\"):\n",
    "                        rprint(\n",
    "                            f\"...unzipping {dataset_long_names[idx]} to created target folder: {final_directory}\"\n",
    "                        )\n",
    "                        subprocess.run([\"mkdir\", \"-p\", final_directory], check=True)\n",
    "                        subprocess.run(\n",
    "                            [\n",
    "                                \"/usr/bin/unzip\",\n",
    "                                \"-o\",\n",
    "                                \"-qq\",\n",
    "                                \"-d\",\n",
    "                                f\"{final_directory}\",\n",
    "                                f\"{source_directory}{os.sep}{target_filename}\",\n",
    "                            ],\n",
    "                            check=True,\n",
    "                        )\n",
    "                        process1 = subprocess.Popen(\n",
    "                            [\n",
    "                                \"/usr/bin/find\",\n",
    "                                f\"{final_directory}\",\n",
    "                                \"-type\",\n",
    "                                \"f\",\n",
    "                                \"-print\",\n",
    "                            ],\n",
    "                            stdout=subprocess.PIPE,\n",
    "                        )\n",
    "                        process2 = subprocess.Popen(\n",
    "                            [\"wc\", \"-l\"], stdin=process1.stdout, stdout=subprocess.PIPE\n",
    "                        )\n",
    "\n",
    "                        # Close the output of process1 to allow process2 to receive EOF\n",
    "                        process1.stdout.close()\n",
    "                        output, error = process2.communicate()\n",
    "                        process2.stdout.close()\n",
    "                        number_files = output.decode().strip()\n",
    "                        rprint(f\"......completed, {number_files} files extracted.\")\n",
    "                    else:\n",
    "                        rprint(\n",
    "                            f\"......failed, unable to find ({source_directory}{os.sep}{target_filename}{os.sep})\"\n",
    "                        )\n",
    "                except (subprocess.CalledProcessError, Exception) as e:\n",
    "                    process_exception(e)\n",
    "                break\n",
    "        else:\n",
    "            rprint(\n",
    "                f\"...either the source directory ({source_directory})  or the ({target_directory}) isn't present for extraction.\"\n",
    "            )\n",
    "    else:\n",
    "        rprint(\n",
    "            \"ERROR: Local downloads folder not found/created.  Check the output to ensure your folder is created.\"\n",
    "        )\n",
    "        rprint(f\"...target folder: {target_directory}\")\n",
    "        rprint(\"...if you can't find the problem contact the instructor.\")\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDLoEWmVR9Sk"
   },
   "source": [
    "#### Download NOAA GDS 0.25 Degree Data for a Range of Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1737665520751,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "IZrJ8FhpR78m",
    "outputId": "111a3aec-6ad8-4889-c3ed-7f8e9ed4e737",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://polar.ncep.noaa.gov/global/data_access.shtml\n",
    "# Global Forecast System (GFS), 0.25 degree resolution\n",
    "def download_noaa() -> None:\n",
    "\n",
    "    print(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    dataset_url = \"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gfs.20250304/00/atmos/gfs.t00z.atmf000.nc\"\n",
    "    dataset_url_pattern_begin = (\n",
    "        f\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gfs.\"\n",
    "    )\n",
    "    dataset_filename_pattern = \"gfs.t00z.atmf000.nc\"\n",
    "    dataset_url_pattern_end = f\"/00/atmos/{dataset_filename_pattern}\"\n",
    "    dataset_date_month = \"03\"\n",
    "    dataset_day_start = int(18)\n",
    "    dataset_day_end = int(19)\n",
    "\n",
    "    print(\"...Performing `wget` on target GFS records.\")\n",
    "    target_folder = WORKING_FOLDER\n",
    "    if os.path.isdir(target_folder):\n",
    "        target_directory = f\"{target_folder}{os.sep}data\"\n",
    "        if os.path.isdir(target_directory):\n",
    "            for idx, day in enumerate(range(dataset_day_start, dataset_day_end)):\n",
    "                if day < 10:\n",
    "                    day = f\"0{day}\"\n",
    "                target_date = f\"2025{dataset_date_month}{day}\"\n",
    "                target_url = \"\".join(\n",
    "                    [dataset_url_pattern_begin, target_date, dataset_url_pattern_end]\n",
    "                )\n",
    "\n",
    "                # remove potentially partial downloads\n",
    "                target_partial_file = (\n",
    "                    f\"{target_folder}{os.sep}{dataset_filename_pattern}\"\n",
    "                )\n",
    "                if os.path.isfile(target_partial_file):\n",
    "                    print(\n",
    "                        f\"...removing {dataset_filename_pattern} as it is likely a partial download.\"\n",
    "                    )\n",
    "                    subprocess.run(\n",
    "                        [\"/usr/bin/rm\", \"-rf\", f\"{target_partial_file}\"], check=True\n",
    "                    )\n",
    "\n",
    "                exit_code=0\n",
    "                stdout=\"\"\n",
    "                stderr=\"\"\n",
    "                try:\n",
    "                    print(f\"......searching for download of day:{target_date}\")\n",
    "                    # subprocess.run([\"/usr/bin/wget\", \"--show-progress\", f\"--directory-prefix={target_directory}\", f\"{target_url}\"], check=True)\n",
    "                    #process = subprocess.run([\"/usr/bin/wget\", \"--quiet\", \" --server-response\", \"--no-check-certificate\", f\"--directory-prefix={target_directory}\", f\"{target_url}\"], check=True, text=True, capture_output=True, shell=False)\n",
    "                    process = subprocess.run([\"/usr/bin/wget\", \"--quiet\", \"--no-check-certificate\", f\"--directory-prefix={target_directory}\", f\"{target_url}\"], check=True)\n",
    "                    print(process)\n",
    "                    stdout = process.stdout\n",
    "                    stderr = process.stderr\n",
    "                    exit_code = process.check_returncode()\n",
    "                except (subprocess.CalledProcessError) as e:\n",
    "                    stdout = e.stdout\n",
    "                    stderr = e.stderr\n",
    "                    exit_code=e.returncode\n",
    "                    process_exception(e)\n",
    "                    pass\n",
    "\n",
    "                if exit_code == 0:\n",
    "                    try:\n",
    "                        if os.path.isfile(dataset_filename_pattern):\n",
    "                            print(f\"......completed download of day:{target_date}\")\n",
    "                        target_filename = \"_\".join([target_date, dataset_filename_pattern])\n",
    "                        os.rename(dataset_filename_pattern, target_filename)\n",
    "                        print(f\".........renamed file to {target_filename}\")\n",
    "                        if os.path.isfile(target_filename):\n",
    "                            print(f\".........{BOLD_START}SUCCESS{BOLD_END}.\")\n",
    "                        else:\n",
    "                            print(f\".........inspect download, there could be a problem.\")\n",
    "                    except (subprocess.CalledProcessError, Exception) as e:\n",
    "                        process_exception(e)\n",
    "                        pass\n",
    "                else:\n",
    "                        print(f\"......didn't complete download of day:{target_date}\")\n",
    "                        print(f\".........{BOLD_START}FAIL{BOLD_END}.\")\n",
    "                        print(\"\")\n",
    "                        print(\"\")\n",
    "                break\n",
    "        else:\n",
    "            print(\n",
    "                f\"ERROR: Target folder, {target_directory}, isn't present for {target_date} download.\"\n",
    "            )\n",
    "            raise SystemError\n",
    "    else:\n",
    "        print(\n",
    "            \"ERROR: Local downloads folder not found/created.  Check the output to ensure your folder is created.\"\n",
    "        )\n",
    "        print(f\"...target folder: {target_directory}\")\n",
    "        raise SystemError\n",
    "\n",
    "    print(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Single Specific NetCDF (MS Bight in Gulf of America) from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_test() -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    THE_FILE = \"ACS.txt\"\n",
    "    THE_ID = \"12L8VRY6J1Sj-B1vIf-ODh4kjHWHqIzm8\"\n",
    "\n",
    "    THE_FILE = \"MissBight_2020010900.nc\"\n",
    "    THE_ID = \"1uYMFrdeVD7_qvG2wRbyu6ir9C6b4wAZC\"\n",
    "\n",
    "    target_folder = f\"{WORKING_FOLDER}{os.sep}data\"\n",
    "\n",
    "    target_ids = [THE_ID]\n",
    "    target_filenames = [THE_FILE]\n",
    "\n",
    "    for idx, the_id in enumerate(target_ids):\n",
    "        try:\n",
    "            if os.path.isfile(f\"{target_folder}{os.sep}{target_filenames[idx]}\"):\n",
    "                rprint(f\"...no need to download {target_filenames[idx]} again.\")\n",
    "            else:\n",
    "                rprint(f\"...downloading {target_filenames[idx]}.\")\n",
    "                subprocess.run(\n",
    "                    [\n",
    "                        \"gdown\",\n",
    "                        f\"{the_id}\",\n",
    "                        \"--no-check-certificate\",\n",
    "                        \"--continue\",\n",
    "                        \"-O\",\n",
    "                        f\"{target_folder}{os.sep}{target_filenames[idx]}\",\n",
    "                    ],\n",
    "                    check=True,\n",
    "                )\n",
    "        except (subprocess.CalledProcessError, Exception) as e:\n",
    "            process_exception(e)\n",
    "            raise SystemError\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTrnAspo3CWR"
   },
   "source": [
    "#### Check your resources from a CPU/GPU perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1737665454280,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "hmGUvC7M3B0H",
    "outputId": "638ecead-d943-42f2-a2eb-99790d7fe3bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hardware_stats() -> None:\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    print(\n",
    "        f\"{BOLD_START}List Devices{BOLD_END} #########################################\"\n",
    "    )\n",
    "    try:\n",
    "        from tensorflow.python.client import device_lib\n",
    "\n",
    "        rprint(device_lib.list_local_devices())\n",
    "        print(\"\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    print(\n",
    "        f\"{BOLD_START}Devices Counts{BOLD_END} ########################################\"\n",
    "    )\n",
    "    try:\n",
    "        rprint(\n",
    "            f\"Num GPUs Available: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\"\n",
    "        )\n",
    "        rprint(\n",
    "            f\"Num CPUs Available: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\"\n",
    "        )\n",
    "        print(\"\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    print(\n",
    "        f\"{BOLD_START}Optional Enablement{BOLD_END} ####################################\"\n",
    "    )\n",
    "    try:\n",
    "        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    if gpus:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        try:\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "            rprint(\n",
    "                str(\n",
    "                    str(len(gpus))\n",
    "                    + \" Physical GPUs,\"\n",
    "                    + str(len(logical_gpus))\n",
    "                    + \" Logical GPU\"\n",
    "                )\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            # Visible devices must be set before GPUs have been initialized\n",
    "            rprint(str(repr(e)))\n",
    "        print(\"\")\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_house() -> None:\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    gc.collect()\n",
    "\n",
    "    # could leave the GPU unstable so holding off.\n",
    "    # torch.cuda.empty_cache()\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuke_file(target_filename: str) -> None:\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    if os.path.isfile(target_filename):\n",
    "        try:\n",
    "            # removing existing file, else you would append\n",
    "            subprocess.run([\"rm\", \"-rf\", f\"{target_filename}\"], check=True)\n",
    "        except (subprocess.CalledProcessError, Exception) as e:\n",
    "            process_exception(e)\n",
    "            raise SystemError\n",
    "    # rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46JMSTY2QjWD"
   },
   "source": [
    "## Input Sources\n",
    "### Create the storage locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1737665454282,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "pu8f4w5XK6i8",
    "outputId": "2aa1c04c-7c5a-4348-93a1-67ea14f79c83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the folder that will hold our content.\n",
    "def create_storage_locations(inc_directory: str) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    target_folder = inc_directory\n",
    "    sub_folders = [\"downloads\", \"data\"]\n",
    "    rprint(f\"Creating project infrastructure:\")\n",
    "    try:\n",
    "        for idx, subdir in enumerate(sub_folders):\n",
    "            target_directory = f\"{target_folder}{os.sep}{subdir}\"\n",
    "            rprint(f\"...creating ({target_directory}) to store project data.\")\n",
    "            if os.path.isfile(target_directory):\n",
    "                raise OSError(\n",
    "                    f\"Cannot create your folder ({target_directory}) a file of the same name already exists there, work with your instructor or remove it yourself.\"\n",
    "                )\n",
    "            elif os.path.isdir(target_directory):\n",
    "                print(\n",
    "                    f\"......folder named ({target_directory}) {BOLD_START}already exists{BOLD_END}, we won't try to create a new folder.\"\n",
    "                )\n",
    "            else:\n",
    "                subprocess.run([\"mkdir\", \"-p\", target_directory], check=True)\n",
    "    except (subprocess.CalledProcessError, Exception) as e:\n",
    "        process_exception(e)\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_basic_map(data) -> None:\n",
    "\n",
    "    # plt.figure()\n",
    "    lat = the_netcdf.variables[\"latitude\"][:]\n",
    "    lon = the_netcdf.variables[\"longitude\"][:]\n",
    "    data = the_netcdf.variables[\"water_temp\"][0, 0, :, :]\n",
    "\n",
    "    m = Basemap(\n",
    "        projection=\"mill\",\n",
    "        lat_ts=10,\n",
    "        llcrnrlon=lon.min(),\n",
    "        urcrnrlon=lon.max(),\n",
    "        llcrnrlat=lat.min(),\n",
    "        urcrnrlat=lat.max(),\n",
    "        resolution=\"c\",\n",
    "    )\n",
    "\n",
    "    Lon, Lat = meshgrid(lon, lat)\n",
    "    x, y = m(Lon, Lat)\n",
    "\n",
    "    # cs = m.pcolormesh(x,y,data,shading='flat', cmap=plt.cm.jet)\n",
    "    cs = m.pcolormesh(x, y, data, cmap=plt.cm.jet)\n",
    "\n",
    "    m.drawcoastlines()\n",
    "    m.fillcontinents()\n",
    "    m.drawmapboundary()\n",
    "    m.drawparallels(np.arange(-90.0, 120.0, 30.0), labels=[1, 0, 0, 0])\n",
    "    m.drawmeridians(np.arange(-180.0, 180.0, 60.0), labels=[0, 0, 0, 1])\n",
    "\n",
    "    colorbar(cs)\n",
    "    plt.title(\"Example 1: Global RTOFS SST from NOMADS\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate Various Data Storage Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Read\n",
    "\n",
    "Routine created to simulate large read, at once, of many large files for profiling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "#Profiling function call\n",
    "def initial_read(inc_source_filenames: []) -> {}:\n",
    "    the_netcdfs=read_netcdfs(inc_source_filenames)\n",
    "    the_payload=gather_variables(PRODUCT_LNAME, inc_source_filenames, the_netcdfs)\n",
    "\n",
    "    return the_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read NetCDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_netcdfs(inc_source_filenames: []) -> []:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    the_list = []\n",
    "\n",
    "    rprint(f\"...reading NetCDF4 from list of {len(inc_source_filenames)} files:\")\n",
    "    for target_filename in inc_source_filenames:\n",
    "        try:\n",
    "            rprint(f\"......reading NetCDF4 ({target_filename})\")\n",
    "            the_netcdf = Dataset(target_filename, \"r\", format=\"NETCDF4\")\n",
    "            the_list.append(the_netcdf)\n",
    "        except Exception as e:\n",
    "            process_exception(e)\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    return the_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ocean surface only for a single variable, entire world high resolution\n",
    "def gather_variables(inc_product_name:str, inc_file_list:[], inc_netcdfs:[]) -> {}:\n",
    "\n",
    "        rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "        geospatial_lat_nm=LAT_SNAME\n",
    "        geospatial_lon_nm=LONG_SNAME\n",
    "        product_nm=inc_product_name\n",
    "        local_payload={}\n",
    "\n",
    "\n",
    "        master_lat=[]\n",
    "        master_lon=[]\n",
    "        master_varAry=[]\n",
    "    \n",
    "        #single variable exercise\n",
    "        print(\"...pulling data for and stacking it to the core data structure:\")\n",
    "        for idx in range(len(inc_file_list)):\n",
    "            print(f\"......appending {inc_file_list[idx]}\")\n",
    "            lat_length=0\n",
    "            lon_length=0\n",
    "            varAry_length=0\n",
    "            if geospatial_lat_nm in inc_netcdfs[idx].variables:\n",
    "                lat_var = inc_netcdfs[idx].variables[geospatial_lat_nm]\n",
    "                lat_length = len(lat_var)\n",
    "            if geospatial_lon_nm in inc_netcdfs[idx].variables:\n",
    "                lon_var = inc_netcdfs[idx].variables[geospatial_lon_nm]\n",
    "                lon_length = len(lon_var)\n",
    "            if product_nm in inc_netcdfs[idx].variables:\n",
    "                product_var = inc_netcdfs[idx].variables[product_nm]\n",
    "                product_length = len(product_var)\n",
    "\n",
    "            if (lat_length > 0 and lon_length > 0 and product_length > 0):\n",
    "                master_lat.append(np.array(inc_netcdfs[idx].variables[geospatial_lat_nm][:][:].flatten(),dtype=AI_NUMPY_DATA_TYPE))\n",
    "                master_lon.append(np.array(inc_netcdfs[idx].variables[geospatial_lon_nm][:][:].flatten(),dtype=AI_NUMPY_DATA_TYPE))\n",
    "                master_varAry.append(np.array(inc_netcdfs[idx].variables[product_nm][0][0][:][:].flatten(),dtype=AI_NUMPY_DATA_TYPE))\n",
    "            else:\n",
    "                print(f\"FAILURE with processing {inc_file_list[idx]}\")\n",
    "\n",
    "        #Concatenate all in one time     \n",
    "        new_master_lat=np.concatenate(master_lat) \n",
    "        new_master_lon=np.concatenate(master_lon) \n",
    "        new_master_varAry=np.concatenate(master_varAry) \n",
    "\n",
    "        print(f\"...{BOLD_START}{geospatial_lat_nm:10}{\" data type:\":20}{BOLD_END}{str(type(new_master_lat)):20}\")\n",
    "        print(f\".......shape:{new_master_lat.shape}\")\n",
    "        print(f\"....datatype:{new_master_lat.dtype}\")\n",
    "\n",
    "        print(f\"...{BOLD_START}{geospatial_lon_nm:10}{\" data type:\":20}{BOLD_END}{str(type(new_master_lon)):20}\")\n",
    "        print(f\".......shape:{new_master_lon.shape}\")\n",
    "        print(f\"....datatype:{new_master_lon.dtype}\")\n",
    "\n",
    "        print(f\"...{BOLD_START}{\"Data type\":20}({product_nm:20}){BOLD_END}{str(type(new_master_varAry)):20}\")\n",
    "        print(f\".......shape:{new_master_varAry.shape}\")\n",
    "        print(f\"....datatype:{new_master_varAry.dtype}\")\n",
    "\n",
    "        local_payload[LAT_LNAME]=new_master_lat\n",
    "        local_payload[LONG_LNAME]=new_master_lon\n",
    "        local_payload[PRODUCT_LNAME]=new_master_varAry\n",
    "    \n",
    "        rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")\n",
    "    \n",
    "        return local_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pandas(inc_payload: {}) -> pd.DataFrame():\n",
    "\n",
    "    if DEBUG_USING_GPU == 1:\n",
    "        import cudf.pandas\n",
    "\n",
    "        cudf.pandas.install()\n",
    "        import pandas as pd\n",
    "    else:\n",
    "        import pandas as pd\n",
    "\n",
    "    latSeries = pd.Series(inc_payload[LAT_LNAME].flatten())\n",
    "    lonSeries = pd.Series(inc_payload[LONG_LNAME].flatten())\n",
    "    #varSeries = pd.Series(inc_payload[PRODUCT_LNAME][0, 0, :, :].flatten())\n",
    "    varSeries = pd.Series(inc_payload[PRODUCT_LNAME].flatten())\n",
    "\n",
    "    # define a Panda.DataFrame()\n",
    "    frame = {\n",
    "        LAT_LNAME: latSeries,\n",
    "        LONG_LNAME: lonSeries,\n",
    "        PRODUCT_LNAME: varSeries,\n",
    "    }\n",
    "\n",
    "    # instantiate a dataframe\n",
    "    df = pd.DataFrame(frame)\n",
    "\n",
    "    # ensure the data is cast as expected\n",
    "    df[LAT_LNAME].astype(AI_PANDAS_DATA_TYPE)\n",
    "    df[LONG_LNAME].astype(AI_PANDAS_DATA_TYPE)\n",
    "    df[PRODUCT_LNAME].astype(AI_PANDAS_DATA_TYPE)\n",
    "\n",
    "    # clean up behind yourself\n",
    "    del latSeries, lonSeries, varSeries, frame\n",
    "    clean_house()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polars DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_polars(inc_payload: {}) -> pd.DataFrame():\n",
    "\n",
    "    import polars as pl\n",
    "\n",
    "    latSeries = pl.Series(\"Float64\",inc_payload[LAT_LNAME].flatten(), dtype=pl.Float64)\n",
    "    lonSeries = pl.Series(\"Float64\",inc_payload[LONG_LNAME].flatten(), dtype=pl.Float64)\n",
    "    varSeries = pl.Series(\"Float64\",inc_payload[PRODUCT_LNAME].flatten(), dtype=pl.Float64)\n",
    "    \n",
    "    # define a Polars.DataFrame()\n",
    "    frame = {\n",
    "        LAT_LNAME: latSeries,\n",
    "        LONG_LNAME: lonSeries,\n",
    "        PRODUCT_LNAME: varSeries,\n",
    "    }\n",
    "\n",
    "    # instantiate a dataframe\n",
    "    df = pl.DataFrame(frame)\n",
    "\n",
    "    # clean up behind yourself\n",
    "    del latSeries, lonSeries, varSeries, frame\n",
    "    clean_house()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_polars(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_POLARS_EXT}\"\n",
    "\n",
    "    # create dataframe\n",
    "    df = build_polars(inc_payload)\n",
    "\n",
    "    # Apache Feather\n",
    "    # WRITE\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_polars(target_filename, df)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Polars Apache Feather Write Execution time: {execution_time:.4f} seconds\")\n",
    "        try:\n",
    "            os.sync()\n",
    "            time.sleep(1)\n",
    "            print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    # READ\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        start_time = time.perf_counter()\n",
    "        read_polars(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Polars Read Apache Feather Execution time: {execution_time:.4f} seconds\")\n",
    "        try:\n",
    "            os.sync()\n",
    "            time.sleep(1)\n",
    "            print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    # PARQUET\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_polars.parquet\"\n",
    "    # WRITE\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_polars_parquet(target_filename, df)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Polars Write Parquet Execution time: {execution_time:.4f} seconds\")\n",
    "        try:\n",
    "            os.sync()\n",
    "            time.sleep(1)\n",
    "            print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    # READ\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        start_time = time.perf_counter()\n",
    "        read_polars_parquet(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Polars Read Parquet Execution time: {execution_time:.4f} seconds\")\n",
    "        try:\n",
    "            os.sync()\n",
    "            time.sleep(1)\n",
    "            print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "# Serialize to Feather format\n",
    "def write_polars(target_pandas_filename: str, df: pl.DataFrame) -> None:\n",
    "    df.write_ipc(target_pandas_filename)\n",
    "\n",
    "@profile\n",
    "def write_polars_parquet(target_parquet_filename: str, df: pl.DataFrame) -> None:\n",
    "    if df is not None:\n",
    "        df.write_parquet(target_parquet_filename)\n",
    "    else:\n",
    "        print(f\"ERROR: {__name__} experienced an error where the incoming Polars DataFrame was non-existent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "# Deserialize from Feather format\n",
    "def read_polars(target_pandas_filename: str) -> None:\n",
    "    df = pl.read_ipc(target_pandas_filename)\n",
    "                        \n",
    "@profile\n",
    "def read_polars_parquet(target_parquet_filename: str) -> None:\n",
    "    df_parquet = pl.read_parquet(target_parquet_filename)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pandas(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_PANDAS_EXT}\"\n",
    "\n",
    "    # create dataframe\n",
    "    df = build_pandas(inc_payload)\n",
    "    # quick stats\n",
    "    quick_df_stats(df, 3)\n",
    "\n",
    "    # Get memory usage of each column in bytes\n",
    "    memory_usage_per_column = df.memory_usage(deep=True)\n",
    "\n",
    "    # Get total memory usage of the DataFrame in bytes\n",
    "    total_memory_usage = df.memory_usage().sum()\n",
    "    print(f\"Original Dataframe memory use: {total_memory_usage:20,}\")\n",
    "\n",
    "    # WRITE\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_pandas(target_filename, df)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Pandas Write Execution time: {execution_time:.4f} seconds\")\n",
    "        try:\n",
    "            os.sync()\n",
    "            time.sleep(1)\n",
    "            print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    # READ\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        start_time = time.perf_counter()\n",
    "        read_pandas(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Pandas Read Execution time: {execution_time:.4f} seconds\")\n",
    "        try:\n",
    "            os.sync()\n",
    "            time.sleep(1)\n",
    "            print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_pandas(target_pandas_filename: str, df: pd.DataFrame) -> None:\n",
    "    df.to_pickle(target_pandas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_pandas(target_pandas_filename: str) -> None:\n",
    "    df = pd.read_pickle(target_pandas_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_numpy(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = (\n",
    "        f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_numpy.{OUTPUT_NUMPY_EXT}\"\n",
    "    )\n",
    "\n",
    "    # WRITE\n",
    "    #for idx in range(0,ITERATIONS):\n",
    "    nuke_file(target_filename)\n",
    "    start_time = time.perf_counter()\n",
    "    write_numpy_native(target_filename, inc_payload)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"...Numpy Write Execution time: {execution_time:.4f} seconds\")\n",
    "    try:\n",
    "        os.sync()\n",
    "        time.sleep(1)\n",
    "        print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    #df = build_pandas(inc_payload)\n",
    "    #for idx in range(0,ITERATIONS):\n",
    "    #    nuke_file(target_filename)\n",
    "    #    start_time = time.perf_counter()\n",
    "    #    write_numpy_as_df(target_filename, df)\n",
    "    #    end_time = time.perf_counter()\n",
    "    #    execution_time = end_time - start_time\n",
    "    #    print(f\"...Numpy pd.DataFrame Write Execution time: {execution_time:.4f} seconds\")\n",
    "    #    try:\n",
    "    #        os.sync()\n",
    "    #        time.sleep(1)\n",
    "    #        print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "    #    except Exception as e:\n",
    "    #        print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    # READ\n",
    "    for idx in range(0,ITERATIONS):\n",
    "        start_time = time.perf_counter()\n",
    "        read_numpy(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...Numpy Read Execution time: {execution_time:.4f} seconds\")\n",
    "        try:\n",
    "            #os.sync()\n",
    "            time.sleep(0.25)\n",
    "            print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_numpy_as_df(target_numpy_filename: str, df: pd.DataFrame) -> None:\n",
    "    if df is not None:\n",
    "        df_numpy = df.to_numpy()\n",
    "        np.savez(target_numpy_filename, df_numpy)\n",
    "    else:\n",
    "        print(f\"ERROR, {__name__} failed because the incoming array was None.\")\n",
    "        \n",
    "@profile\n",
    "def write_numpy_native(target_numpy_filename: str, inc_payload: {}) -> None:\n",
    "    # varying sized arrays\n",
    "    if inc_payload is not None:\n",
    "        np.savez(\n",
    "            target_numpy_filename,\n",
    "            LAT_LNAME=inc_payload[LAT_LNAME],\n",
    "            LONG_LNAME=inc_payload[LONG_LNAME],\n",
    "            PRODUCT_LNAME=inc_payload[PRODUCT_LNAME],\n",
    "        )\n",
    "    else:\n",
    "        print(f\"ERROR, {__name__} failed because the incoming array was None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_numpy(target_numpy_filename: str) -> None:\n",
    "    loaded_arr = np.load(target_numpy_filename + \".npz\")\n",
    "\n",
    "    # to unpack\n",
    "    # new_lat = loaded_arr[\"lat\"]\n",
    "    # new_lon = loaded_arr[\"lon\"]\n",
    "    # new_product = loaded_arr[\"product\"]\n",
    "    # del loaded_arr, new_lat, new_lon, new_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pytorch(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = (\n",
    "        f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_TORCH_EXT}\"\n",
    "    )\n",
    "\n",
    "    dev = \"cpu\"\n",
    "    # input_tensor = input_tensor.to(device)\n",
    "    lat_tensor = torch.tensor(inc_payload[LAT_LNAME].flatten(), dtype=AI_TORCH_DATA_TYPE).to(dev)\n",
    "    lon_tensor = torch.tensor(inc_payload[LONG_LNAME].flatten(), dtype=AI_TORCH_DATA_TYPE).to(dev)\n",
    "    var_tensor = torch.tensor(inc_payload[PRODUCT_LNAME].flatten(), dtype=AI_TORCH_DATA_TYPE).to(dev)\n",
    "    \n",
    "    for idx in range(0, ITERATIONS):\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_pytorch(target_filename, lat_tensor, lon_tensor, var_tensor)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...PyTorch Write Execution time: {execution_time:.4f} seconds\")\n",
    "        try:\n",
    "            os.sync()\n",
    "            time.sleep(1)\n",
    "            print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        start_time = time.perf_counter()\n",
    "        read_pytorch(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...PyTorch Read Execution time: {execution_time:.4f} seconds\")\n",
    "        try:\n",
    "            os.sync()\n",
    "            time.sleep(1)\n",
    "            print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR performing os.sync(), Exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_pytorch(target_pytorch_filename: str, lat_tensor: torch.tensor, lon_tensor: torch.tensor, var_tensor: torch.tensor) -> None:\n",
    "    # Save multiple tensors as a list\n",
    "    tensors_list = [lat_tensor, lon_tensor, var_tensor]\n",
    "    torch.save(tensors_list, target_pytorch_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_pytorch(target_pytorch_filename: str) -> None:\n",
    "    # Load the tensor from the file\n",
    "    tensor_loaded = torch.load(target_pytorch_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tensorflow(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_TENSORFLOW_EXT}\"\n",
    "    with tf.device(THE_DEVICE_NAME):\n",
    "        # create a TFRecord to store the data\n",
    "        lat_list = tf.train.FloatList(value=inc_payload[LAT_LNAME].flatten().tolist(), )\n",
    "        lon_list = tf.train.FloatList(value=inc_payload[LONG_LNAME].flatten().tolist(), )\n",
    "        varAry_list = tf.train.FloatList(\n",
    "            value=inc_payload[PRODUCT_LNAME].flatten().tolist(), \n",
    "        )\n",
    "        feature = {\n",
    "            LAT_LNAME: tf.train.Feature(float_list=lat_list),\n",
    "            LONG_LNAME: tf.train.Feature(float_list=lon_list),\n",
    "            PRODUCT_LNAME: tf.train.Feature(float_list=varAry_list),\n",
    "        }\n",
    "        tfRecord = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature)\n",
    "        ).SerializeToString()\n",
    "\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        nuke_file(target_filename)\n",
    "        start_time = time.perf_counter()\n",
    "        write_tensorflow(target_filename, tfRecord)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...TensorFlow Write Execution time: {execution_time:.4f} seconds\")\n",
    "        try:\n",
    "            os.sync()\n",
    "            time.sleep(1)\n",
    "            print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    for idx in range(0, ITERATIONS):\n",
    "        start_time = time.perf_counter()\n",
    "        read_tensorflow(target_filename)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"...TensorFlow Read Execution time: {execution_time:.4f} seconds\")\n",
    "        try:\n",
    "            os.sync()\n",
    "            time.sleep(1)\n",
    "            print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_tensorflow(target_tensorflow_filename: str, tfRecord) -> None:\n",
    "    with tf.io.TFRecordWriter(target_tensorflow_filename) as writer:\n",
    "        writer.write(tfRecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example_proto):\n",
    "    feature_description = {\n",
    "        \"latitude\": tf.io.FixedLenSequenceFeature(\n",
    "            [], dtype=tf.float32, allow_missing=True\n",
    "        ),\n",
    "        \"longitude\": tf.io.FixedLenSequenceFeature(\n",
    "            [], dtype=tf.float32, allow_missing=True\n",
    "        ),\n",
    "        \"product\": tf.io.FixedLenSequenceFeature(\n",
    "            [], dtype=tf.float32, allow_missing=True\n",
    "        ),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_tensorflow(target_tensorflow_filename: str) -> None:\n",
    "\n",
    "    with tf.device(THE_DEVICE_NAME):\n",
    "        dataset = tf.data.TFRecordDataset(target_tensorflow_filename)\n",
    "        tfRecord = dataset.map(parse_tfrecord_fn)\n",
    "        # for record in tfRecord:\n",
    "        #    lat = record[\"latitude\"]\n",
    "        #    lon = record[\"longitude\"]\n",
    "        #    varAry = record[\"product\"]\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### def process_xarray(the_netcdf) -> None:\n",
    "def process_xarray(inc_netcdf_filenames: []) -> None:\n",
    "\n",
    "    # xarray on GPU: https://github.com/xarray-contrib/cupy-xarray\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "\n",
    "    ds_xr = xr.open_mfdataset(inc_netcdf_filenames, combine='nested', concat_dim='time', parallel=True)\n",
    "    ds_xr = ds_xr.drop_vars(AI_XARRAY_REMOVE_VARIABLES)\n",
    "    #ds_xr = xr.combine_nested(inc_netcdf_filenames, concat_dim=['time', ])\n",
    "    #ds_list = [xr.open_dataset(file) for file in inc_netcdf_filenames]\n",
    "    #for idx,dataset in enumerate(ds_list):\n",
    "    #    ds_list[idx]=dataset.drop_vars(AI_XARRAY_REMOVE_VARIABLES)\n",
    "    #    ds_list[idx].variables[\"lat\"].astype(AI_NUMPY_DATA_TYPE)\n",
    "    #    ds_list[idx].variables[\"lon\"].astype(AI_NUMPY_DATA_TYPE)\n",
    "    #print(ds_list[0])\n",
    "    # Replace 'time' with the appropriate dimension if needed\n",
    "    #dsxr = xr.concat(ds_list[0:len(ds_list)-3], dim='time') \n",
    "    # WRITE - NetCDF\n",
    "\n",
    "    try:\n",
    "        print(\"Starting Xarray NetCDF Write routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_xarray.{OUTPUT_XARRAY_EXT}\"\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_xarray_netcdf(target_filename, ds_xr)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Write (NetCDF) Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "        print(\"Starting Xarray NetCDF Read routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            read_xarray_netcdf(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Read (NetCDF) Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    # WRITE - HDF5\n",
    "    try:\n",
    "        print(\"Starting Xarray HDF Write routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            target_filename = (\n",
    "                f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_xarray.h5\"\n",
    "            )\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_xarray_hdf5(target_filename, ds_xr)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Write (HDF5) Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "\n",
    "        print(\"Starting Xarray HDF Read routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            read_xarray_hdf5(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Read (HDF5) Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "    # WRITE - ZARR, acting up...not sure why\n",
    "    try:\n",
    "        print(\"Starting Xarray Zarr Write routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_xarray.{OUTPUT_ZARR_EXT}\"\n",
    "            print(f\"delete {target_filename}\")\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_xarray_zarr(target_filename, ds_xr)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Write (Zarr) Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        print(\"Starting Xarray Zarr Read routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            read_xarray_zarr(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Read (Zarr) Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    # WRITE - PICKLE, cannot pickle xarray very easily at all\n",
    "    try:\n",
    "        print(\"Starting Xarray Pickle Write routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_xarray.{OUTPUT_PANDAS_EXT}\"\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_xarray_pickle(target_filename, ds_xr)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Write (Pickle) Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "        print(\"Starting Xarray Pickle Read routines.\")\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            read_xarray_pickle(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Xarray Read (Pickle) Execution time: {execution_time:.4f} seconds\")\n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_house()\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_xarray_netcdf(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    if ds_xr is not None:\n",
    "        ds_xr.to_netcdf(target_xarray_filename)\n",
    "    else:\n",
    "        print(\"ERROR saving incoming ds_xr as pickled file because there is no content.\")\n",
    "\n",
    "@profile\n",
    "def write_xarray_hdf5(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    if ds_xr is not None:\n",
    "        ds_xr.to_netcdf(target_xarray_filename)\n",
    "    else:\n",
    "        print(\"ERROR saving incoming ds_xr as pickled file because there is no content.\")\n",
    "\n",
    "\n",
    "def write_xarray_zarr(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    if ds_xr is not None:\n",
    "        ds_xr.to_zarr(target_xarray_filename, mode='w')\n",
    "    else:\n",
    "        print(\"ERROR saving incoming ds_xr as pickled file because there is no content.\")\n",
    "    \n",
    "def write_xarray_pickle(target_xarray_filename: str, ds_xr: xr.Dataset) -> None:\n",
    "    if ds_xr is not None:\n",
    "        pkl = pickle.dumps(ds_xr, protocol=1)\n",
    "        with open(target_xarray_filename, \"wb\") as file:\n",
    "            # Use pickle.dump() to serialize and write the data to the file\n",
    "            pickle.dump(pkl, target_xarray_filename)\n",
    "    else:\n",
    "        print(\"ERROR saving incoming ds_xr as pickled file because there is no content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_xarray_netcdf(target_xarray_filename: str) -> None:\n",
    "    ds_xr_loaded = xr.open_dataset(target_xarray_filename, engine=\"netcdf4\")\n",
    "\n",
    "@profile\n",
    "def read_xarray_hdf5(target_xarray_filename: str) -> None:\n",
    "    ds_xr_loaded = xr.open_dataset(target_xarray_filename, engine=\"netcdf4\")\n",
    "\n",
    "@profile\n",
    "def read_xarray_zarr(target_xarray_filename: str) -> None:\n",
    "    ds_xr_loaded = xr.open_zarr(target_xarray_filename)\n",
    "\n",
    "@profile\n",
    "def read_xarray_pickle(target_xarray_filename: str) -> None:\n",
    "    try:\n",
    "        with open(target_xarray_filename, \"rb\") as file:\n",
    "            #ds_xr = pickle.load(file, protocol=-1)\n",
    "            #changed protocol because of saving issues\n",
    "            ds_xr = pickle.load(file, protocol=1)\n",
    "    except (FileNotFoundError, Exception) as e:\n",
    "        process_exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apache Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_parquet(inc_payload: {}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    # build the xarray dataset from scratch\n",
    "    df = build_pandas(inc_payload)\n",
    "\n",
    "    try:\n",
    "        # WRITE \n",
    "        for idx in range(0, ITERATIONS):\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_pandas.{OUTPUT_PARQUET_EXT}\"\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_parquet(target_filename, df)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Parquet Write Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\") \n",
    "\n",
    "        # READ\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            read_parquet(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Parquet Read Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\") \n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    del df\n",
    "    clean_house()\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_parquet(target_parquet_filename: str, df: pd.DataFrame) -> None:\n",
    "    if df is not None:\n",
    "        df.to_parquet(target_parquet_filename, compression=\"gzip\")\n",
    "    else:\n",
    "        print(f\"ERROR: {__name__} experienced an error where the incoming Pandas DataFrame was non-existent.\")\n",
    "\n",
    "@profile\n",
    "def read_parquet(target_parquet_filename: str) -> None:\n",
    "    df_parquet = pd.read_parquet(target_parquet_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_zarr(the_payload:{}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    # Zarr\n",
    "    # Save xarray Dataset to Zarr\n",
    "    # ds_zarr=ds_xr.to_zarr('data.zarr')\n",
    "    import zarr\n",
    "        \n",
    "    # ds_xr = xr.open_dataset(target_filename)\n",
    "    geospatial_lat_nm = LAT_SNAME\n",
    "    geospatial_lon_nm = LONG_SNAME\n",
    "    product_nm = PRODUCT_LNAME\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_zarr.{OUTPUT_ZARR_EXT}\"\n",
    "\n",
    "    lat=np.array(the_payload[LAT_LNAME].flatten(), dtype=AI_NUMPY_DATA_TYPE)\n",
    "    lon=np.array(the_payload[LONG_LNAME].flatten(), dtype=AI_NUMPY_DATA_TYPE)\n",
    "    varAry=np.array(the_payload[PRODUCT_LNAME].flatten(), dtype=AI_NUMPY_DATA_TYPE)\n",
    "\n",
    "    try:\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            # WRITE\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_zarr(target_filename, lat,lon,varAry)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Zarr Write Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\") \n",
    "\n",
    "        for idx in range(0, ITERATIONS):\n",
    "            # READ\n",
    "            start_time = time.perf_counter()\n",
    "            read_zarr(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Zarr Read Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\") \n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_zarr(target_zarr_filename: str, lat, lon, varAry) -> None:\n",
    "        root = zarr.group(store=target_zarr_filename, overwrite=True)\n",
    "        z_lat_grp = root.create_group(LAT_LNAME)\n",
    "        z_lon_grp = root.create_group(LONG_LNAME)\n",
    "        z_product_grp = root.create_group(PRODUCT_LNAME)\n",
    "        z_lat_ary = z_lat_grp.create_array( name=LAT_LNAME,shape=lat.shape, chunks=\"auto\", dtype=AI_NUMPY_DATA_TYPE)\n",
    "        z_lon_ary = z_lon_grp.create_array( name=LONG_LNAME,shape=lon.shape, chunks=\"auto\", dtype=AI_NUMPY_DATA_TYPE)\n",
    "        z_product_ary = z_product_grp.create_array( name=PRODUCT_LNAME,shape=varAry.shape, chunks=\"auto\", dtype=AI_NUMPY_DATA_TYPE)\n",
    "    \n",
    "        z_lat_ary[:] = lat\n",
    "        z_lon_ary[:] = lon\n",
    "        z_product_ary[:] = varAry\n",
    "    \n",
    "        print(f\"{BOLD_START}Zarr Node Tree:{BOLD_END}\")\n",
    "        print(root.tree())\n",
    "        #zarr.save(target_zarr_filename, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_zarr(target_zarr_filename: str) -> None:\n",
    "    # Load the group from the directory\n",
    "    z = zarr.open_group(target_zarr_filename, mode='r')\n",
    "    print(z.info)\n",
    "    print(z.tree())\n",
    "    print(z.groups())\n",
    "    for the_group in z.groups():\n",
    "        print(the_group)\n",
    "    \"\"\"\n",
    "    # Verify the structure and data\n",
    "    assert isinstance(loaded_group, zarr.Group)\n",
    "    assert 'root' in loaded_group\n",
    "    assert 'bar' in loaded_group['foo']\n",
    "    assert 'baz' in loaded_group['foo']\n",
    "    np.testing.assert_array_equal(bar, loaded_group['foo']['bar'])\n",
    "    np.testing.assert_array_equal(baz, loaded_group['foo']['baz'])\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dask(inc_payload:{}) -> None:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}.{OUTPUT_DASK_EXT}\"\n",
    "    df = build_pandas(inc_payload)\n",
    "    df_dask = dd.from_pandas(df, npartitions=1)\n",
    "    # Convert DataFrame rows to dictionaries and create a Dask bag\n",
    "    #df_bag = db.from_sequence(df.to_dict(orient=\"records\"))\n",
    "\n",
    "    # Print the Dask bag (computation is lazy, so compute() is needed to see the result)\n",
    "    #print(df_bag.compute())\n",
    "    try:\n",
    "        \"\"\"\n",
    "        for idx in range(0,ITERATIONS):\n",
    "            # WRITE\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_dask.hdf\"\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_dask_hdf(target_filename, df_dask)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Dask Write HDF Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\") \n",
    "            \n",
    "        for idx in range(0,ITERATIONS): \n",
    "            # READ\n",
    "            start_time = time.perf_counter()\n",
    "            read_dask_hdf(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Dask Read parquet Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\") \n",
    "        \"\"\"\n",
    "\n",
    "        #parquet\n",
    "        for idx in range(0,ITERATIONS): \n",
    "            #WRITE\n",
    "            target_filename = f\"{WORKING_FOLDER}{os.sep}data{os.sep}{DATA_VERSION_RELEASE}_dask.parquet\"\n",
    "            nuke_file(target_filename)\n",
    "            start_time = time.perf_counter()\n",
    "            write_dask_parquet(target_filename, df_dask)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Dask Write parquet Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\") \n",
    "\n",
    "        for idx in range(0,ITERATIONS): \n",
    "            # READ\n",
    "            start_time = time.perf_counter()\n",
    "            read_dask_parquet(target_filename)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"...Dask Read parquet Execution time: {execution_time:.4f} seconds\")\n",
    "            try:\n",
    "                os.sync()\n",
    "                time.sleep(1)\n",
    "                print(\"Force write everything committed successfully and then wait 1 second.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR performing os.sync(), Exception: {e}\") \n",
    "    except Exception as e:\n",
    "        process_exception(e)\n",
    "        pass\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def write_dask_hdf(target_dask_filename: str, inc_dataframe: dd) -> None:\n",
    "    if dd is not None:\n",
    "        inc_dataframe.to_hdf(target_dask_filename, key=PRODUCT_LNAME,) \n",
    "    else:\n",
    "        print(f\"ERROR: {__name__} had an error because the Dask DataFrame was non-existent.\")\n",
    "    \n",
    "@profile\n",
    "def write_dask_parquet(target_dask_filename: str, inc_dataframe: dd) -> None:\n",
    "    if inc_dataframe is not None:\n",
    "        inc_dataframe.to_parquet(target_dask_filename, ) \n",
    "    else:\n",
    "        print(f\"ERROR: {__name__} had an error because the Dask DataFrame was non-existent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_dask_hdf(target_dask_filename: str) -> None:\n",
    "    # Load the group from the directory\n",
    "    dask_dataframe=dd.read_hdf(target_dask_filename, key=PRODUCT_LNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def read_dask_parquet(target_dask_filename: str) -> None:\n",
    "    # Load the group from the directory\n",
    "    dask_dataframe=dd.read_parquet(target_dask_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySTAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def process_pystac(inc_payload:{}) -> None:\n",
    "\n",
    "        rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample pandas DataFrame\n",
    "        data = {\n",
    "            \"id\": [\"item1\", \"item2\"],\n",
    "            \"geometry\": [\n",
    "                {\"type\": \"Point\", \"coordinates\": [1, 1]},\n",
    "                {\"type\": \"Point\", \"coordinates\": [2, 2]},\n",
    "            ],\n",
    "            \"datetime\": [pd.Timestamp(\"2023-01-01\"), pd.Timestamp(\"2023-01-02\")],\n",
    "            \"properties\": [{\"prop1\": \"value1\"}, {\"prop1\": \"value2\"}],\n",
    "        }\n",
    "        \n",
    "        # Create a STAC Catalog\n",
    "        catalog = pystac.Catalog.from_dict(\n",
    "            {\"type\": \"Catalog\", \"id\": \"acs\", \"stac_version\": \"1.0.0\"}\n",
    "        )\n",
    "        \n",
    "        # Convert DataFrame to STAC Items and add to Catalog\n",
    "        for index, row in df.iterrows():\n",
    "            item = pystac.Item(\n",
    "                id=row[\"id\"],\n",
    "                geometry=row[\"geometry\"],\n",
    "                datetime=row[\"datetime\"].to_pydatetime(),\n",
    "                properties=row[\"properties\"],\n",
    "            )\n",
    "            catalog.add_item(item)\n",
    "        \n",
    "        # Write the catalog to a file\n",
    "        catalog.normalize_hrefs(\"./pystac_data\")\n",
    "        catalog.save_object(pystac.Catalog, \"acs.json\")\n",
    "        \n",
    "        # Read STAC catalog into a DataFrame\n",
    "        # df_from_stac = df_from(catalog)\n",
    "        # print(df_from_stac)\n",
    "        \"\"\"\n",
    "        rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main routine that executes all code, does return a data frame of data for further analysis if desired.\n",
    "#\n",
    "#  @param (None)\n",
    "def process(inc_input_directory: str,\n",
    "            inc_input_pattern: str) -> {}:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    # variables\n",
    "    source_nc_list = []\n",
    "    source_filenames_list = []\n",
    "\n",
    "    # setup storage solution\n",
    "    create_storage_locations(inc_input_directory)\n",
    "\n",
    "\n",
    "    # identify target files\n",
    "    print(\"...marshaling data files:\")\n",
    "    target_directory = f\"{inc_input_directory}{os.sep}data\"\n",
    "    if os.path.isdir(target_directory):\n",
    "        for file in os.listdir(target_directory):\n",
    "            print(f\"......processing {file} from {target_directory}\")\n",
    "            filename, file_extension = os.path.splitext(file)\n",
    "            if file_extension.lower() in LOWER_EXTENSIONS:\n",
    "                if inc_input_pattern in file:\n",
    "                    source_filenames_list.append(os.path.join(target_directory, file))\n",
    "    else:\n",
    "        print(\n",
    "            \"Target directory ({target_directory}) does not exist, cannot continue execution.  Check your paths.\"\n",
    "        )\n",
    "        raise SystemError\n",
    "\n",
    "    source_filenames_list = sorted(source_filenames_list)\n",
    "    #for idx in range(0,ITERATIONS):\n",
    "    the_payload = initial_read(source_filenames_list)\n",
    "    \n",
    "    #process_numpy(the_payload)\n",
    "    #process_pandas(the_payload)\n",
    "    process_polars(the_payload)\n",
    "    #process_pytorch(the_payload)\n",
    "    #process_tensorflow(the_payload)\n",
    "    #process_zarr(the_payload)\n",
    "    #process_parquet(the_payload)\n",
    "    #disabled HDF output, very expensive\n",
    "    #process_dask(the_payload)\n",
    "    \n",
    "    #expensive execution\n",
    "    #process_xarray(source_filenames_list)\n",
    "\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Routine (call all other routines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Polars Apache Feather Write Execution time: 6.6219 seconds\n",
      "Force write everything committed successfully and then wait 1 second.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # note that this design now deviates from previous methods.\n",
    "    # Implementation will assume a single execution of a single PIID folder, scanning results and\n",
    "    # appending metrics to a single ASCII file as the code proceeds thus ensuring multi-processor, *nix driven execution.\n",
    "\n",
    "    start_t = perf_counter()\n",
    "    print(\"BEGIN PROGRAM\")\n",
    "\n",
    "    ############################################\n",
    "    # CONSTANTS\n",
    "    ############################################\n",
    "\n",
    "    # Semantic Versioning\n",
    "    VERSION_NAME = \"MLDATAREADY\"\n",
    "    VERSION_MAJOR = 0\n",
    "    VERSION_MINOR = 0\n",
    "    VERSION_RELEASE = 1\n",
    "\n",
    "    DATA_VERSION_RELEASE = \"-\".join(\n",
    "        [\n",
    "            str(VERSION_NAME),\n",
    "            str(VERSION_MAJOR),\n",
    "            str(VERSION_MINOR),\n",
    "            str(VERSION_RELEASE),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # OUTPUT EXTENSIONS\n",
    "    OUTPUT_PANDAS_EXT = \"pkl\"\n",
    "    OUTPUT_POLARS_EXT = \"feather\"\n",
    "    OUTPUT_NUMPY_EXT = \"npy\"\n",
    "    OUTPUT_TORCH_EXT = \"pt\"\n",
    "    OUTPUT_XARRAY_EXT = \"nc\"\n",
    "    OUTPUT_ZARR_EXT = \"zarr\"\n",
    "    OUTPUT_PARQUET_EXT = \"parquet\"\n",
    "    OUTPUT_TENSORFLOW_EXT = \"tf\"\n",
    "    OUTPUT_PYSTAC_EXT = \"psc\"\n",
    "    OUTPUT_DASK_EXT = \"dask\"\n",
    "    # location of our working files\n",
    "    # WORKING_FOLDER=\"/content/folderOnColab\"\n",
    "    WORKING_FOLDER = \"./folderOnColab\"\n",
    "    input_directory = \"./folderOnColab\"\n",
    "    output_directory = \"./folderOnColab\"\n",
    "\n",
    "    # Notebook Author details\n",
    "    AUTHOR_NAME = \"Christopher G Wood\"\n",
    "    GITHUB_USERNAME = \"christophergarthwood\"\n",
    "    AUTHOR_EMAIL = \"christopher.g.wood@gmail.com\"\n",
    "\n",
    "    # GEOSPATIAL NAMES\n",
    "    LAT_LNAME = \"latitude\"\n",
    "    LAT_SNAME = \"lat\"\n",
    "    LONG_LNAME = \"longitude\"\n",
    "    LONG_SNAME = \"lon\"\n",
    "    #PRODUCT_LNAME = \"chlor_a\"\n",
    "    #PRODUCT_SNAME = \"chlor_a\"\n",
    "    PRODUCT_LNAME = \"cld_amt\"\n",
    "    PRODUCT_SNAME = \"cld_amt\"\n",
    "\n",
    "    # PRODUCT_LNAME=\"salinity\"\n",
    "    # PRODUCT_SNAME=\"salinity\"\n",
    "\n",
    "    # Encoding\n",
    "    ENCODING = \"utf-8\"\n",
    "    os.environ[\"PYTHONIOENCODING\"] = ENCODING\n",
    "\n",
    "    BOLD_START = \"\\033[1m\"\n",
    "    BOLD_END = \"\\033[0;0m\"\n",
    "    TEXT_WIDTH = 77\n",
    "    AI_NUMPY_DATA_TYPE  = np.float32\n",
    "    AI_PANDAS_DATA_TYPE = \"float32\"\n",
    "    AI_TORCH_DATA_TYPE = torch.float32\n",
    "    AI_XARRAY_REMOVE_VARIABLES= [\"clwmr\", \"delz\", \"dpres\", \"dzdt\", \"grle\", \"hgtsfc\", \"icmr\", \"o3mr\", \"pressfc\", \"rwmr\", \"snmr\", \"spfh\", \"tmp\", \"ugrd\", \"vgrd\",]\n",
    "    # You can also adjust the verbosity by changing the value of TF_CPP_MIN_LOG_LEVEL:\n",
    "    #\n",
    "    # 0 = all messages are logged (default behavior)\n",
    "    # 1 = INFO messages are not printed\n",
    "    # 2 = INFO and WARNING messages are not printed\n",
    "    # 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "    TF_CPP_MIN_LOG_LEVEL_SETTING = 0\n",
    "\n",
    "    # Set the Seed for the experiment (ask me why?)\n",
    "    # seed the pseudorandom number generator\n",
    "    # THIS IS ESSENTIAL FOR CONSISTENT MODEL OUTPUT, remember these are random in nature.\n",
    "    # SEED_INIT = 7\n",
    "    # random.seed(SEED_INIT)\n",
    "    # tf.random.set_seed(SEED_INIT)\n",
    "    # np.random.seed(SEED_INIT)\n",
    "\n",
    "    DEBUG_STACKTRACE = 0\n",
    "    DEBUG_USING_GPU = 0   #no gpu utilization on 0, 1 is gpu utilization\n",
    "    NUM_PROCESSORS = 10\n",
    "    ITERATIONS = 10000\n",
    "\n",
    "    # make comparisons lower case and include wild card character at the end of each to catch anomalous file extensions like xlsx, etc.\n",
    "    EXTENSIONS = [\".nc\"]\n",
    "    LOWER_EXTENSIONS = [x.lower() for x in EXTENSIONS]\n",
    "\n",
    "    THE_DEVICE_NAME = \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
    "    if DEBUG_USING_GPU == 1:\n",
    "        THE_DEVICE_NAME = \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    # GPU Setup (for multiple GPU devices)\n",
    "    device = torch.cuda.current_device()\n",
    "\n",
    "    # softare watermark\n",
    "    lib_diagnostics()\n",
    "\n",
    "    # hardware specs\n",
    "    get_hardware_stats()\n",
    "\n",
    "    # download the data\n",
    "    #download_noaa()\n",
    "    #download_test()\n",
    "    \n",
    "    # - Core workhorse routine\n",
    "    process(input_directory, \"2025\")\n",
    "\n",
    "    # - Save the results\n",
    "    # save_output()\n",
    "\n",
    "    end_t = perf_counter()\n",
    "    print(\"END PROGRAM\")\n",
    "    print(f\"Elapsed time: {end_t - start_t}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "STEM-004_ComputerVision.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "033cb5a8c21c4ca5a13b5d03e8b78fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebb9ca01cf52473699f21f15c5ec7d34",
      "placeholder": "​",
      "style": "IPY_MODEL_689fe090492947608518efc9f3bd6d5d",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n          "
     }
    },
    "0d8988d435644a928829ef71435a3e83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1015b1ce380943d48c7a6386444c768a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f5405303114146a28663cb1c46db7beb",
       "IPY_MODEL_bdc74a6d20a041819b6fc6d1c3f85a25",
       "IPY_MODEL_033cb5a8c21c4ca5a13b5d03e8b78fa0"
      ],
      "layout": "IPY_MODEL_2b447186de7a4571821ca59295744ce3"
     }
    },
    "202c8798fdda4778bf09a2124c37a6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "usfs-ai-bootcamp",
       "usfa-ai-advanced-training",
       "I will setup my own"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Set Your Project:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_dd3de71c792f4762af3b280ac9b7f5e6",
      "style": "IPY_MODEL_fc3fc48bdebe4501998819fbda36152c"
     }
    },
    "2b447186de7a4571821ca59295744ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "31b012d7d41a4d06933aa38e8e3bd74a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "479233081fbc44c8afe9a044ebb95faf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "689fe090492947608518efc9f3bd6d5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad3b9f4855d541e49303c03fb9f07db7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Accept",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_f196621910824a00a97d06e5433cb294",
      "style": "IPY_MODEL_31b012d7d41a4d06933aa38e8e3bd74a",
      "tooltip": ""
     }
    },
    "bdc74a6d20a041819b6fc6d1c3f85a25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_202c8798fdda4778bf09a2124c37a6c7",
       "IPY_MODEL_ad3b9f4855d541e49303c03fb9f07db7"
      ],
      "layout": "IPY_MODEL_0d8988d435644a928829ef71435a3e83"
     }
    },
    "cf363ab273bf4eeba7fbea4d79ed3a64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd3de71c792f4762af3b280ac9b7f5e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebb9ca01cf52473699f21f15c5ec7d34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f196621910824a00a97d06e5433cb294": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5405303114146a28663cb1c46db7beb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf363ab273bf4eeba7fbea4d79ed3a64",
      "placeholder": "​",
      "style": "IPY_MODEL_479233081fbc44c8afe9a044ebb95faf",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n\n        <table><tr><td>\n            <span style=\"font-family: Tahoma;font-size: 18\">\n              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n              Please verify that you are in the appropriate project and that the:</br>\n              <center><code><b>PROJECT_ID</b></code> </br></center>\n              aligns with the Project Id in the upper left corner of this browser and that the location:\n              <center><code><b>LOCATION</b></code> </br></center>\n              aligns with the instructions provided.\n            </span>\n          </td></tr></table></br></br>\n\n    "
     }
    },
    "fc3fc48bdebe4501998819fbda36152c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
