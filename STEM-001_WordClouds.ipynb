{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg3iJooMQjWA"
      },
      "source": [
        "# Natural Language Processing (NLP)\n",
        "## Text Categorization\n",
        "\n",
        "+ **Natural Language Processing (NLP)** is a discipline within AI that combines computing with human communication.\n",
        "+ Applications are Speech Recognition, Speech Tagging, Machine Translation, Sentiment Analysis, Chatbots, Text Summarization and a multitude of other functions.\n",
        "\n",
        "\n",
        "### References:\n",
        "+ https://realpython.com/python-nltk-sentiment-analysis/\n",
        "+ https://www.nltk.org/howto/classify.html\n",
        "+ https://www.nltk.org/book/ch06.html\n",
        "+ https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
        "+ [NLTK Summary](https://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3)\n",
        "+ [How to summarize text with OpenAI and LangChain](https://medium.com/@johnidouglasmarangon/how-to-summarize-text-with-openai-and-langchain-e038fc922af)\n",
        "+ [Text Summary with OpenAI GPT-3 API](https://medium.com/muthoni-wanyoike/implementing-text-summarization-using-openais-gpt-3-api-dcd6be4f6933)\n",
        "    + [Improved Implementation with OpenAI](https://medium.com/@tanguyvans/how-to-summarize-long-texts-using-openai-improving-coherence-and-structure-d896c5510c45)\n",
        "+ [Text Summary with Multiple Tools](https://medium.com/analytics-vidhya/text-summarization-using-bert-gpt2-xlnet-5ee80608e961)\n",
        "+ [Text Summary with Llama2](https://medium.com/@tushitdavergtu/llama2-and-text-summarization-e3eafb51fe28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbr46-4B_g--",
        "tags": []
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME =\"cio-training-vertex-colab\"\n",
        "PROJECT_ID  =\"usfs-ai-bootcamp\"\n",
        "LOCATION    = \"us-central1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shY7a4DVQjWB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#- Google Colab Check\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import datetime\n",
        "\n",
        "RunningInCOLAB = False\n",
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "current_time   = datetime.datetime.now()\n",
        "\n",
        "if RunningInCOLAB:\n",
        "    print(f\"You are running this notebook in Google Colab at {current_time} in the {PROJECT_ID} lab.\")\n",
        "else:\n",
        "    print(f\"You are likely running this notebook with Jupyter iPython runtime at {current_time} in the {PROJECT_ID} lab.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDEc0UzfOVNw"
      },
      "source": [
        "## Library Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlsjoFw5QjWC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib.util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqob1J7RQjWD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "libraries=[\"nltk\", \"bs4\", \"wordcloud\", \"pathlib\", \"numpy\", \"Pillow\", \"pandas\", \"seaborn\"]\n",
        "import importlib.util\n",
        "\n",
        "for library in libraries:\n",
        "    if library == \"Pillow\":\n",
        "      spec = importlib.util.find_spec(\"PIL\")\n",
        "    else:\n",
        "      spec = importlib.util.find_spec(library)\n",
        "    if spec is None:\n",
        "      print(\"Installing library \" + library)\n",
        "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"])\n",
        "    else:\n",
        "      print(\"Library \" + library + \" already installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vadEsUuQjWD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#- Import Natural Language Processing (NLP) specific libs\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer  # A word stemmer based on the Porter stemming algorithm.  Porter, M. \"An algorithm for suffix stripping.\" Program 14.3 (1980): 130-137.\n",
        "from nltk import pos_tag\n",
        "from nltk.tree import tree\n",
        "from nltk import FreqDist\n",
        "\n",
        "#from nltk.book import * #<- Large Download, only pull if you want raw material to work with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJuXEPlkSo9p",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# More NLP specific libraries\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import nltk\n",
        "from nltk.stem import *\n",
        "\n",
        "from bs4 import BeautifulSoup                 #used to parse the text\n",
        "from wordcloud import WordCloud, STOPWORDS    #custom library specifically designed to make word clouds\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# a set of libraries that perhaps should always be in Python source\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import os\n",
        "import socket\n",
        "import sys\n",
        "import getopt\n",
        "import inspect\n",
        "import warnings\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import itertools\n",
        "import datetime\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import io\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Additional libraries for this work\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import math\n",
        "from base64 import b64decode\n",
        "from IPython.display import Image\n",
        "import requests\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Data Science Libraries\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Graphics\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import PIL.ImageOps\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# progress bar\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shu7BuR6QjWC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#- NLTK required resources\n",
        "#- Required to load necessary files to support NLTK\n",
        "#- Downloads repository of knowledge to augment (this is the data portion) the library\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"words\")\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download('punkt_tab')\n",
        "#nltk.download(\"all\")  #<- Only do this if you want the full spectrum of all possible packages, it's a LOT!\n",
        "\n",
        "# Noun Part of Speech Tags used by NLTK\n",
        "# More can be found here\n",
        "# http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/\n",
        "#NOUNS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "#VERBS = ['VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdsZsLtONmOE"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InWUUhBiNpXT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Outputs library version history of effort.\n",
        "#\n",
        "def lib_diagnostics() -> None:\n",
        "\n",
        "    import pkg_resources\n",
        "\n",
        "    package_name_length=40\n",
        "    package_version_length=20\n",
        "\n",
        "    # Get installed packages\n",
        "    the_packages=[\"nltk\", \"numpy\", \"os\", \"pandas\", \"seaborn\"]\n",
        "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
        "    for package_idx, package_name in enumerate(installed):\n",
        "         if package_name in the_packages:\n",
        "             installed_version = installed[package_name]\n",
        "             print(f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
        "        print(f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\")\n",
        "        print(f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        print(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
        "        print(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
        "        print(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
        "        print(f\"{'     current':<40}#: {torch.cuda.current_device()}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "\n",
        "    try:\n",
        "      print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
        "    except Exception as e:\n",
        "      pass\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sVS7OY9CT69"
      },
      "source": [
        "## Function Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwOiLPSGCVud",
        "tags": []
      },
      "outputs": [],
      "source": [
        "lib_diagnostics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JMSTY2QjWD"
      },
      "source": [
        "# Input Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1AKqaU6akMe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "#- API Parameters for things like WordCloud\n",
        "#- Variables help hold information for later use\n",
        "#- The \"constants\" represent variables that we don't anticipate changing over the course of the program.\n",
        "###########################################\n",
        "IMG_BACKGROUND=None                             #None without quotes or \"black\", \"white\", etc...\n",
        "IMG_FONT_SIZE_MIN=10\n",
        "IMG_WIDTH=1024\n",
        "IMG_HEIGHT=768\n",
        "FIGURE_WIDTH=11\n",
        "FIGURE_HEIGHT=8.5\n",
        "WORD_FREQ=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQeN6mr_GS1Z",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!rm -rf ./folderOnColab && echo \"Ok, removed.\" || { echo \"No folder to remove.\"; exit 1; }\n",
        "#!mkdir -p ./folderOnColab && echo \"Folder created.\" || { echo \"Failed to create folder, it might already exist.\";  }\n",
        "#!gsutil -m cp -r gs://usfs-gcp-rand-test-data-usc1/public_source/jbooks/ANewHope.txt ./folderOnColab\n",
        "\n",
        "target_folder=\"./folderOnColab\"\n",
        "target_files=[\"ANewHope.txt\", \"slf*.txt\", \"alb*.txt\"]\n",
        "print(f\"Creating a folder ({target_folder}) to store project data.\")\n",
        "subprocess.run([\"mkdir\", \"-p\" , target_folder])\n",
        "if os.path.isdir(target_folder):\n",
        "  for idx, filename in enumerate(target_files):\n",
        "    print(f\"Copying {filename} to target folder: {target_folder}\")\n",
        "    subprocess.run([\"gsutil\", \"-m\" , \"cp\", \"-r\", f\"gs://{BUCKET_NAME}/training-data/jbooks/{filename}\",  target_folder], check=True)\n",
        "else:\n",
        "    print(\"ERROR: Local folder not found/created.  Check the output to ensure your folder is created.\")\n",
        "    print(f\"...target folder: {target_folder}\")\n",
        "    print(\"...if you can't find the problem contact the instructor.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9I0VKVILoAZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "data=\"\"\n",
        "\n",
        "#select the filename you want to process your body of text from: ANewHope.txt, slf_final_wordcloud_content.txt, alb_final_wordcloud_content.txt\n",
        "target_filename=target_folder+os.sep+\"slf_final_wordcloud_content.txt\"          #<- Change here\n",
        "\n",
        "\n",
        "#check for the file's existence\n",
        "if os.path.isfile(target_filename):\n",
        "  #open the file, read the contents and close the file\n",
        "  f = open(target_filename, \"r\", encoding=\"cp1252\")\n",
        "  data=f.read()\n",
        "  f.close()\n",
        "else:\n",
        "    print(\"ERROR: File not found.  Check the previous code block to ensure you file copied.\")\n",
        "    print(f\"...target file: {target_filename}\")\n",
        "    print(\"...if you can't find the problem contact the instructor.\")\n",
        "\n",
        "if len(data)<1:\n",
        "    print(\"ERROR: There is no content in your data variable.\")\n",
        "    print(\"...Verify you copied the input file correctly.\")\n",
        "    print(\"...if you can't find the problem contact the instructor.\")\n",
        "else:\n",
        "    print(f\"It appears your data file was read, your data file has {len(data):,} elements of data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BysXtCJLa_K"
      },
      "source": [
        "\n",
        "# *Natural Language Processing Methods *\n",
        "\n",
        "+ **Tokenization** is a fundamental task that involves breaking down a piece of text into smaller units called tokens. These tokens can be individual characters, entire words, or even subwold pieces, depending on the tokenization method used.\n",
        "+ **Stop words** are inconsequential words that appear frequently in a language but don’t contribute much to the overall understanding of a sentence or document. They often include articles, prepositions, conjunctions, and common verbs.\n",
        "+ **Stemming** is a text preprocessing technique that involves reducing words to their base or root form, known as stems.\n",
        "+ **Lemmatization** aims to identify and convert words to their base forms by considering their grammatical properties and meanings.\n",
        "\n",
        "\n",
        "***Why might a Data Scientist use NLP techniques on a body of text when performing AI work?***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PHVPawdQjWE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "#- Demonstrate use of tokens and stopwords\n",
        "###########################################\n",
        "\n",
        "#Perform a tokenization at the sentence level of the data.\n",
        "response=sent_tokenize(data)\n",
        "print(f\"There are {len(response)} sentences.\")\n",
        "\n",
        "#extra code to debug and see how the output looks.\n",
        "#for the_index, the_sentence in enumerate(response):\n",
        "#    print(f\"{the_index}. {the_sentence.strip()}\")\n",
        "#\n",
        "#print(\"\\n\")\n",
        "\n",
        "#Perform a tokenization at the word level of the data.\n",
        "response=word_tokenize(data)\n",
        "print(f\"There are {len(response)} words.\")\n",
        "\n",
        "#apply stop words to remove inconsequential words that appear frequently but don't influence the overall understanding of the setences.\n",
        "#gather the stop words for the NLTK library into a variable\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "#create a list data structure that will hold the resulting words\n",
        "filtered_list = []\n",
        "#break the overall data into \"word\" tokens after making everything lowercase (why would we do that?  Ask the instructor?)\n",
        "word_token_response=word_tokenize(data.lower())\n",
        "\n",
        "#Python \"lamba\" expression, very efficient for loop, used to continue normalizing the data by only allowing alpha characters that are equal to or greater than 2.\n",
        "wordlist = [x for x in word_token_response if (len(x)>=2 and x.isalpha())]\n",
        "\n",
        "#loop through each word in the wordlist and verify that it is not a stop word.  if the word is not a stop word, save it for later use.\n",
        "for word in tqdm(wordlist):\n",
        "    if word.casefold() not in stop_words:\n",
        "         filtered_list.append(word)\n",
        "\n",
        "print(f\"\\nThere are {len(filtered_list)} remaining words after cleaning them up.\")\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(f\"Word Frequency (top {WORD_FREQ} most used words)\")\n",
        "fq=FreqDist(filtered_list)\n",
        "print(fq.most_common(WORD_FREQ))\n",
        "\n",
        "#let's plot the most 10 common words\n",
        "print(\"\")\n",
        "\n",
        "## Creating FreqDist for whole BoW, keeping the 20 most common tokens\n",
        "all_fdist = FreqDist(filtered_list).most_common(WORD_FREQ)\n",
        "\n",
        "## Conversion to Pandas series via Python Dictionary for easier plotting\n",
        "all_fdist = pd.Series(dict(all_fdist))\n",
        "\n",
        "## Setting figure, ax into variables\n",
        "fig, ax = plt.subplots(figsize=(FIGURE_WIDTH,FIGURE_HEIGHT))\n",
        "\n",
        "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
        "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
        "plt.xticks(rotation=30);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of Stem and Lemmatization"
      ],
      "metadata": {
        "id": "4oGbe8IT1oZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables for NLTK specific functions\n",
        "wnl = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Example inflections to reduce\n",
        "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
        "\n",
        "# Perform stemming and lemmatization to show the differences\n",
        "print(\"{0:20}{1:20}{2:20}\".format(\"--Word--\",\"--Stem--\",\"--Lemma--\"))\n",
        "\n",
        "for word in example_words:\n",
        "   print (\"{0:20}{1:20}{2:20}\".format(word, stemmer.stem(word), wnl.lemmatize(word, pos=\"v\")))"
      ],
      "metadata": {
        "id": "0F6-Ztyh1tqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stem and Lemmatization In Depth\n",
        "\n",
        "## Stemming\n",
        "\n",
        "**Stemming** is a technique used to reduce an inflected word down to its word stem. For example, the words “programming,” “programmer,” and “programs” can all be reduced down to the common word stem “program.” In other words, “program” can be used as a synonym for the prior three inflection words.\n",
        "\n",
        "Performing this text-processing technique is often useful for dealing with sparsity and/or standardizing vocabulary. Not only does it help with reducing redundancy, as most of the time the word stem and their inflected words have the same meaning, it also allows NLP models to learn links between inflected words and their word stem, which helps the model understand their usage in similar contexts.\n",
        "\n",
        "**Stemming** algorithms function by taking a list of frequent prefixes and suffixes found in inflected words and chopping off the end or beginning of the word. This can occasionally result in word stems that are not real words; thus, we can affirm this approach certainly has its pros, but it’s not without its limitations.\n",
        "\n",
        "### Advantages of Stemming\n",
        "Improved model performance: Stemming reduces the number of unique words that need to be processed by an algorithm, which can improve its performance. Additionally, it can also make the algorithm run faster and more efficiently.\n",
        "Grouping similar words: Words with a similar meaning can be grouped together, even if they have distinct forms. This can be a useful technique in tasks such as document classification, where it’s important to identify key topics or themes within a document.\n",
        "\n",
        "Easier to analyze and understand: Since stemming typically reduces the size of the vocabulary, it’s much easier to analyze, compare, and understand texts. This is helpful in tasks such as sentiment analysis, where the goal is to determine the sentiment of a document.\n",
        "\n",
        "### Disadvantages of Stemming\n",
        "\n",
        "Overstemming / False positives: This is when a stemming algorithm reduces separate inflected words to the same word stem even though they are not related; for example, the Porter Stemmer algorithm stems  \"universal\", \"university\", and \"universe\" to the same word stem. Though they are etymologically related, their meanings in the modern day are from widely different domains. Treating them as synonyms will reduce relevance in search results.\n",
        "\n",
        "Understemming / False negatives: This is when a stemming algorithm reduces inflected words to different word stems, but they should be the same. For example, the Porter Stemmer algorithm does not reduce the words “alumnus,” “alumnae,” and “alumni” to the same word stem, although they should be treated as synonyms.\n",
        "\n",
        "Language challenges: As the target language's morphology, spelling, and character encoding get more complicated, stemmers become more difficult to design; For example, an Italian stemmer is more complicated than an English stemmer because there is a higher number of verb inflections. A Russian stemmer is even more complex due to more noun declensions.\n",
        "\n",
        "## Lemmatization\n",
        "\n",
        "**Lemmatization** is another technique used to reduce inflected words to their root word. It describes the algorithmic process of identifying an inflected word’s “lemma” (dictionary form) based on its intended meaning.\n",
        "\n",
        "As opposed to stemming, **lemmatization** relies on accurately determining the intended part-of-speech and the meaning of a word based on its context. This means it takes into consideration where the inflected word falls within a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document.\n",
        "\n",
        "“Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma”\n",
        "\n",
        "In other words, to lemmatize a document typically means to “doing things correctly” since it involves using a vocabulary and performing morphological analysis of words to remove only the inflectional ends and return the base or dictionary form of a word, which is known as the “lemma.” For example, you can expect a lemmatization algorithm to map “runs,” “running,” and “ran” to the lemma, “run.”  \n",
        "\n",
        "### Advantages of Lemmatization\n",
        "\n",
        "Accuracy: Lemmatization does not merely cut words off as you see in stemming algorithms. Analysis of words is conducted based on the word’s POS to take context into consideration when producing lemmas. Also, lemmatization leads to real dictionary words being produced.\n",
        "\n",
        "### Disadvantages of Lemmatization\n",
        "\n",
        "Time-consuming: Compared to stemming, lemmatization is a slow and time-consuming process. This is because lemmatization involves performing morphological analysis and deriving the meaning of words from a dictionary."
      ],
      "metadata": {
        "id": "ZT_efe3G1Exk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Icd3yDQjWF"
      },
      "source": [
        "# Cloud Map with Stop words applied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnjA2tpZQjWF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "#API Call (resultant is an image)\n",
        "########################################\n",
        "try:\n",
        "    #invoke the word cloud API using the original list of lowercase, alpha, > 2 character words.\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                          background_color=IMG_BACKGROUND,\n",
        "                          min_font_size=IMG_FONT_SIZE_MIN,\n",
        "                          width=IMG_WIDTH,\n",
        "                          height=IMG_HEIGHT,\n",
        "                         ).generate(\" \".join(wordlist))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying invoke the WordCloud call as follows: {str(e)}\")\n",
        "\n",
        "########################################\n",
        "#Show the Results\n",
        "########################################\n",
        "#turn the image into a displayed graphic\n",
        "plt.figure(figsize=(FIGURE_WIDTH,FIGURE_HEIGHT))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrjQnydJQjWF"
      },
      "source": [
        "# Cloud Map without Stop Words Applied to Original Body of Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELhTTfZkQjWF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    wordcloud = WordCloud(#stopwords=STOPWORDS,\n",
        "                          background_color='white',                #options line \"None\" yield transparent background.\n",
        "                          min_font_size=IMG_FONT_SIZE_MIN,\n",
        "                          width=IMG_WIDTH,\n",
        "                          height=IMG_HEIGHT,\n",
        "                         ).generate(data)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying invoke the WordCloud() call as follows: {str(e)}\")\n",
        "########################################\n",
        "#Show the Results\n",
        "########################################\n",
        "#turn the image into a displayed graphic\n",
        "plt.figure(figsize=(FIGURE_WIDTH,FIGURE_HEIGHT))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK386XTKFCOw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "target_folder=\"./folderOnColab\"\n",
        "masks=[\"usfs_mask.jpg\", \"slf_mask.jpg\", \"force_mask.jpg\", \"kitty_mask.jpg\", \"pine_tree_mask.png\", \"paw_mask.jpg\"]\n",
        "if os.path.isdir(target_folder):\n",
        "  for idx, filename in enumerate(masks):\n",
        "    print(f\"Copying {filename} to target folder: {target_folder}\")\n",
        "    subprocess.run([\"gsutil\", \"-m\" , \"cp\", \"-r\", f\"gs://{BUCKET_NAME}/training-data/llm/masks/{filename}\",  target_folder], check=True)\n",
        "else:\n",
        "    print(\"ERROR: Local folder not found/created.  Check the output to ensure your folder is created.\")\n",
        "    print(f\"...target folder: {target_folder}\")\n",
        "    print(\"...if you can't find the problem contact the instructor.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4elpHP-uTX-F",
        "tags": []
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "#- Images, Mask loading and setup\n",
        "###########################################\n",
        "#setup the plot mechanism to show the images.\n",
        "# Adds a subplot at the 1st position\n",
        "rows=1\n",
        "cols=6\n",
        "position=1\n",
        "img_ary=[]\n",
        "fig = plt.figure(figsize=(FIGURE_WIDTH, FIGURE_HEIGHT))\n",
        "#fig = plt.figure()\n",
        "\n",
        "\n",
        "#loop through the lists and print them side by side\n",
        "for idx, filename in enumerate(masks):\n",
        "    print(f\"Processing...{filename}\")\n",
        "    fig.add_subplot(rows,cols, position)\n",
        "    img_ary.append(Image.open(target_folder+os.sep+filename))\n",
        "    # showing image\n",
        "    plt.imshow(img_ary[idx])\n",
        "    plt.axis('off')\n",
        "    plt.title(os.path.basename(filename))\n",
        "    # Adds a subplot at the next position\n",
        "    position += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV2_6CMDQjWF"
      },
      "source": [
        "# With a Mask\n",
        "\n",
        "Note that the \"mask\" is effectively an image of same size that's black and white.  Blacked out portions become the area you can populate so I took the USFS logo, inverted so the white becomes black and filled in the verbiage there.\n",
        "\n",
        "Next I paste the transparent PNG on top of the original logo thus putting the words in the middle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfuA9qTnQjWF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "#Mask loading and setup\n",
        "########################################\n",
        "\n",
        "#choose a different mask if you want different results, options are: usfs_mask.jpg, slf_mask.jpg, force_mask.jpg\n",
        "target_filename=\"kitty_mask.jpg\"                        #<- change here\n",
        "the_logo=Image.open(target_folder+os.sep+target_filename)\n",
        "\n",
        "try:\n",
        "    #invert the colors of the mask image, so that the image is \"reversed\"\n",
        "    inverted_image = PIL.ImageOps.invert(the_logo)\n",
        "    # Convert the image to a numeric representation (a 3D array)\n",
        "    the_mask = np.array(inverted_image)\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying read the mask image and invert the colors call as follows: {str(e)}\")\n",
        "\n",
        "\n",
        "########################################\n",
        "#Data Marshaling, NLP setup\n",
        "########################################\n",
        "data = \" \".join(filtered_list)\n",
        "\n",
        "########################################\n",
        "#API Call\n",
        "########################################\n",
        "try:\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                      mode=\"RGBA\",\n",
        "                      background_color=IMG_BACKGROUND, #white, black, blue, etc\n",
        "                      min_font_size=IMG_FONT_SIZE_MIN,\n",
        "                      width=IMG_WIDTH,\n",
        "                      height=IMG_HEIGHT,\n",
        "                      mask=the_mask,\n",
        "                     ).generate(\" \".join(wordlist))\n",
        "    the_image= wordcloud.to_image()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying invoke the WordCloud() call as follows: {str(e)}\")\n",
        "\n",
        "#turn the image into a displayed graphic\n",
        "plt.figure(figsize=(8.5,11))\n",
        "plt.axis('off')\n",
        "plt.imshow(the_image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeoxHrHNQjWF"
      },
      "source": [
        "# Merge the Images into a single instance\n",
        "\n",
        "You might want to try another image and text block on your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KltRE5VQjWG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# creating a image object (main image)\n",
        "im1=the_logo\n",
        "im2=the_image\n",
        "alpha = im2.getchannel('A')\n",
        "\n",
        "#transparency save\n",
        "im1.paste(im2, (0,0), im2)\n",
        "\n",
        "the_final_image=im1\n",
        "plt.figure(figsize=(8.5,11))\n",
        "plt.axis('off')\n",
        "plt.imshow(the_final_image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "## Utilize NLP techniques to process your own data\n",
        "\n",
        "+ Select one of the data filenames and read-in a different filename.\n",
        "+ Observe the word frequency, stop word influence on size of data, stemming, and lemmatization results.\n",
        "+ Observe the differences in the Word Cloud\n",
        "+ Change the mask used to a different one and create a final output.\n",
        "\n",
        "## Uber-Challenge\n",
        "+ Load the Spotted Lantern Fly data file and the Eastern LongHorn Beetle data files either separately or joined together.\n",
        "+ Repeat the experiment to see the influence of the new data.\n",
        "\n",
        "## Pro-Tip:\n",
        "\n",
        "Either create a new block of code that reads the 2nd file and combine the resulting variables `data = data_slf + \" \" + data_alb` or create an array, iterate through the target files and combine them."
      ],
      "metadata": {
        "id": "gD0bf7_r4UWW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "STEM-001_WordClouds.ipynb",
      "provenance": [
        {
          "file_id": "https://github.com/christophergarthwood/jbooks/blob/main/STEM-001_WordClouds.ipynb",
          "timestamp": 1716214402332
        }
      ]
    },
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m125",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}