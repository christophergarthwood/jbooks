{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg3iJooMQjWA"
      },
      "source": [
        "# Natural Language Processing (NLP)\n",
        "## Text Categorization\n",
        "\n",
        "+ **Natural Language Processing (NLP)** is a discipline within AI that combines computing with human communication.\n",
        "+ Applications are Speech Recognition, Speech Tagging, Machine Translation, Sentiment Analysis, Chatbots, Text Summarization and a multitude of other functions.\n",
        "\n",
        "\n",
        "### References:\n",
        "+ https://realpython.com/python-nltk-sentiment-analysis/\n",
        "+ https://www.nltk.org/howto/classify.html\n",
        "+ https://www.nltk.org/book/ch06.html\n",
        "+ https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
        "+ [NLTK Summary](https://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3)\n",
        "+ [How to summarize text with OpenAI and LangChain](https://medium.com/@johnidouglasmarangon/how-to-summarize-text-with-openai-and-langchain-e038fc922af)\n",
        "+ [Text Summary with OpenAI GPT-3 API](https://medium.com/muthoni-wanyoike/implementing-text-summarization-using-openais-gpt-3-api-dcd6be4f6933)\n",
        "    + [Improved Implementation with OpenAI](https://medium.com/@tanguyvans/how-to-summarize-long-texts-using-openai-improving-coherence-and-structure-d896c5510c45)\n",
        "+ [Text Summary with Multiple Tools](https://medium.com/analytics-vidhya/text-summarization-using-bert-gpt2-xlnet-5ee80608e961)\n",
        "+ [Text Summary with Llama2](https://medium.com/@tushitdavergtu/llama2-and-text-summarization-e3eafb51fe28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Hbr46-4B_g--"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME=\"ai-training-2024-08-09-bucket\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shY7a4DVQjWB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#- Google Colab Check\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "RunningInCOLAB = False\n",
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if RunningInCOLAB:\n",
        "    print(\"You are running this notebook in Google Colab.\")\n",
        "else:\n",
        "    print(\"You are running this notebook with Jupyter iPython runtime.\")\n",
        "    print(\"Assumption is you have the required libraries to execute this notebook.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDEc0UzfOVNw"
      },
      "source": [
        "## Library Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlsjoFw5QjWC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib.util"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "_8-m1w0z_4wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqob1J7RQjWD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "libraries=[\"nltk\", \"bs4\", \"wordcloud\", \"pathlib\", \"numpy\", \"Pillow\"]\n",
        "import importlib.util\n",
        "\n",
        "for library in libraries:\n",
        "    if library == \"Pillow\":\n",
        "      spec = importlib.util.find_spec(\"PIL\")\n",
        "    else:\n",
        "      spec = importlib.util.find_spec(library)\n",
        "    if spec is None:\n",
        "      print(\"Installing library \" + library)\n",
        "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"])\n",
        "    else:\n",
        "      print(\"Library \" + library + \" already installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vadEsUuQjWD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#- Import Natural Language Processing (NLP) specific libs\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer  # A word stemmer based on the Porter stemming algorithm.  Porter, M. \"An algorithm for suffix stripping.\" Program 14.3 (1980): 130-137.\n",
        "from nltk import pos_tag\n",
        "from nltk.tree import tree\n",
        "from nltk import FreqDist\n",
        "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#from nltk.book import * #<- Large Download, only pull if you want raw material to work with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJuXEPlkSo9p",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# More NLP specific libraries\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import nltk\n",
        "from nltk.stem import *\n",
        "\n",
        "from bs4 import BeautifulSoup                 #used to parse the text\n",
        "from wordcloud import WordCloud, STOPWORDS    #custom library specifically designed to make word clouds\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# a set of libraries that perhaps should always be in Python source\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import os\n",
        "import socket\n",
        "import sys\n",
        "import getopt\n",
        "import inspect\n",
        "import warnings\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import itertools\n",
        "import datetime\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import io\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Additional libraries for this work\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import math\n",
        "from base64 import b64decode\n",
        "from IPython.display import Image\n",
        "import requests\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Data Science Libraries\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import numpy as np\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Graphics\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import PIL.ImageOps\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# progress bar\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shu7BuR6QjWC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#- NLTK required resources\n",
        "#- Required to load necessary files to support NLTK\n",
        "#- Downloads repository of knowledge to augment (this is the data portion) the library\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"words\")\n",
        "nltk.download('punkt')\n",
        "#nltk.download(\"all\")  #<- Only do this if you want the full spectrum of all possible packages, it's a LOT!\n",
        "\n",
        "# Noun Part of Speech Tags used by NLTK\n",
        "# More can be found here\n",
        "# http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/\n",
        "NOUNS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "VERBS = ['VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdsZsLtONmOE"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InWUUhBiNpXT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Outputs library version history of effort.\n",
        "#\n",
        "def lib_diagnostics() -> None:\n",
        "\n",
        "    import pkg_resources\n",
        "\n",
        "    package_name_length=40\n",
        "    package_version_length=20\n",
        "\n",
        "    # Get installed packages\n",
        "    the_packages=[\"nltk\", \"numpy\", \"os\", \"pandas\"]\n",
        "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
        "    for package_idx, package_name in enumerate(installed):\n",
        "         if package_name in the_packages:\n",
        "             installed_version = installed[package_name]\n",
        "             print(f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
        "        print(f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\")\n",
        "        print(f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        print(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
        "        print(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
        "        print(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
        "        print(f\"{'     current':<40}#: {torch.cuda.current_device()}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "\n",
        "    try:\n",
        "      print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
        "    except Exception as e:\n",
        "      pass\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sVS7OY9CT69"
      },
      "source": [
        "## Function Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwOiLPSGCVud",
        "tags": []
      },
      "outputs": [],
      "source": [
        "lib_diagnostics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JMSTY2QjWD"
      },
      "source": [
        "# Input Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1AKqaU6akMe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "#- API Parameters for things like WordCloud\n",
        "#- Variables help hold information for later use\n",
        "#- The \"constants\" represent variables that we don't anticipate changing over the course of the program.\n",
        "###########################################\n",
        "IMG_BACKGROUND=None                             #None without quotes or \"black\", \"white\", etc...\n",
        "IMG_FONT_SIZE_MIN=10\n",
        "IMG_WIDTH=1024\n",
        "IMG_HEIGHT=768\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2j8L6-DQjWD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "#- SETUP ACCESS TO THE PUBLIC BUCKET\n",
        "###########################################\n",
        "#https://colab.research.google.com/notebooks/io.ipynb?authuser=2#scrollTo=r-exJtdG3XwJ\n",
        "#https://colab.research.google.com/notebooks/snippets/drive.ipynb\n",
        "\n",
        "if RunningInCOLAB:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "else:\n",
        "    print(\"You are running this notebook with Jupyter iPython runtime.\")\n",
        "    print(\"Assumption is you have the required libraries to execute this notebook.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQeN6mr_GS1Z",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!rm -rf ./folderOnColab && echo \"Ok, removed.\" || { echo \"No folder to remove.\"; exit 1; }\n",
        "#!mkdir -p ./folderOnColab && echo \"Folder created.\" || { echo \"Failed to create folder, it might already exist.\";  }\n",
        "#!gsutil -m cp -r gs://usfs-gcp-rand-test-data-usc1/public_source/jbooks/ANewHope.txt ./folderOnColab\n",
        "\n",
        "target_folder=\"./folderOnColab\"\n",
        "target_files=[\"ANewHope.txt\", \"slf*.txt\", \"alb*.txt\"]\n",
        "print(f\"Creating a folder ({target_folder}) to store project data.\")\n",
        "subprocess.run([\"mkdir\", \"-p\" , target_folder])\n",
        "if os.path.isdir(target_folder):\n",
        "  for idx, filename in enumerate(target_files):\n",
        "    print(f\"Copying {filename} to target folder: {target_folder}\")\n",
        "    subprocess.run([\"gsutil\", \"-m\" , \"cp\", \"-r\", f\"gs://{PROJECT_NAME}/public_source/jbooks/{filename}\",  target_folder], check=True)\n",
        "else:\n",
        "    print(\"ERROR: Local folder not found/created.  Check the output to ensure your folder is created.\")\n",
        "    print(f\"...target folder: {target_folder}\")\n",
        "    print(\"...if you can't find the problem contact the instructor.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9I0VKVILoAZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "data=\"\"\n",
        "\n",
        "#select the filename you want to process your body of text from: ANewHope.txt, slf_final_wordcloud_content.txt, alb_final_wordcloud_content.txt\n",
        "target_filename=target_folder+os.sep+\"slf_final_wordcloud_content.txt\"          #<- Change here\n",
        "\n",
        "\n",
        "#check for the file's existence\n",
        "if os.path.isfile(target_filename):\n",
        "  #open the file, read the contents and close the file\n",
        "  f = open(target_filename, \"r\", encoding=\"cp1252\")\n",
        "  data=f.read()\n",
        "  f.close()\n",
        "else:\n",
        "    print(\"ERROR: File not found.  Check the previous code block to ensure you file copied.\")\n",
        "    print(f\"...target file: {target_filename}\")\n",
        "    print(\"...if you can't find the problem contact the instructor.\")\n",
        "\n",
        "if len(data)<1:\n",
        "    print(\"ERROR: There is no content in your data variable.\")\n",
        "    print(\"...Verify you copied the input file correctly.\")\n",
        "    print(\"...if you can't find the problem contact the instructor.\")\n",
        "else:\n",
        "    print(f\"It appears your data file was read, your data file has {len(data):,} elements of data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BysXtCJLa_K"
      },
      "source": [
        "\n",
        "# *Natural Language Processing Methods *\n",
        "\n",
        "+ **Tokenization** is a fundamental task that involves breaking down a piece of text into smaller units called tokens. These tokens can be individual characters, entire words, or even subwold pieces, depending on the tokenization method used.\n",
        "+ **Stop words** are inconsequential words that appear frequently in a language but donâ€™t contribute much to the overall understanding of a sentence or document. They often include articles, prepositions, conjunctions, and common verbs.\n",
        "+ **Stemming** is a text preprocessing technique that involves reducing words to their base or root form, known as stems.\n",
        "+ **Lemmatization** aims to identify and convert words to their base forms by considering their grammatical properties and meanings.\n",
        "\n",
        "\n",
        "***Why might a Data Scientist use NLP techniques on a body of text when performing AI work?***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PHVPawdQjWE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "#- Demonstrate use of tokens and stopwords\n",
        "###########################################\n",
        "\n",
        "#Perform a tokenization at the sentence level of the data.\n",
        "response=sent_tokenize(data)\n",
        "print(f\"There are {len(response)} sentences.\")\n",
        "\n",
        "#extra code to debug and see how the output looks.\n",
        "#for the_index, the_sentence in enumerate(response):\n",
        "#    print(f\"{the_index}. {the_sentence.strip()}\")\n",
        "#\n",
        "#print(\"\\n\")\n",
        "\n",
        "#Perform a tokenization at the word level of the data.\n",
        "response=word_tokenize(data)\n",
        "print(f\"There are {len(response)} words.\")\n",
        "\n",
        "#apply stop words to remove inconsequential words that appear frequently but don't influence the overall understanding of the setences.\n",
        "#gather the stop words for the NLTK library into a variable\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "#create a list data structure that will hold the resulting words\n",
        "filtered_list = []\n",
        "#break the overall data into \"word\" tokens after making everything lowercase (why would we do that?  Ask the instructor?)\n",
        "response=word_tokenize(data.lower())\n",
        "\n",
        "#Python \"lamba\" expression, very efficient for loop, used to continue normalizing the data by only allowing alpha characters that are equal to or greater than 2.\n",
        "wordlist = [x for x in response if (len(x)>=2 and x.isalpha())]\n",
        "\n",
        "#loop through each word in the wordlist and verify that it is not a stop word.  if the word is not a stop word, save it for later use.\n",
        "for word in tqdm(wordlist):\n",
        "    if word.casefold() not in stop_words:\n",
        "         filtered_list.append(word)\n",
        "\n",
        "print(f\"\\nThere are {len(filtered_list)} remaining words after cleaning them up.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Icd3yDQjWF"
      },
      "source": [
        "# Cloud Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnjA2tpZQjWF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "#API Call (resultant is an image)\n",
        "########################################\n",
        "try:\n",
        "    #invoke the word cloud API using the original list of lowercase, alpha, > 2 character words.\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                          background_color=IMG_BACKGROUND,\n",
        "                          min_font_size=IMG_FONT_SIZE_MIN,\n",
        "                          width=IMG_WIDTH,\n",
        "                          height=IMG_HEIGHT,\n",
        "                         ).generate(\" \".join(wordlist))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying invoke the WordCloud call as follows: {str(e)}\")\n",
        "\n",
        "########################################\n",
        "#Show the Results\n",
        "########################################\n",
        "#turn the image into a displayed graphic\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrjQnydJQjWF"
      },
      "source": [
        "# Cloud Map with Stop Words Applied to Original Body of Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELhTTfZkQjWF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "#Data Marshaling\n",
        "########################################\n",
        "data = \" \".join(filtered_list)\n",
        "\n",
        "########################################\n",
        "#Basic Clean-Up\n",
        "########################################\n",
        "try:\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                          background_color='white',                #options line \"None\" yield transparent background.\n",
        "                          min_font_size=IMG_FONT_SIZE_MIN,\n",
        "                          width=IMG_WIDTH,\n",
        "                          height=IMG_HEIGHT,\n",
        "                         ).generate(data)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying invoke the WordCloud() call as follows: {str(e)}\")\n",
        "########################################\n",
        "#Show the Results\n",
        "########################################\n",
        "#turn the image into a displayed graphic\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK386XTKFCOw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "target_folder=\"./folderOnColab\"\n",
        "masks=[\"usfs_mask.jpg\", \"slf_mask.jpg\", \"force_mask.jpg\"]\n",
        "if os.path.isdir(target_folder):\n",
        "  for idx, filename in enumerate(masks):\n",
        "    print(f\"Copying {filename} to target folder: {target_folder}\")\n",
        "    subprocess.run([\"gsutil\", \"-m\" , \"cp\", \"-r\", f\"gs://{PROJECT_NAME}/public_source/llm/masks/{filename}\",  target_folder], check=True)\n",
        "else:\n",
        "    print(\"ERROR: Local folder not found/created.  Check the output to ensure your folder is created.\")\n",
        "    print(f\"...target folder: {target_folder}\")\n",
        "    print(\"...if you can't find the problem contact the instructor.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4elpHP-uTX-F",
        "tags": []
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "#- Images, Mask loading and setup\n",
        "###########################################\n",
        "#setup the plot mechanism to show the images.\n",
        "# Adds a subplot at the 1st position\n",
        "rows=1\n",
        "cols=3\n",
        "position=1\n",
        "img_ary=[]\n",
        "fig = plt.figure(figsize=(8.5,11))\n",
        "#fig = plt.figure()\n",
        "\n",
        "\n",
        "#loop through the lists and print them side by side\n",
        "for idx, filename in enumerate(masks):\n",
        "    print(f\"Processing...{filename}\")\n",
        "    fig.add_subplot(rows,cols, position)\n",
        "    img_ary.append(Image.open(target_folder+os.sep+filename))\n",
        "    # showing image\n",
        "    plt.imshow(img_ary[idx])\n",
        "    plt.axis('off')\n",
        "    plt.title(os.path.basename(filename))\n",
        "    # Adds a subplot at the next position\n",
        "    position += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV2_6CMDQjWF"
      },
      "source": [
        "# With a Mask\n",
        "\n",
        "Note that the \"mask\" is effectively an image of same size that's black and white.  Blacked out portions become the area you can populate so I took the USFS logo, inverted so the white becomes black and filled in the verbiage there.\n",
        "\n",
        "Next I paste the transparent PNG on top of the original logo thus putting the words in the middle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfuA9qTnQjWF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "#Mask loading and setup\n",
        "########################################\n",
        "\n",
        "#choose a different mask if you want different results, options are: usfs_mask.jpg, slf_mask.jpg, force_mask.jpg\n",
        "target_filename=\"slf_mask.jpg\"                        #<- change here\n",
        "the_logo=Image.open(target_folder+os.sep+target_filename)\n",
        "\n",
        "try:\n",
        "    #invert the colors of the mask image, so that the image is \"reversed\"\n",
        "    inverted_image = PIL.ImageOps.invert(the_logo)\n",
        "    # Convert the image to a numeric representation (a 3D array)\n",
        "    the_mask = np.array(inverted_image)\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying read the mask image and invert the colors call as follows: {str(e)}\")\n",
        "\n",
        "\n",
        "########################################\n",
        "#Data Marshaling, NLP setup\n",
        "########################################\n",
        "data = \" \".join(filtered_list)\n",
        "\n",
        "########################################\n",
        "#API Call\n",
        "########################################\n",
        "try:\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                      mode=\"RGBA\",\n",
        "                      background_color=IMG_BACKGROUND, #white, black, blue, etc\n",
        "                      min_font_size=IMG_FONT_SIZE_MIN,\n",
        "                      width=IMG_WIDTH,\n",
        "                      height=IMG_HEIGHT,\n",
        "                      mask=the_mask,\n",
        "                     ).generate(\" \".join(wordlist))\n",
        "    the_image= wordcloud.to_image()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying invoke the WordCloud() call as follows: {str(e)}\")\n",
        "\n",
        "#turn the image into a displayed graphic\n",
        "plt.figure(figsize=(8.5,11))\n",
        "plt.axis('off')\n",
        "plt.imshow(the_image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeoxHrHNQjWF"
      },
      "source": [
        "# Merge the Images into a single instance\n",
        "\n",
        "You might want to try another image and text block on your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KltRE5VQjWG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# creating a image object (main image)\n",
        "im1=the_logo\n",
        "im2=the_image\n",
        "alpha = im2.getchannel('A')\n",
        "\n",
        "#transparency save\n",
        "im1.paste(im2, (0,0), im2)\n",
        "\n",
        "the_final_image=im1\n",
        "plt.figure(figsize=(8.5,11))\n",
        "plt.axis('off')\n",
        "plt.imshow(the_final_image)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "STEM-001_WordClouds.ipynb",
      "provenance": [
        {
          "file_id": "https://github.com/christophergarthwood/jbooks/blob/main/STEM-001_WordClouds.ipynb",
          "timestamp": 1716214402332
        }
      ]
    },
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m123",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}