{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg3iJooMQjWA"
      },
      "source": [
        "# Natural Language Processing (NLP)\n",
        "## Machine Learning - 001\n",
        "\n",
        "<center>\n",
        "<table align=\"center\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/christophergarthwood/jbooks/blob/main/STEM-001_WordClouds.ipynb\">\n",
        "      <img src=\"https://github.com/christophergarthwood/jbooks/blob/GOOGLE_MODS/img/GoogleColab-logo.png?raw=1\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/notebooks?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Link to Colab Enterprise\n",
        "    </a>\n",
        "  </td>   \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/christophergarthwood/jbooks/blob/main/STEM-001_WordClouds.ipynb\">\n",
        "      <img src=\"https://github.com/christophergarthwood/jbooks/blob/GOOGLE_MODS/img/GitHub-logo.jpg?raw=1\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Link to Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "</center>\n",
        "</br></br></br>\n",
        "\n",
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Christopher G Wood](https://github.com/christophergarthwood)  |\n",
        "\n",
        "# Overview\n",
        "\n",
        "This notebook focuses on using  **Natural Language Process (NLP)** techniques to manipulate textual data in preparation for Prompt Engineering and interfacing with a Large Language Model (LLM).\n",
        "\n",
        "**Natural Language Processing (NLP)** is a machine learning technology that allows computers to understand, interpret, and manipulate human language. NLP is a branch of artificial intelligence (AI) and computer science that uses computational linguistics, statistics, machine learning, and deep learning to analyze and process text and voice data.\n",
        "\n",
        "NLP has many real-world applications, including:\n",
        "\n",
        "***Virtual assistants:*** NLP powers virtual assistants like Siri, Alexa, Cortana, and the Oracle Digital Assistant.\n",
        "\n",
        "***Search engines:*** NLP helps search engines understand the intent behind user queries, providing more relevant results.\n",
        "\n",
        "***Email spam filtering:*** NLP can identify spam in emails.\n",
        "\n",
        "***Translation apps:*** NLP can translate text or speech into foreign languages.\n",
        "\n",
        "***Document summarization:*** NLP can summarize documents.\n",
        "\n",
        "***Sentiment analysis:*** NLP can analyze the sentiment of text or speech.\n",
        "\n",
        "***Grammar and spell checking:*** NLP can check grammar and spelling.\n",
        "\n",
        "***Customer service bots:*** NLP can automate customer support tasks.\n",
        "\n",
        "NLP is closely related to computational linguistics, which is the study of how language works. NLP is also separate from speech recognition, which turns sound into text and vice versa.\n",
        "\n",
        "For more information, see the references below.\n",
        "\n",
        "### Reference for General Jupyter Ecosystem:\n",
        "+ https://www.dataquest.io/blog/jupyter-notebook-tutorial/\n",
        "+ https://jupyter.org/\n",
        "+ https://cloud.google.com/vertex-ai/docs/tutorials/jupyter-notebooks#vertex-ai-workbench\n",
        "+ https://github.com/christophergarthwood/jbooks/blob/main/000_Intro.ipynb\n",
        "+ https://www.geeksforgeeks.org/how-to-use-jupyter-notebook-an-ultimate-guide/\n",
        "\n",
        "### References for NLP\n",
        "+ https://realpython.com/python-nltk-sentiment-analysis/\n",
        "+ https://www.nltk.org/howto/classify.html\n",
        "+ https://www.nltk.org/book/ch06.html\n",
        "+ https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
        "+ [NLTK Summary](https://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3)\n",
        "+ [How to summarize text with OpenAI and LangChain](https://medium.com/@johnidouglasmarangon/how-to-summarize-text-with-openai-and-langchain-e038fc922af)\n",
        "+ https://www.geeksforgeeks.org/natural-language-processing-overview/\n",
        "+ https://aws.amazon.com/what-is/nlp/\n",
        "+ [Text Summary with Multiple Tools](https://medium.com/analytics-vidhya/text-summarization-using-bert-gpt2-xlnet-5ee80608e961)\n",
        "+ [Text Summary with Llama2](https://medium.com/@tushitdavergtu/llama2-and-text-summarization-e3eafb51fe28)\n",
        "+[spaCy](https://spacy.io/)\n",
        "+ https://coolinfographics.com/word-clouds\n",
        "+ https://peritract.github.io/2020/04/05/wordclouds-in-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da0L4uF30pJE"
      },
      "source": [
        "### Initial Variable Setup / Project Startup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hbr46-4B_g--",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Let's define some variables (information holders) for our project overall\n",
        "\n",
        "global PROJECT_ID, BUCKET_NAME, LOCATION\n",
        "BUCKET_NAME =\"jbooks_ai_ml_public\"\n",
        "PROJECT_ID  =\"ai-bootcamp\"\n",
        "LOCATION    = \"us-central1\"\n",
        "\n",
        "BOLD_START=\"\\033[1m\"\n",
        "BOLD_END=\"\\033[0m\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "bc70dba7b1b74daeb7514e961bb8b598",
            "e86c32652bad4585a43e71be5355912c",
            "110cb1fed2c84ed89ec0a33e1b711f2e",
            "ea07eed901d24c489c652ffe19e4138d",
            "4d0018f6d7ea47bcaa3cade07a7ca9fa",
            "e54cb10db5f94b238084afe35556898b",
            "e8e7e7c506e94d399825313ad1bb62ff",
            "26929618524e4d179a3fd1b89a1827e9",
            "acfb80d1c3bb492e9316a8b70547fd43",
            "a1ef0e195dbe4c21ab0ca765941347fd",
            "ef6627886544486ab7cdce43dcc1ecad",
            "c7b9e052f53b44e3bbc81deec236ebc3",
            "e21f816f968e47378912a2f2842e10d0",
            "97434169ba3449e688592ecfd589f556",
            "7ab73300cabd4b578800afe02059d7a2",
            "65ee23dcd53040519d09c94be9fcb027"
          ]
        },
        "id": "m4fEl50ptuIX",
        "outputId": "7fb2f452-4f2b-4602-8afe-f613198df077"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='\\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc70dba7b1b74daeb7514e961bb8b598"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Now create a means of enforcing project id selection\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "def wait_for_button_press():\n",
        "\n",
        "    button_pressed = False\n",
        "\n",
        "    # Create widgets\n",
        "    html_widget = widgets.HTML(\n",
        "\n",
        "    value=\"\"\"\n",
        "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
        "\n",
        "        <table><tr><td>\n",
        "            <span style=\"font-family: Tahoma;font-size: 18\">\n",
        "              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n",
        "              Please verify that you are in the appropriate project and that the:</br>\n",
        "              <center><code><b>PROJECT_ID</b></code> </br></center>\n",
        "              aligns with the Project Id in the upper left corner of this browser and that the location:\n",
        "              <center><code><b>LOCATION</b></code> </br></center>\n",
        "              aligns with the instructions provided.\n",
        "            </span>\n",
        "          </td></tr></table></br></br>\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "    project_list=[\"ai-bootcamp\", \"I will setup my own\"]\n",
        "    dropdown = widgets.Dropdown(\n",
        "        options=project_list,\n",
        "        value=project_list[0],\n",
        "        description='Set Your Project:',\n",
        "    )\n",
        "\n",
        "    html_widget2 = widgets.HTML(\n",
        "    value=\"\"\"\n",
        "        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n",
        "          \"\"\")\n",
        "\n",
        "    button = widgets.Button(description=\"Accept\")\n",
        "\n",
        "    # Function to handle the selection change\n",
        "    def on_change(change):\n",
        "        global PROJECT_ID\n",
        "        if change['type'] == 'change' and change['name'] == 'value':\n",
        "            #print(\"Selected option:\", change['new'])\n",
        "            PROJECT_ID=change['new']\n",
        "\n",
        "    # Observe the dropdown for changes\n",
        "    dropdown.observe(on_change)\n",
        "\n",
        "    def on_button_click(b):\n",
        "        nonlocal button_pressed\n",
        "        global PROJECT_ID\n",
        "        button_pressed = True\n",
        "        #button.disabled = True\n",
        "        button.close()  # Remove the button from display\n",
        "        with output:\n",
        "          #print(f\"Button pressed...continuing\")\n",
        "          #print(f\"Selected option: {dropdown.value}\")\n",
        "          PROJECT_ID=dropdown.value\n",
        "\n",
        "    button.on_click(on_button_click)\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Create centered layout\n",
        "    centered_layout = widgets.VBox([\n",
        "                                    html_widget,\n",
        "                                    widgets.HBox([dropdown, button]),\n",
        "                                    html_widget2,\n",
        "    ], layout=widgets.Layout(\n",
        "                              display='flex',\n",
        "                              flex_flow='column',\n",
        "                              align_items='center',\n",
        "                              width='100%'\n",
        "    ))\n",
        "    # Display the layout\n",
        "    display(centered_layout)\n",
        "\n",
        "\n",
        "wait_for_button_press()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGZR3_I00swG"
      },
      "source": [
        "### Environment Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shY7a4DVQjWB",
        "outputId": "129b94e9-b4f6-421c-a1ca-fb3b93022bd3",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are running this notebook in  Google Colab  at \u001b[1m2025-08-04 21:04:49.711673\u001b[0m on \u001b[1mLinux\u001b[0m in the \u001b[1mai-bootcamp\u001b[0m lab.\n"
          ]
        }
      ],
      "source": [
        "#- Environment Check\n",
        "import datetime\n",
        "import platform\n",
        "\n",
        "RunningInCOLAB = False\n",
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "current_time   = datetime.datetime.now()\n",
        "operating_system=platform.system()\n",
        "\n",
        "if RunningInCOLAB:\n",
        "    python_environment=\" Google Colab \"\n",
        "    from IPython.core.interactiveshell import InteractiveShell\n",
        "    InteractiveShell.ast_node_interactivity = \"all\"\n",
        "else:\n",
        "    python_environment=\" Python command-line \"\n",
        "\n",
        "print(f\"You are running this notebook in {python_environment} at {BOLD_START}{current_time}{BOLD_END} on {BOLD_START}{operating_system}{BOLD_END} in the {BOLD_START}{PROJECT_ID}{BOLD_END} lab.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDEc0UzfOVNw"
      },
      "source": [
        "### Library Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qlsjoFw5QjWC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Import key libraries necessary to support dynamic installation of additional libraries\n",
        "import sys\n",
        "# Use subprocess to support running operating system commands from the program, using the \"bang\" (!)\n",
        "# symbology is supported, however that does not translate to an actual python script, this is a more\n",
        "# agnostic approach.\n",
        "import subprocess\n",
        "import importlib.util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8sGvyMt1OrH"
      },
      "source": [
        "#### Install Libraries for Runtime\n",
        "\n",
        "Creates an install process that installs missing libraries and/or verifies their existence without re-install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqob1J7RQjWD",
        "outputId": "fd1a1bc5-711f-4c9d-f36a-8ac663e74f73",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing library backoff\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'backoff', '--quiet'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Library nltk already installed.\n",
            "Library bs4 already installed.\n",
            "Library wordcloud already installed.\n",
            "Library pathlib already installed.\n",
            "Library numpy already installed.\n",
            "Library Pillow already installed.\n",
            "Library pandas already installed.\n",
            "Installing library python-dotenv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'python-dotenv', '--quiet'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Library seaborn already installed.\n",
            "Library rich already installed.\n",
            "Installing library rich[jupyter]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'rich[jupyter]', '--quiet'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing library piexif\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'piexif', '--quiet'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing library PyMuPDF\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'PyMuPDF', '--quiet'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing library unidecode\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'unidecode', '--quiet'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Library spacy already installed.\n",
            "Installing library gensim\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'gensim', '--quiet'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing library cluestar\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'cluestar', '--quiet'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing library watermark\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'watermark', '--quiet'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing library watermark[GPU]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'watermark[GPU]', '--quiet'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing library scattertext\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'scattertext', '--quiet'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Identify the libraries you'd like to add to this Runtime environment.\n",
        "libraries=[\"backoff\", \"nltk\", \"bs4\", \"wordcloud\", \"pathlib\", \"numpy\", \"Pillow\", \"pandas\",\n",
        "           \"python-dotenv\", \"seaborn\", \"rich\", \"rich[jupyter]\", \"piexif\", \"PyMuPDF\",\"unidecode\",\n",
        "           \"spacy\", \"cluestar\", \"watermark\", \"watermark[GPU]\", \"scattertext\",]\n",
        "\n",
        "#removed gensim, a great library but it conflicts with a lot of other stuff.\n",
        "\n",
        "# Loop through each library and test for existence, if not present install quietly\n",
        "for library in libraries:\n",
        "    if library == \"Pillow\":\n",
        "      spec = importlib.util.find_spec(\"PIL\")\n",
        "    else:\n",
        "      spec = importlib.util.find_spec(library)\n",
        "    if spec is None:\n",
        "      print(\"Installing library \" + library)\n",
        "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"], check=True)\n",
        "    else:\n",
        "      print(\"Library \" + library + \" already installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1vadEsUuQjWD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#- Import Natural Language Processing (NLP) specific libs\n",
        "import nltk\n",
        "\n",
        "# Libraries designed to \"chop up\" text into manageable units\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Library used to remove words that have no real semantic value in a sentence\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Libraries designed to support different functions for text processing.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import pos_tag\n",
        "from nltk.tree import tree\n",
        "from nltk import FreqDist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PJuXEPlkSo9p",
        "tags": [],
        "outputId": "65a98fd2-fe1b-4d7c-d38b-e054eb08714e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3908206714.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#- Data Science Libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "#- Import additional libraries that add value to the project related to NLP\n",
        "\n",
        "# Beautiful Soup (BS4) is used to parse HTML documents.\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Word cloud building library\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "#- Set of libraries that perhaps should always be in Python source\n",
        "import backoff\n",
        "import datetime\n",
        "from dotenv import load_dotenv\n",
        "import gc\n",
        "import getopt\n",
        "import glob\n",
        "import inspect\n",
        "import io\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import platform\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "from io import StringIO\n",
        "import subprocess\n",
        "import socket\n",
        "import sys\n",
        "import textwrap\n",
        "import tqdm\n",
        "import traceback\n",
        "import warnings\n",
        "import time\n",
        "from time import perf_counter\n",
        "from rich import print as rprint\n",
        "from rich.console import Console\n",
        "from rich.traceback import install\n",
        "import locale\n",
        "\n",
        "#- Displays system info\n",
        "from watermark import watermark as the_watermark\n",
        "from py3nvml import py3nvml\n",
        "\n",
        "#- Additional libraries for this work\n",
        "import math\n",
        "from base64 import b64decode\n",
        "#from IPython.display import Image, Markdown\n",
        "from IPython.display import Image, Markdown\n",
        "import IPython.display as display, io, jinja2, base64\n",
        "\n",
        "import requests\n",
        "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
        "import unidecode\n",
        "\n",
        "#- Data Science Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "#- Graphics\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from matplotlib.cbook import get_sample_data\n",
        "from matplotlib.offsetbox import (AnnotationBbox, DrawingArea, OffsetImage,\n",
        "                                  TextArea)\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib.patches import Circle\n",
        "from PIL import Image as PIL_Image\n",
        "import PIL.ImageOps\n",
        "\n",
        "#- Image meta-data for Section 508 compliance\n",
        "import piexif\n",
        "from piexif.helper import UserComment\n",
        "\n",
        "#- Pandas for data frames\n",
        "import pandas\n",
        "\n",
        "#- Progress bar\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c31oCX0otO-C"
      },
      "source": [
        "## Application Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1AKqaU6akMe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# API Parameters for things like WordCloud, variables help hold information for later use\n",
        "# The \"constants\" represent variables that we don't anticipate changing over the course of the program.\n",
        "IMG_BACKGROUND=\"black\"     #options are black, white, another color or None\n",
        "IMG_FONT_SIZE_MIN=10\n",
        "IMG_WIDTH=1024\n",
        "IMG_HEIGHT=768\n",
        "IMG_INTERP=\"bilinear\"\n",
        "IMG_ALPHA=0.8\n",
        "IMG_ASPECT=\"equal\"\n",
        "FIGURE_WIDTH=11\n",
        "FIGURE_HEIGHT=8.5\n",
        "WORD_FREQ=10\n",
        "\n",
        "# specify how image formats will be saved\n",
        "IMG_EXT=\".jpg\"\n",
        "\n",
        "# used to fully display the error stack, set to 1 if you want to see a ridiculous amount of debugging information\n",
        "DEBUG_STACKTRACE=0\n",
        "\n",
        "# location of our working files\n",
        "WORKING_FOLDER=\"./content/folderOnColab\"\n",
        "\n",
        "# Notebook Author details\n",
        "AUTHOR_NAME=\"Christopher G Wood\"\n",
        "GITHUB_USERNAME=\"christophergarthwood\"\n",
        "AUTHOR_EMAIL=\"christopher.g.wood@gmail.com\"\n",
        "\n",
        "# Encoding\n",
        "ENCODING  =\"utf-8\"\n",
        "os.environ['PYTHONIOENCODING']=ENCODING\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdsZsLtONmOE"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InWUUhBiNpXT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Functions are like legos that do one thing, this function outputs library version history of effort.\n",
        "def lib_diagnostics() -> None:\n",
        "\n",
        "    import pkg_resources\n",
        "\n",
        "    package_name_length=20\n",
        "    package_version_length=10\n",
        "\n",
        "    # Show notebook details\n",
        "    #%watermark?\n",
        "    #%watermark --github_username christophergwood --email christopher.g.wood@gmail.com --date --time --iso8601 --updated --python --conda --hostname --machine --githash --gitrepo --gitbranch --iversions --gpu\n",
        "    # Watermark\n",
        "    rprint(the_watermark(author=f\"{AUTHOR_NAME}\", github_username=f\"GITHUB_USERNAME\", email=f\"{AUTHOR_EMAIL}\",iso8601=True, datename=True, current_time=True, python=True, updated=True, hostname=True, machine=True, gitrepo=True, gitbranch=True, githash=True))\n",
        "\n",
        "\n",
        "    print(f\"{BOLD_START}Packages:{BOLD_END}\")\n",
        "    print(\"\")\n",
        "    # Get installed packages\n",
        "    the_packages=[\"nltk\", \"numpy\", \"os\", \"pandas\", \"seaborn\"]\n",
        "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
        "    for package_idx, package_name in enumerate(installed):\n",
        "         if package_name in the_packages:\n",
        "             installed_version = installed[package_name]\n",
        "             rprint(f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\")\n",
        "\n",
        "    try:\n",
        "        rprint(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
        "        rprint(f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\")\n",
        "        rprint(f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        rprint(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
        "        rprint(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
        "        rprint(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
        "        rprint(f\"{'     current':<40}#: {torch.cuda.current_device()}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "\n",
        "    try:\n",
        "      print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiYoOv625oSI"
      },
      "source": [
        "### Section 508 Compliance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d1nJmRHiRYr"
      },
      "outputs": [],
      "source": [
        "# Routines designed to support adding ALT text to an image generated through Matplotlib.\n",
        "\n",
        "def capture(figure):\n",
        "   buffer = io.BytesIO()\n",
        "   figure.savefig(buffer)\n",
        "   #return F\"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
        "   return F\"data:image/jpg;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
        "\n",
        "def make_accessible(figure, template, **kwargs):\n",
        "   return display.Markdown(F\"\"\"![]({capture(figure)} \"{template.render(**globals(), **kwargs)}\")\"\"\")\n",
        "\n",
        "\n",
        "# requires JPG's or TIFFs\n",
        "def add_alt_text(image_path, alt_text):\n",
        "    try:\n",
        "        if os.path.isfile(image_path):\n",
        "          img = PIL_Image.open(image_path)\n",
        "          if \"exif\" in img.info:\n",
        "              exif_dict = piexif.load(img.info[\"exif\"])\n",
        "          else:\n",
        "              exif_dict={}\n",
        "\n",
        "          w, h = img.size\n",
        "          if \"0th\" not in exif_dict:\n",
        "            exif_dict[\"0th\"]={}\n",
        "          exif_dict[\"0th\"][piexif.ImageIFD.XResolution] = (w, 1)\n",
        "          exif_dict[\"0th\"][piexif.ImageIFD.YResolution] = (h, 1)\n",
        "\n",
        "          software_version=\" \".join([\"STEM-001 with Python v\", str(sys.version).split(\" \")[0]])\n",
        "          exif_dict[\"0th\"][piexif.ImageIFD.Software]=software_version.encode(\"utf-8\")\n",
        "\n",
        "          if \"Exif\" not in exif_dict:\n",
        "            exif_dict[\"Exif\"]={}\n",
        "          exif_dict[\"Exif\"][piexif.ExifIFD.UserComment] = UserComment.dump(alt_text, encoding=\"unicode\")\n",
        "\n",
        "          exif_bytes = piexif.dump(exif_dict)\n",
        "          img.save(image_path, \"jpeg\", exif=exif_bytes)\n",
        "        else:\n",
        "          rprint(f\"Cound not fine {image_path} for ALT text modification, please check your paths.\")\n",
        "\n",
        "    except (FileExistsError, FileNotFoundError, Exception) as e:\n",
        "        process_exception(e)\n",
        "\n",
        "# Appears to solve a problem associated with GPU use on Colab, see: https://github.com/explosion/spaCy/issues/11909\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwe0arcyrvES"
      },
      "outputs": [],
      "source": [
        "# this function displays the stack trace on errors from a central location making adjustments to the display on an error easier to manage\n",
        "# functions perform useful solutions for highly repetitive code\n",
        "def process_exception(inc_exception: Exception) -> None:\n",
        "  if DEBUG_STACKTRACE==1:\n",
        "    traceback.print_exc()\n",
        "    console.print_exception(show_locals=True)\n",
        "  else:\n",
        "    rprint(repr(inc_exception))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdK4I7bTKXBu"
      },
      "source": [
        "### Setup Instances of Variables from Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shu7BuR6QjWC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Setup the rich print console for future use\n",
        "if DEBUG_STACKTRACE==1:\n",
        "  console = Console()\n",
        "\n",
        "# NLTK required resources, required to load necessary files to support NLTK\n",
        "# Downloads repository of knowledge to augment (this is the data portion) the library\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"words\")\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "#- Only do this if you want the full spectrum of all possible packages, it's a LOT!\n",
        "#nltk.download(\"all\")\n",
        "\n",
        "# Noun Part of Speech Tags used by NLTK\n",
        "# More can be found here\n",
        "# http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/\n",
        "#NOUNS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "#VERBS = ['VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
        "\n",
        "# Use the 'Agg' backend for non-interactive environments\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "# Ensure UTF-8 Encoding is set\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sVS7OY9CT69"
      },
      "source": [
        "## Function Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwOiLPSGCVud",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Now call the function just created and get input on what versions of software we're using.\n",
        "lib_diagnostics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi5Vl91h5oSI"
      },
      "outputs": [],
      "source": [
        "#supress non-certificate type request because it's too much on the screen.\n",
        "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
        "\n",
        "# Ignore InsecureRequestWarning specifically\n",
        "warnings.simplefilter(\"ignore\", InsecureRequestWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JMSTY2QjWD"
      },
      "source": [
        "# Input Sources\n",
        "\n",
        "Your input source could be a file locally hosted, a web page, an Application Programmer's Interface (API) call or something else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHYa2NqupYN_"
      },
      "outputs": [],
      "source": [
        "# Create the folder that will hold our content.\n",
        "target_folder=WORKING_FOLDER\n",
        "rprint(f\"Creating a folder ({target_folder}) to store project data.\")\n",
        "\n",
        "try:\n",
        "  if os.path.isfile(target_folder):\n",
        "    raise OSError(\"Cannot create your folder a file of the same name already exists there, work with your instructor or remove it yourself.\")\n",
        "  elif os.path.isdir(target_folder):\n",
        "    print(f\"The folder named ({target_folder}) {BOLD_START}already exists{BOLD_END}, we won't try to create a new folder.\")\n",
        "  else:\n",
        "    subprocess.run([\"mkdir\", \"-p\" , target_folder], check=True)\n",
        "except (subprocess.CalledProcessError, Exception) as e:\n",
        "  process_exception(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQeN6mr_GS1Z",
        "tags": []
      },
      "outputs": [],
      "source": [
        "target_folder=WORKING_FOLDER\n",
        "\n",
        "if RunningInCOLAB:\n",
        "    # Let's move some data over from our GCP bucket to this local machine\n",
        "    # The following is a list of the files we're going to pull over\n",
        "    target_files=[\"ANewHope.txt\", \"slf*.txt\", \"alb*.txt\"]\n",
        "    if os.path.isdir(target_folder):\n",
        "      for idx, filename in enumerate(target_files):\n",
        "        print(f\"Copying {filename} to target folder: {target_folder}\")\n",
        "        try:\n",
        "          subprocess.run([\"gsutil\", \"-m\" , \"cp\", \"-r\", f\"gs://{BUCKET_NAME}/training-data/jbooks/{filename}\",  target_folder], check=True)\n",
        "        except (subprocess.CalledProcessError, Exception) as e:\n",
        "          process_exception(e)\n",
        "    else:\n",
        "        print(\"ERROR: Local folder not found/created.  Check the output to ensure your folder is created.\")\n",
        "        print(f\"...target folder: {target_folder}\")\n",
        "        print(\"...if you can't find the problem contact the instructor.\")\n",
        "else:\n",
        "  # since you're not running COLAB let's try downloading directly from another site.\n",
        "  # list of file id's required to download appropriate content\n",
        "  target_files=[\"1JdtVja-6QHRFOUQc0gorcmJITr-NwrjF\", \"1FxrXDSSF7J1LYGX02CZ3D9kGAv0ZuRyA\", \"1oJFmPHiE2jgSVLWKtCpb-jO5q_dirheA\"]\n",
        "  target_filenames=[\"ANewHope.txt\",\"slf_final_wordcloud_content.txt\", \"alb_final_wordcloud_content.txt\"]\n",
        "  for idx, the_name in enumerate(target_files):\n",
        "    try:\n",
        "      subprocess.run([\"gdown\", f\"{the_name}\", \"--no-check-certificate\",  \"--continue\", \"-O\", f\"{target_folder}{os.sep}{target_filenames[idx]}\"], check=True)\n",
        "    except (subprocess.CalledProcessError, Exception) as e:\n",
        "      process_exception(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9I0VKVILoAZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Now, setup a variable to store the actual content in the file\n",
        "data=\"\"\n",
        "\n",
        "# select the filename you want to process your body of text from: ANewHope.txt, slf_final_wordcloud_content.txt, alb_final_wordcloud_content.txt\n",
        "target_filename=target_folder+os.sep+\"slf_final_wordcloud_content.txt\"          #<- Change here, names must be exact and stay between the double quotes\n",
        "\n",
        "\n",
        "# check for the file's existence\n",
        "if os.path.isfile(target_filename):\n",
        "  #open the file, read the contents and close the file\n",
        "  try:\n",
        "    with open(target_filename, \"r\", encoding=\"cp1252\") as my_file:\n",
        "        data=my_file.read()\n",
        "  except (FileNotFoundError,PermissionError,IOError,UnicodeDecodeError, Exception) as e:\n",
        "    process_exception(e)\n",
        "else:\n",
        "    rprint(\"ERROR: File not found.  Check the previous code block to ensure you file copied.\")\n",
        "    rprint(f\"...target file: {target_filename}\")\n",
        "    rprint(\"...if you can't find the problem contact the instructor.\")\n",
        "\n",
        "if len(data)<1:\n",
        "    rprint(\"ERROR: There is no content in your data variable.\")\n",
        "    rprint(\"...Verify you copied the input file correctly.\")\n",
        "    rprint(\"...if you can't find the problem contact the instructor.\")\n",
        "else:\n",
        "    rprint(f\"It appears your data file was read, your data file has {len(data):,} elements of data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BysXtCJLa_K"
      },
      "source": [
        "\n",
        "# *Natural Language Processing Methods *\n",
        "\n",
        "+ **Tokenization** is a fundamental task that involves breaking down a piece of text into smaller units called tokens. These tokens can be individual characters, entire words, or even subwold pieces, depending on the tokenization method used.\n",
        "+ **Stop words** are inconsequential words that appear frequently in a language but don’t contribute much to the overall understanding of a sentence or document. They often include articles, prepositions, conjunctions, and common verbs.\n",
        "+ **Stemming** is a text preprocessing technique that involves reducing words to their base or root form, known as stems.\n",
        "+ **Lemmatization** aims to identify and convert words to their base forms by considering their grammatical properties and meanings.\n",
        "\n",
        "\n",
        "***Why might a Data Scientist use NLP techniques on a body of text when performing AI work?***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PHVPawdQjWE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Demonstrate use of tokens and stopwords\n",
        "\n",
        "#Perform a tokenization at the sentence level of the data.\n",
        "response=sent_tokenize(data)\n",
        "rprint(f\"There are {len(response)} sentences.\")\n",
        "\n",
        "#Perform a tokenization at the word level of the data.\n",
        "response=word_tokenize(data)\n",
        "rprint(f\"There are {len(response)} words.\")\n",
        "\n",
        "#apply stop words to remove inconsequential words that appear frequently but don't influence the overall understanding of the setences.\n",
        "#gather the stop words for the NLTK library into a variable\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "#create a list data structure that will hold the resulting words, lists store chunks of data like a carton for eggs stores groups of eggs.\n",
        "filtered_list = []\n",
        "\n",
        "#break the overall data into \"word\" tokens after making everything lowercase (why would we do that?  Ask the instructor?)\n",
        "word_token_response=word_tokenize(data.lower())\n",
        "\n",
        "#Python \"lamba\" expression, very efficient for loop, used to continue normalizing the data by only allowing alpha characters that are equal to or greater than 2.\n",
        "wordlist = [x for x in word_token_response if (len(x)>=2 and x.isalpha())]\n",
        "\n",
        "#loop through each word in the wordlist and verify that it is not a stop word.  if the word is not a stop word, save it for later use.\n",
        "for word in tqdm(wordlist):\n",
        "    if word.casefold() not in stop_words:\n",
        "         filtered_list.append(word)\n",
        "\n",
        "rprint(f\"\\nThere are {len(filtered_list)} remaining words after cleaning them up.\")\n",
        "print(\"\")\n",
        "\n",
        "#Let's see how often certain words appear in the text\n",
        "fq=FreqDist(filtered_list)\n",
        "\n",
        "# Creating FreqDist for whole BoW, keeping the 20 most common tokens\n",
        "all_fdist = FreqDist(filtered_list).most_common(WORD_FREQ)\n",
        "\n",
        "#let's plot the most 10 common words\n",
        "print(\"\")\n",
        "rprint(f\"Word Frequency (top {WORD_FREQ} most used words):\")\n",
        "print(\"\")\n",
        "for idx,the_word in enumerate(all_fdist):\n",
        "    rprint(f\"Word #{idx+1}, {the_word[0]} appears {the_word[1]} times.\")\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(f\"Notice anything?  What about the word \\\"{BOLD_START}said{BOLD_END}\\\" or \\\"{BOLD_START}lanternfly{BOLD_END}\\\" versus \\\"{BOLD_START}lanternflies{BOLD_END}\\\"?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fH5SDhJy4t5"
      },
      "outputs": [],
      "source": [
        "# Conversion to Pandas series via Python Dictionary for easier plotting\n",
        "the_frequency_distribution = pd.Series(dict(all_fdist));\n",
        "\n",
        "# Setting figure to a size determined by the variables at the start of the program\n",
        "fig, ax = plt.subplots(figsize=(FIGURE_WIDTH,FIGURE_HEIGHT));\n",
        "\n",
        "# Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
        "all_plot = sns.barplot(x=the_frequency_distribution.index, y=the_frequency_distribution.values, ax=ax);\n",
        "bit_bucket=plt.xticks(rotation=30);\n",
        "bit_bucket=plt.xlabel('Words');\n",
        "bit_bucket=plt.ylabel('Frequency of Appearance');\n",
        "bit_bucket=plt.title(f'Top {WORD_FREQ} Words Plot');\n",
        "\n",
        "# Create ALT text describing the image.\n",
        "caption_text=f\"Bar plot of most frequently appearing words from the datafile {target_filename} with {all_fdist[0][0]}, {all_fdist[0][-1]} times, appearing most and {all_fdist[-1][0]}, {all_fdist[-1][-1]} times, appearing least.\";\n",
        "\n",
        "bit_bucket=plt.text(0.5,\n",
        "                   0.7,\n",
        "                  caption_text,\n",
        "                  rotation=0.0,\n",
        "                  ha=\"left\",\n",
        "                  va=\"center\",\n",
        "                  fontsize=IMG_FONT_SIZE_MIN,\n",
        "                  family='serif',\n",
        "                  style='oblique',\n",
        "                  wrap=True,\n",
        "                  bbox=dict(boxstyle=\"round\",\n",
        "                            ec=(1., 0.5, 0.5),\n",
        "                            fc=(1., 0.8, 0.8),\n",
        "                            ),\n",
        "                  transform=fig.transFigure\n",
        "                  );\n",
        "\n",
        "# Save the plot to a file that can be pulled up at a later date.\n",
        "target_filename=target_folder+os.sep+f\"STEM-001_word_frequency{IMG_EXT}\";\n",
        "plt.savefig(target_filename);\n",
        "add_alt_text(target_folder+os.sep+f\"STEM-001_word_frequency{IMG_EXT}\", caption_text);\n",
        "rprint(\"Next display is a \" + caption_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVfL0oJaXl9f"
      },
      "outputs": [],
      "source": [
        "# Display Section 508 compliant image on the scrren with ALT text\n",
        "\n",
        "try:\n",
        "  make_accessible(plt, jinja2.Template(caption_text), len=len)\n",
        "except (NameError, Exception) as e:\n",
        "  process_exception(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY6qso_cELp1"
      },
      "outputs": [],
      "source": [
        "plt.close();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oGbe8IT1oZ_"
      },
      "source": [
        "# Example of Stem and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F6-Ztyh1tqG"
      },
      "outputs": [],
      "source": [
        "# Initialize variables for NLTK specific functions\n",
        "wnl = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Example inflections to reduce\n",
        "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
        "\n",
        "# Perform stemming and lemmatization to show the differences\n",
        "print(\"{0:20}{1:20}{2:20}\".format(\"--Word--\",\"--Stem--\",\"--Lemma--\"))\n",
        "\n",
        "for word in example_words:\n",
        "   print (\"{0:20}{1:20}{2:20}\".format(word, stemmer.stem(word), wnl.lemmatize(word, pos=\"v\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT_efe3G1Exk"
      },
      "source": [
        "# Stem and Lemmatization In Depth\n",
        "\n",
        "## Stemming\n",
        "\n",
        "**Stemming** is a technique used to reduce an inflected word down to its word stem. For example, the words “programming,” “programmer,” and “programs” can all be reduced down to the common word stem “program.” In other words, “program” can be used as a synonym for the prior three inflection words.\n",
        "\n",
        "Performing this text-processing technique is often useful for dealing with sparsity and/or standardizing vocabulary. Not only does it help with reducing redundancy, as most of the time the word stem and their inflected words have the same meaning, it also allows NLP models to learn links between inflected words and their word stem, which helps the model understand their usage in similar contexts.\n",
        "\n",
        "**Stemming** algorithms function by taking a list of frequent prefixes and suffixes found in inflected words and chopping off the end or beginning of the word. This can occasionally result in word stems that are not real words; thus, we can affirm this approach certainly has its pros, but it’s not without its limitations.\n",
        "\n",
        "### Advantages of Stemming\n",
        "Improved model performance: Stemming reduces the number of unique words that need to be processed by an algorithm, which can improve its performance. Additionally, it can also make the algorithm run faster and more efficiently.\n",
        "Grouping similar words: Words with a similar meaning can be grouped together, even if they have distinct forms. This can be a useful technique in tasks such as document classification, where it’s important to identify key topics or themes within a document.\n",
        "\n",
        "Easier to analyze and understand: Since stemming typically reduces the size of the vocabulary, it’s much easier to analyze, compare, and understand texts. This is helpful in tasks such as sentiment analysis, where the goal is to determine the sentiment of a document.\n",
        "\n",
        "### Disadvantages of Stemming\n",
        "\n",
        "Overstemming / False positives: This is when a stemming algorithm reduces separate inflected words to the same word stem even though they are not related; for example, the Porter Stemmer algorithm stems  \"universal\", \"university\", and \"universe\" to the same word stem. Though they are etymologically related, their meanings in the modern day are from widely different domains. Treating them as synonyms will reduce relevance in search results.\n",
        "\n",
        "Understemming / False negatives: This is when a stemming algorithm reduces inflected words to different word stems, but they should be the same. For example, the Porter Stemmer algorithm does not reduce the words “alumnus,” “alumnae,” and “alumni” to the same word stem, although they should be treated as synonyms.\n",
        "\n",
        "Language challenges: As the target language's morphology, spelling, and character encoding get more complicated, stemmers become more difficult to design; For example, an Italian stemmer is more complicated than an English stemmer because there is a higher number of verb inflections. A Russian stemmer is even more complex due to more noun declensions.\n",
        "\n",
        "## Lemmatization\n",
        "\n",
        "**Lemmatization** is another technique used to reduce inflected words to their root word. It describes the algorithmic process of identifying an inflected word’s “lemma” (dictionary form) based on its intended meaning.\n",
        "\n",
        "As opposed to stemming, **lemmatization** relies on accurately determining the intended part-of-speech and the meaning of a word based on its context. This means it takes into consideration where the inflected word falls within a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document.\n",
        "\n",
        "“Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma”\n",
        "\n",
        "In other words, to lemmatize a document typically means to “doing things correctly” since it involves using a vocabulary and performing morphological analysis of words to remove only the inflectional ends and return the base or dictionary form of a word, which is known as the “lemma.” For example, you can expect a lemmatization algorithm to map “runs,” “running,” and “ran” to the lemma, “run.”  \n",
        "\n",
        "### Advantages of Lemmatization\n",
        "\n",
        "Accuracy: Lemmatization does not merely cut words off as you see in stemming algorithms. Analysis of words is conducted based on the word’s POS to take context into consideration when producing lemmas. Also, lemmatization leads to real dictionary words being produced.\n",
        "\n",
        "### Disadvantages of Lemmatization\n",
        "\n",
        "Time-consuming: Compared to stemming, lemmatization is a slow and time-consuming process. This is because lemmatization involves performing morphological analysis and deriving the meaning of words from a dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Icd3yDQjWF"
      },
      "source": [
        "# Cloud Map with Stop words applied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnjA2tpZQjWF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Word Cloud API Call with stopwords applied, resultant is an image.\n",
        "\n",
        "# Note that you can add your own stop words as follows\n",
        "# Create stopword list:\n",
        "# STOPWORDS.update([\"lantern\", \"beetle\"])\n",
        "\n",
        "try:\n",
        "    #invoke the word cloud API using the original list of lowercase, alpha, > 2 character words.\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                          background_color=IMG_BACKGROUND,\n",
        "                          min_font_size=IMG_FONT_SIZE_MIN,\n",
        "                          width=IMG_WIDTH,\n",
        "                          height=IMG_HEIGHT,\n",
        "                         ).generate(\" \".join(wordlist))\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying invoke the WordCloud call as follows: {str(e)}\")\n",
        "    process_exception(e)\n",
        "\n",
        "# Show the Results, turn the image into a graphic that is saved.\n",
        "caption_text=f\"\"\"Image with a width of {IMG_WIDTH} by height of {IMG_HEIGHT} with a {IMG_BACKGROUND} background comprised of many different words like {all_fdist[0][0]} appearing largest, followed by {all_fdist[1][0]} and so forth.\"\"\"\n",
        "plt.figure(figsize=(FIGURE_WIDTH,FIGURE_HEIGHT));\n",
        "plt.imshow(wordcloud, interpolation=IMG_INTERP, alpha=IMG_ALPHA, aspect=IMG_ASPECT);\n",
        "plt.axis('off');\n",
        "plt.savefig(target_folder+os.sep+f\"STEM-001_word_cloud_one{IMG_EXT}\");\n",
        "add_alt_text(target_folder+os.sep+f\"STEM-001_word_cloud_one{IMG_EXT}\", caption_text);\n",
        "rprint(caption_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9jrKOi07M6N"
      },
      "outputs": [],
      "source": [
        "# Display Section 508 compliant image on the scrren with ALT text for World Cloud Number One\n",
        "try:\n",
        "  make_accessible(plt, jinja2.Template(caption_text), len=len)\n",
        "except (NameError, Exception) as e:\n",
        "  process_exception(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwZEU4yNERWH"
      },
      "outputs": [],
      "source": [
        "plt.close();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrjQnydJQjWF"
      },
      "source": [
        "# Cloud Map without Stop Words Applied to Original Body of Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELhTTfZkQjWF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Second word cloud without stop words applied and a white background\n",
        "try:\n",
        "    IMG_BACKGROUND=\"white\"\n",
        "    wordcloud = WordCloud(\n",
        "                          background_color=IMG_BACKGROUND,\n",
        "                          min_font_size=IMG_FONT_SIZE_MIN,\n",
        "                          width=IMG_WIDTH,\n",
        "                          height=IMG_HEIGHT,\n",
        "                         ).generate(data)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying invoke the WordCloud() call as follows: {str(e)}\")\n",
        "\n",
        "#Show the Results, turn the image into a displayed graphic\n",
        "caption_text=f\"\"\"Image with a width of {IMG_WIDTH} by height of {IMG_HEIGHT} with a {IMG_BACKGROUND} background comprised of many different words that are not uniformly organized nor set to lowercase.\"\"\"\n",
        "plt.figure(figsize=(FIGURE_WIDTH,FIGURE_HEIGHT));\n",
        "plt.imshow(wordcloud, interpolation=IMG_INTERP, alpha=IMG_ALPHA, aspect=IMG_ASPECT);\n",
        "plt.axis('off');\n",
        "plt.savefig(target_folder+os.sep+f\"STEM-001_word_cloud_two{IMG_EXT}\");\n",
        "add_alt_text(target_folder+os.sep+f\"STEM-001_word_cloud_two{IMG_EXT}\", caption_text);\n",
        "rprint(caption_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yot6lYdu7fUf"
      },
      "outputs": [],
      "source": [
        "# Display Section 508 compliant image on the scrren with ALT text for World Cloud Number Two, minus stop words (all text)\n",
        "try:\n",
        "  make_accessible(plt, jinja2.Template(caption_text), len=len)\n",
        "except (NameError, Exception) as e:\n",
        "  process_exception(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnSjU6MXEU7a"
      },
      "outputs": [],
      "source": [
        "plt.close();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK386XTKFCOw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Identify and download some masks that will be used to force the word cloud into a specific shape\n",
        "\n",
        "masks=[\"usfs_mask.jpg\", \"slf_mask.jpg\", \"force_mask.jpg\",]\n",
        "if os.path.isdir(target_folder):\n",
        "  for idx, filename in enumerate(masks):\n",
        "    rprint(f\"Copying {filename} to target folder: {target_folder}\")\n",
        "    try:\n",
        "      #subprocess.run([\"gsutil\", \"-m\" , \"cp\", \"-r\", f\"gs://{BUCKET_NAME}/training-data/llm/masks/{filename}\",  target_folder], check=True)\n",
        "      subprocess.run([\"gsutil\", \"cp\", \"-r\", f\"gs://{BUCKET_NAME}/training-data/llm/masks/{filename}\",  target_folder], check=True)\n",
        "    except (subprocess.CalledProcessError, Exception) as e:\n",
        "      process_exception(e)\n",
        "else:\n",
        "    rprint(\"ERROR: Local folder not found/created.  Check the output to ensure your folder is created.\")\n",
        "    rprint(f\"...target folder: {target_folder}\")\n",
        "    rprint(\"...if you can't find the problem contact the instructor.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4elpHP-uTX-F",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "# Images, Mask loading and setup, setup the plot mechanism to show the images.\n",
        "\n",
        "rows=1\n",
        "cols=6\n",
        "position=1\n",
        "img_ary=[]\n",
        "fig = plt.figure(figsize=(FIGURE_WIDTH, FIGURE_HEIGHT));\n",
        "\n",
        "\n",
        "# loop through the lists and print them side by side\n",
        "for idx, filename in enumerate(masks):\n",
        "    print(f\"Processing...{filename}\")\n",
        "    bit_bucket=fig.add_subplot(rows,cols, position);\n",
        "    img_ary.append(PIL_Image.open(target_folder+os.sep+filename));\n",
        "    # showing image\n",
        "    bit_bucket=plt.imshow(img_ary[idx]);\n",
        "    bit_bucket=plt.axis('off');\n",
        "    bit_bucket=plt.title(os.path.basename(filename));\n",
        "    # Adds a subplot at the next position\n",
        "    position += 1;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBYcsB9NEka3"
      },
      "outputs": [],
      "source": [
        "plt.close();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV2_6CMDQjWF"
      },
      "source": [
        "# With a Mask\n",
        "\n",
        "Note that the \"mask\" is effectively an image of same size that's black and white.  Blacked out portions become the area you can populate so I took the USFS logo, inverted so the white becomes black and filled in the verbiage there.\n",
        "\n",
        "Next I paste the transparent PNG on top of the original logo thus putting the words in the middle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfuA9qTnQjWF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Mask loading and setup\n",
        "\n",
        "# Use the 'Agg' backend for non-interactive environments\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "#choose a different mask if you want different results, options are: usfs_mask.jpg, slf_mask.jpg, force_mask.jpg, kitty_mask.jpg, pine_tree_mask.png, paw_mawk.jpg\n",
        "target_filename=\"usfs_mask.jpg\"                        #<- change here\n",
        "the_logo=PIL_Image.open(target_folder+os.sep+target_filename)\n",
        "\n",
        "try:\n",
        "    #invert the colors of the mask image, so that the image is \"reversed\"\n",
        "    inverted_image = PIL.ImageOps.invert(the_logo)\n",
        "    # Convert the image to a numeric representation (a 3D array)\n",
        "    the_mask = np.array(inverted_image)\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying read the mask image and invert the colors call as follows: {str(e)}\")\n",
        "\n",
        "\n",
        "# Data Marshaling from original filtered list, meaning stopwords applied\n",
        "data = \" \".join(filtered_list)\n",
        "\n",
        "########################################\n",
        "#API Call\n",
        "########################################\n",
        "try:\n",
        "    wordcloud = WordCloud(\n",
        "                      stopwords=STOPWORDS,\n",
        "                      mode=\"RGBA\",\n",
        "                      #critical this be None to ensure merging with the transparency mask\n",
        "                      background_color=None,\n",
        "                      #background_color=IMG_BACKGROUND,\n",
        "                      min_font_size=IMG_FONT_SIZE_MIN,\n",
        "                      width=IMG_WIDTH,\n",
        "                      height=IMG_HEIGHT,\n",
        "                      mask=the_mask,\n",
        "                     ).generate(\" \".join(wordlist))\n",
        "    the_image= wordcloud.to_image()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR detected trying invoke the WordCloud() call as follows: {str(e)}\")\n",
        "\n",
        "#turn the image into a displayed graphic\n",
        "caption_text=f\"\"\"Image with a width of {IMG_WIDTH} by height of {IMG_HEIGHT} with a {IMG_BACKGROUND} background comprised of rationalized words with your mask, {target_filename}, pushing words out of the way of that shape.\"\"\"\n",
        "plt.figure(figsize=(FIGURE_WIDTH,FIGURE_HEIGHT));\n",
        "plt.imshow(the_image, interpolation=IMG_INTERP, alpha=IMG_ALPHA, aspect=IMG_ASPECT);\n",
        "plt.axis('off');\n",
        "plt.savefig(target_folder+os.sep+f\"STEM-001_word_merged{IMG_EXT}\");\n",
        "add_alt_text(target_folder+os.sep+f\"STEM-001_word_merged{IMG_EXT}\", caption_text);\n",
        "rprint(caption_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okEdGfDTCMSy"
      },
      "outputs": [],
      "source": [
        "# Display Section 508 compliant image on the scrren with ALT text for World Cloud Number Two, minus stop words (all text)\n",
        "try:\n",
        "  make_accessible(plt, jinja2.Template(caption_text), len=len)\n",
        "except (NameError, Exception) as e:\n",
        "  process_exception(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPQZAkHlExcg"
      },
      "outputs": [],
      "source": [
        "plt.close();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeoxHrHNQjWF"
      },
      "source": [
        "# Merge the Images into a single instance\n",
        "\n",
        "You might want to try another image and text block on your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KltRE5VQjWG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# creating a image object (main image)\n",
        "im1=the_logo\n",
        "im2=the_image\n",
        "alpha = im2.getchannel('A')\n",
        "\n",
        "#transparency save\n",
        "im1.paste(im2, (0,0), im2)\n",
        "\n",
        "the_final_image=im1\n",
        "\n",
        "caption_text=f\"\"\"Image with a width of {IMG_WIDTH} by height of {IMG_HEIGHT} with a {IMG_BACKGROUND} background comprised of rationalized words with your mask, {target_filename}, pushing words out of the way of that shape showing the actual mask.\"\"\"\n",
        "plt.figure(figsize=(FIGURE_WIDTH,FIGURE_HEIGHT));\n",
        "plt.imshow(the_final_image, interpolation=\"bilinear\", alpha=0.8, aspect=\"equal\");\n",
        "plt.axis('off');\n",
        "plt.savefig(target_folder+os.sep+f\"STEM-001_word_final{IMG_EXT}\");\n",
        "add_alt_text(target_folder+os.sep+f\"STEM-001_word_final{IMG_EXT}\", caption_text);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0zwW5x_UAAa"
      },
      "outputs": [],
      "source": [
        "# Display Section 508 compliant image on the scrren with ALT text for World Cloud Number Two, minus stop words (all text)\n",
        "try:\n",
        "  make_accessible(plt, jinja2.Template(caption_text), len=len)\n",
        "except (NameError, Exception) as e:\n",
        "  process_exception(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqq3shwME1hh"
      },
      "outputs": [],
      "source": [
        "plt.close();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD0bf7_r4UWW"
      },
      "source": [
        "# Advanced Assignments\n",
        "\n",
        "## Utilize NLP techniques to process your own data\n",
        "\n",
        "+ Select one of the data filenames and read-in a different filename.\n",
        "+ Observe the word frequency, stop word influence on size of data, stemming, and lemmatization results.\n",
        "+ Observe the differences in the Word Cloud\n",
        "+ Change the mask used to a different one and create a final output.\n",
        "\n",
        "## Uber-Challenge\n",
        "+ Load the Spotted Lantern Fly data file and the Eastern LongHorn Beetle data files either separately or joined together.\n",
        "+ Repeat the experiment to see the influence of the new data.\n",
        "\n",
        "## Pro-Tip:\n",
        "\n",
        "Either create a new block of code that reads the 2nd file and combine the resulting variables `data = data_slf + \" \" + data_alb` or create an array, iterate through the target files and combine them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InewfnVi0QK5"
      },
      "source": [
        "## Webpage Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a40SzNOcTHAv"
      },
      "outputs": [],
      "source": [
        "# Example of getting data from a URL\n",
        "\n",
        "# web interaction libraries\n",
        "import requests\n",
        "import urllib.request\n",
        "from urllib.request import urlopen\n",
        "import io\n",
        "import ssl\n",
        "\n",
        "# web scraper\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# word cloud (extras)\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "# Read a web page, web scraping\n",
        "url=\"https://www.usfa.fema.gov/wui/outreach/wildfire-evacuation.html\"\n",
        "target_folder=WORKING_FOLDER\n",
        "target_filename=\"STEM-001-advanced-webpage.html\"\n",
        "target_file=target_folder+os.sep+target_filename\n",
        "\n",
        "# Download the data file locally and save to file\n",
        "the_request = requests.get(url, allow_redirects=True);\n",
        "open(target_file, 'wb').write(the_request.content);\n",
        "\n",
        "# Create a context with default certificates\n",
        "context = ssl.create_default_context()\n",
        "\n",
        "# Create a context with disabled verification\n",
        "context = ssl.create_default_context()\n",
        "context.check_hostname = False\n",
        "context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "rprint(f\"Access the URL provided ({url})\")\n",
        "\n",
        "# open the connection\n",
        "the_page=urlopen(url, context=context);\n",
        "soup = BeautifulSoup(the_page, 'html.parser');\n",
        "\n",
        "#OR\n",
        "\n",
        "the_page = requests.get(url).content;\n",
        "soup = BeautifulSoup(the_page, 'html.parser');\n",
        "\n",
        "#OR read the local download\n",
        "\n",
        "# check for the file's existence\n",
        "if os.path.isfile(target_file):\n",
        "  #open the file, read the contents and close the file\n",
        "  try:\n",
        "    with open(target_file, \"r\", encoding=\"utf-8\") as my_file:\n",
        "        #the_page=my_file.read();\n",
        "        soup = BeautifulSoup(my_file, 'html.parser');\n",
        "  except (FileNotFoundError,PermissionError,IOError,UnicodeDecodeError, Exception) as e:\n",
        "    process_exception(e)\n",
        "else:\n",
        "    rprint(\"ERROR: File not found.  Check the previous code block to ensure you file copied.\")\n",
        "    rprint(f\"...target file: {target_file}\")\n",
        "    rprint(\"...if you can't find the problem contact the instructor.\")\n",
        "\n",
        "\n",
        "rprint(\"Find all headers.\")\n",
        "#titles = soup.findAll(re.compile('^h[1-4]'))\n",
        "titles = soup.find_all(re.compile('^h[1-4]'))\n",
        "\n",
        "print(\"\")\n",
        "the_titles=[]\n",
        "print(f\"{BOLD_START}Article Titles{BOLD_END}\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "for title in titles:\n",
        "    rprint(title.text)\n",
        "    word_token_response=word_tokenize(title.text.lower())\n",
        "    wordlist = [x for x in word_token_response if (len(x)>=2 and x.isalpha())]\n",
        "    for word in wordlist:\n",
        "      if word.casefold() not in stop_words:\n",
        "         the_titles.append(word)\n",
        "print(\"\")\n",
        "\n",
        "# To see the whole page (it's a lot)\n",
        "#print(soup.prettify())\n",
        "\n",
        "# Download custom mask\n",
        "url=\"https://cdn.pixabay.com/photo/2022/05/29/11/36/fire-7228995_1280.png\"\n",
        "target_folder=WORKING_FOLDER\n",
        "target_filename=\"fire_mask.png\"\n",
        "target_mask_file=target_folder+os.sep+target_filename\n",
        "\n",
        "# Mask loading and setup\n",
        "# Download the data file locally and save to file\n",
        "# Reference: https://www.datacamp.com/tutorial/wordcloud-python\n",
        "# Reference: https://medium.com/@m3redithw/wordclouds-with-python-c287887acc8b\n",
        "\n",
        "rprint(\"Downloading and saving a fire graphic mask.\")\n",
        "the_request = requests.get(url, allow_redirects=True);\n",
        "open(target_mask_file, 'wb').write(the_request.content);\n",
        "\n",
        "# Use the 'Agg' backend for non-interactive environments\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "rprint(\"Loading fire mask.\")\n",
        "the_mask=PIL_Image.open(target_mask_file);\n",
        "the_original_mask= np.array(PIL_Image.open(target_mask_file));\n",
        "the_transformed_mask = np.array(PIL_Image.open(target_mask_file));\n",
        "\n",
        "#the_transformed_mask[the_transformed_mask == 255] = 0\n",
        "the_transformed_mask[the_transformed_mask == 0 ] = 255;\n",
        "\n",
        "\n",
        "try:\n",
        "  rprint(\"Invoke the Word Cloud API using the fire mask.\")\n",
        "  wordcloud_fire = WordCloud(\n",
        "                             stopwords=STOPWORDS,\n",
        "                             mode=\"RGBA\",\n",
        "                             background_color=None,\n",
        "                             min_font_size=IMG_FONT_SIZE_MIN,\n",
        "                             width=IMG_WIDTH,\n",
        "                             height=IMG_HEIGHT,\n",
        "                             mask=the_transformed_mask\n",
        "                            ).generate(\" \".join(the_titles))\n",
        "\n",
        "  rprint(\"Create coloring from image.\")\n",
        "  image_colors = ImageColorGenerator(the_original_mask)\n",
        "\n",
        "  rprint(\"Align colors of words with colors in mask.\")\n",
        "  wordcloud_fire.recolor(color_func=image_colors)\n",
        "\n",
        "  rprint(\"Save mask to an image.\")\n",
        "  the_image= wordcloud_fire.to_image()\n",
        "\n",
        "except Exception as e:\n",
        "    process_exception(e)\n",
        "\n",
        "# creating a image object (main image)\n",
        "im1=the_mask\n",
        "im2=the_image\n",
        "alpha = im2.getchannel('A')\n",
        "\n",
        "#transparency save\n",
        "im1.paste(im2, (0,0), im2)\n",
        "\n",
        "the_final_image=im1\n",
        "\n",
        "#turn the image into a displayed graphic\n",
        "plt.close() #clear the slate of prior images\n",
        "caption_text=f\"\"\"Image with a width of {IMG_WIDTH} by height of {IMG_HEIGHT} with a {IMG_BACKGROUND} background comprised of rationalized words from your downloaded file shaped like a fire.\"\"\"\n",
        "plt.figure(figsize=(FIGURE_WIDTH,FIGURE_HEIGHT));\n",
        "plt.imshow(\n",
        "    the_final_image,\n",
        "    interpolation=IMG_INTERP,\n",
        "    alpha=1.0,\n",
        "    aspect=IMG_ASPECT,\n",
        "    );\n",
        "plt.axis('off');\n",
        "plt.savefig(target_folder+os.sep+f\"STEM-001_advanced_webpage{IMG_EXT}\");\n",
        "add_alt_text(target_folder+os.sep+f\"STEM-001_advanced_webpage{IMG_EXT}\", caption_text);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggAb9eNQspJS"
      },
      "outputs": [],
      "source": [
        "# Display Section 508 compliant image on the scrren with ALT text for World Cloud Number Two, minus stop words (all text)\n",
        "try:\n",
        "  make_accessible(plt, jinja2.Template(caption_text), len=len)\n",
        "except (NameError, Exception) as e:\n",
        "  process_exception(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMXeyC6H0JQ1"
      },
      "source": [
        "## Portable Document Format (PDF) Read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTV-Onn_U2aG"
      },
      "outputs": [],
      "source": [
        "# PDF library\n",
        "import pymupdf\n",
        "\n",
        "\n",
        "# Read a Portable Document Format (PDF)\n",
        "url=\"https://www.epa.gov/sites/default/files/2021-04/documents/wildfires_td.pdf\"\n",
        "target_folder=WORKING_FOLDER\n",
        "target_filename=\"STEM-001-advanced-pdf.pdf\"\n",
        "target_file=target_folder+os.sep+target_filename\n",
        "\n",
        "# Download the data file locally and save to file\n",
        "rprint(f\"Access the URL provided ({url}) and save to a local file.\")\n",
        "the_request = requests.get(url, allow_redirects=True);\n",
        "open(target_file, 'wb').write(the_request.content);\n",
        "\n",
        "# Create a context with default certificates\n",
        "context = ssl.create_default_context()\n",
        "\n",
        "# Create a context with disabled verification\n",
        "context = ssl.create_default_context()\n",
        "context.check_hostname = False\n",
        "context.verify_mode = ssl.CERT_NONE\n",
        "\"\"\"\n",
        "the_request = requests.get(url)\n",
        "with open(target_file, 'wb') as fd:\n",
        "     fd.write(response.content)\n",
        "\"\"\"\n",
        "rprint(\"Now read the local file and harvest the data from the PDF.\")\n",
        "# check for the file's existence\n",
        "if os.path.isfile(target_file):\n",
        "  #open the file, read the contents and close the file\n",
        "  try:\n",
        "      doc = pymupdf.open(target_file)\n",
        "      total_text=\"\"\n",
        "      rprint(\"...processing each page of the document.\")\n",
        "      total_text = chr(12).join([page.get_text() for page in doc])\n",
        "\n",
        "  except (FileNotFoundError,PermissionError,IOError,UnicodeDecodeError, Exception) as e:\n",
        "    process_exception(e)\n",
        "else:\n",
        "    rprint(\"ERROR: File not found.  Check the previous code block to ensure you file copied.\")\n",
        "    rprint(f\"...target file: {target_file}\")\n",
        "    rprint(\"...if you can't find the problem contact the instructor.\")\n",
        "\n",
        "print(\"\")\n",
        "rprint(\"Process the text from the PDF.\")\n",
        "filtered_list=[]\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "word_token_response=word_tokenize(total_text.lower())\n",
        "wordlist = [x for x in word_token_response if (len(x)>=2 and x.isalpha())]\n",
        "for word in wordlist:\n",
        "  if word.casefold() not in stop_words:\n",
        "     filtered_list.append(word)\n",
        "print(\"\")\n",
        "\n",
        "# Download custom mask\n",
        "url=\"https://cdn.pixabay.com/photo/2022/05/29/11/36/fire-7228995_1280.png\"\n",
        "target_folder=WORKING_FOLDER\n",
        "target_filename=\"fire_mask.png\"\n",
        "target_mask_file=target_folder+os.sep+target_filename\n",
        "\n",
        "# Mask loading and setup\n",
        "# Download the data file locally and save to file\n",
        "# Reference: https://www.datacamp.com/tutorial/wordcloud-python\n",
        "# Reference: https://medium.com/@m3redithw/wordclouds-with-python-c287887acc8b\n",
        "\n",
        "rprint(\"Downloading and saving a fire graphic mask.\")\n",
        "the_request = requests.get(url, allow_redirects=True);\n",
        "open(target_mask_file, 'wb').write(the_request.content);\n",
        "\n",
        "# Use the 'Agg' backend for non-interactive environments\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "rprint(\"Loading fire mask.\")\n",
        "the_mask=PIL_Image.open(target_mask_file);\n",
        "the_original_mask= np.array(PIL_Image.open(target_mask_file));\n",
        "the_transformed_mask = np.array(PIL_Image.open(target_mask_file));\n",
        "\n",
        "#the_transformed_mask[the_transformed_mask == 255] = 0\n",
        "the_transformed_mask[the_transformed_mask == 0 ] = 255;\n",
        "\n",
        "\n",
        "try:\n",
        "  rprint(\"Invoke the Word Cloud API using the fire mask.\")\n",
        "  wordcloud_fire = WordCloud(\n",
        "                             stopwords=STOPWORDS,\n",
        "                             mode=\"RGBA\",\n",
        "                             background_color=None,\n",
        "                             min_font_size=IMG_FONT_SIZE_MIN,\n",
        "                             width=IMG_WIDTH,\n",
        "                             height=IMG_HEIGHT,\n",
        "                             mask=the_transformed_mask\n",
        "                            ).generate(\" \".join(filtered_list))\n",
        "\n",
        "  rprint(\"Create coloring from image.\")\n",
        "  image_colors = ImageColorGenerator(the_original_mask)\n",
        "\n",
        "  rprint(\"Align colors of words with colors in mask.\")\n",
        "  wordcloud_fire.recolor(color_func=image_colors)\n",
        "\n",
        "  rprint(\"Save mask to an image.\")\n",
        "  the_image= wordcloud_fire.to_image()\n",
        "\n",
        "except Exception as e:\n",
        "    process_exception(e)\n",
        "\n",
        "# creating a image object (main image)\n",
        "im1=the_mask\n",
        "im2=the_image\n",
        "alpha = im2.getchannel('A')\n",
        "\n",
        "#transparency save\n",
        "im1.paste(im2, (0,0), im2)\n",
        "\n",
        "the_final_image=im1\n",
        "\n",
        "#turn the image into a displayed graphic\n",
        "plt.close() #clear the slate of prior images\n",
        "caption_text=f\"\"\"Image with a width of {IMG_WIDTH} by height of {IMG_HEIGHT} with a {IMG_BACKGROUND} background comprised of rationalized words from your downloaded file shaped like a fire.\"\"\"\n",
        "plt.figure(figsize=(FIGURE_WIDTH,FIGURE_HEIGHT));\n",
        "plt.imshow(\n",
        "    the_final_image,\n",
        "    interpolation=IMG_INTERP,\n",
        "    alpha=1.0,\n",
        "    aspect=IMG_ASPECT,\n",
        "    );\n",
        "plt.axis('off');\n",
        "plt.savefig(target_folder+os.sep+f\"STEM-001_advanced_webpage{IMG_EXT}\");\n",
        "add_alt_text(target_folder+os.sep+f\"STEM-001_advanced_webpage{IMG_EXT}\", caption_text);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys7uX7InwrCi"
      },
      "outputs": [],
      "source": [
        "# Display Section 508 compliant image on the scrren with ALT text for World Cloud Number Two, minus stop words (all text)\n",
        "try:\n",
        "  make_accessible(plt, jinja2.Template(caption_text), len=len)\n",
        "except (NameError, Exception) as e:\n",
        "  process_exception(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ01NP7GhCDm"
      },
      "source": [
        "## Let's Talk ScatterText\n",
        "\n",
        "A tool for finding distinguishing terms in corpora and displaying them in an interactive HTML scatter plot. Points corresponding to terms are selectively labeled so that they don't overlap with other labels or points.\n",
        "\n",
        "Reference: https://github.com/JasonKessler/scattertext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b409I-TwhHlk"
      },
      "outputs": [],
      "source": [
        "import scattertext as st\n",
        "\n",
        "df = st.SampleCorpora.ConventionData2012.get_data().assign(\n",
        "    parse=lambda df: df.text.apply(st.whitespace_nlp_with_sentences)\n",
        ")\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfJijqtihpAC"
      },
      "outputs": [],
      "source": [
        "corpus = st.CorpusFromParsedDocuments(\n",
        "                                       df,\n",
        "                                       category_col='party',\n",
        "                                       parsed_col='parse'\n",
        "                                      ).build().get_unigram_corpus().compact(st.AssociationCompactor(2000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ3B1yAIjG4D"
      },
      "outputs": [],
      "source": [
        "html = st.produce_scattertext_explorer(\n",
        "    corpus,\n",
        "    category='democrat',\n",
        "    category_name='Democratic',\n",
        "    not_category_name='Republican',\n",
        "    minimum_term_frequency=0,\n",
        "    pmi_threshold_coefficient=0,\n",
        "    width_in_pixels=1000,\n",
        "    metadata=corpus.get_df()['speaker'],\n",
        "    transform=st.Scalers.dense_rank,\n",
        "    #include_gradient=True,\n",
        "    #left_gradient_term='More Republican',\n",
        "    #middle_gradient_term='Metric: Dense Rank Difference',\n",
        "    #right_gradient_term='More Democratic',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zCdrfhxjI_g"
      },
      "outputs": [],
      "source": [
        "target_filename=f\"{WORKING_FOLDER}/scattertext_demo.html\"\n",
        "try:\n",
        "  with open(target_filename, \"w\", encoding=\"UTF-8\") as my_file:\n",
        "      my_file.write(html)\n",
        "except (FileNotFoundError,PermissionError,IOError,UnicodeDecodeError, Exception) as e:\n",
        "  process_exception(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTl-LHQkjZgW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "#HTMLDisplay (filename=target_filename)\n",
        "HTML(html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rvf_-0Bz_By"
      },
      "source": [
        "## Let's Talk spaCy\n",
        "\n",
        "spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "\n",
        "If you’re working with a lot of text, you’ll eventually want to know more about it. For example, what’s it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n",
        "\n",
        "spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
        "\n",
        "**Features**:\n",
        "\n",
        "+ Non-destructive tokenization\n",
        "+ Named entity recognition\n",
        "+ Support for 63+ languages\n",
        "+ 46 statistical models for 16 languages\n",
        "+ Pretrained word vectors\n",
        "+ State-of-the-art speed\n",
        "+ Easy deep learning integration\n",
        "+ Part-of-speech tagging\n",
        "+ Labelled dependency parsing\n",
        "+ Syntax-driven sentence segmentation\n",
        "+ Built in visualizers for syntax and NER\n",
        "+ Convenient string-to-hash mapping\n",
        "+ Export to numpy data arrays\n",
        "+ Efficient binary serialization\n",
        "+ Easy model packaging and deployment\n",
        "+ Robust, rigorously evaluated accuracy\n",
        "\n",
        "Reference:\n",
        "+ https://v2.spacy.io/\n",
        "+ https://v2.spacy.io/usage/examples\n",
        "+ https://spacy.io/usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af_4oIOY6BbQ"
      },
      "outputs": [],
      "source": [
        "rprint(\"Make sure spaCy is installed.\")\n",
        "\n",
        "# Encountered problems in session where GPU was utilized and System encoding, in session, changes to ANSI*, normalize back to UTF-8\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# Identify the libraries you'd like to add to this Runtime environment.\n",
        "libraries=[\"spacy\",\"spacy[cuda12x]\"]\n",
        "\n",
        "# Loop through each library and test for existence, if not present install quietly\n",
        "for library in libraries:\n",
        "    if library == \"Pillow\":\n",
        "      spec = importlib.util.find_spec(\"PIL\")\n",
        "    else:\n",
        "      spec = importlib.util.find_spec(library)\n",
        "    if spec is None:\n",
        "      print(\"Installing library \" + library)\n",
        "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"], check=True)\n",
        "    else:\n",
        "      print(\"Library \" + library + \" already installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6CgN3Tq86fe"
      },
      "outputs": [],
      "source": [
        "# Import spaCy libraries\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import Doc\n",
        "from spacy.matcher import Matcher\n",
        "from spacy import displacy\n",
        "from spacy.tokens import Span\n",
        "from collections import Counter\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDx5_LPrz4AO"
      },
      "outputs": [],
      "source": [
        "# Setup SpaCy model (load)\n",
        "# Efficient model and small but not as accurate, 12MB\n",
        "#SPACY_MODEL_NAME=\"en_core_web_sm\"\n",
        "\n",
        "# Efficient model and medium but not as accurate, 31mb\n",
        "SPACY_MODEL_NAME=\"en_core_web_md\"\n",
        "\n",
        "#Much larger model, robust but 400MB\n",
        "#SPACY_MODEL_NAME=\"en_core_web_lg\"\n",
        "\n",
        "#Accurate model but large\n",
        "#SPACY_MODEL_NAME=\"en_core_web_trf\"\n",
        "\n",
        "\n",
        "rprint(\"Attempt to utilize GPU's over CPU's.\")\n",
        "try:\n",
        "    spacy.prefer_gpu()\n",
        "except Exception as e:\n",
        "    #no gpu available will default to CPU\n",
        "    rprint(f\"GPU not utilized by SpaCy, see exception: {e}\")\n",
        "    pass\n",
        "\n",
        "print(\"\")\n",
        "rprint(f\"Load the following model: {SPACY_MODEL_NAME}.\")\n",
        "try:\n",
        "    nlp =spacy.load(SPACY_MODEL_NAME)\n",
        "except Exception as e:\n",
        "    process_exception(e)\n",
        "    rprint(\"Unable to load your model, performing a download instead.\")\n",
        "    #!python -m spacy download {SPACY_MODEL}\n",
        "    subprocess.run([\"python\", \"-m\" , \"spacy\", \"download\", SPACY_MODEL_NAME])\n",
        "    pass  #we want to download not cause a problem.\n",
        "finally:\n",
        "    rprint(\"Now load that model.\")\n",
        "    nlp =spacy.load(SPACY_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcQpO8ou8IYV"
      },
      "outputs": [],
      "source": [
        "body_of_text=\"The mission of the United States Forest Service is to maintain the health, diversity, and productivity of the country's forests and grasslands for the benefit of current and future generations. The Forest Service manages the country's national forests for a variety of uses, including: recreation, timber, wilderness, minerals, water, grazing, fish, and wildlife. The Forest Service's mission is captured in the phrase: Caring for the Land and Serving People.\"\n",
        "\n",
        "# Use the spaCy visualizer to \"see\" the sentence structure\n",
        "# Reference: https://spacy.io/usage/visualizers#jupyter\n",
        "doc = nlp(body_of_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSltmSvDEHzg"
      },
      "source": [
        "### Word Frequency (Topic Analysis Start Point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWqlEXXXEElC"
      },
      "outputs": [],
      "source": [
        "# Remove stop words and punctuation symbols\n",
        "words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# 5 commonly occurring words with their frequencies\n",
        "print(f\"{BOLD_START}Common Words{BOLD_END}\")\n",
        "common_words = word_freq.most_common(5)\n",
        "rprint(common_words)\n",
        "\n",
        "# Unique words\n",
        "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
        "print(\"\")\n",
        "print(f\"{BOLD_START}Unique Words{BOLD_END}\")\n",
        "rprint(unique_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDsoA8oHD7ba"
      },
      "source": [
        "### Linguistic Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VemJGTjh6eIx"
      },
      "outputs": [],
      "source": [
        "# Destroy any previous plots so things don't buffer with Agg and what not\n",
        "from IPython.core.display import HTML, display\n",
        "plt.close()\n",
        "%matplotlib inline\n",
        "\n",
        "#displacy.render(doc, style=\"dep\")\n",
        "sentence_spans = list(doc.sents)\n",
        "\n",
        "try:\n",
        "    if RunningInCOLAB:\n",
        "        displacy.render(sentence_spans, style=\"dep\")\n",
        "    else:\n",
        "        displacy.render(sentence_spans, style=\"dep\", jupyter=True)\n",
        "except Exception as e:\n",
        "    print(\"There was a problem rendering the resultant as a graphic, displaying results as pure text.\")\n",
        "    displacy.render(sentence_spans, style=\"dep\", jupyter=True)\n",
        "    #displacy.serve(sentence_spans, style=\"dep\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6L20EZ_0mZS"
      },
      "outputs": [],
      "source": [
        "# Look at the text as entities\n",
        "displacy.render(doc, style=\"ent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj72i9dtDUm9"
      },
      "source": [
        "### Entity Detection\n",
        "\n",
        "Entity detection is a more advanced form of language processing. ED identifies entities in a sentence such as specific locations (GPE), date-related words (DATE), important numbers (CARDINAL), and specific individuals (PERSON) within an input string of text. It is helpful when you want to identify key information from text.\n",
        "\n",
        "We use the property label to grab a label for each entity that has been detected in the text. You can also visualize those entities in spaCy‘s displaCy visualizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prje0wto8m-S"
      },
      "outputs": [],
      "source": [
        "# Look at how content spans the data\n",
        "doc.spans[\"sc\"] = [\n",
        "    Span(doc, 3, 6, \"ORG\"),\n",
        "    Span(doc, 5, 6, \"GPE\"),\n",
        "]\n",
        "\n",
        "displacy.render(doc, style=\"span\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk57_V4DDscG"
      },
      "source": [
        "### Part-Of-Speech (POS) Tagging\n",
        "\n",
        "Part of speech (POS) analyzes the grammatical role each word plays in a sentence. In other words, it determines to which category each word (Noun, Pronoun, Adjective, Verb, Adverb, Preposition, Conjunction, and interjection) belongs. POS tags are useful when you want to assign a syntactic category to each word of the text for future analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7nSjt0F8xEY"
      },
      "outputs": [],
      "source": [
        "# Show a table of entity types\n",
        "print(f\"\"\"{BOLD_START}{\"Word\":20}| {\"Lemma\":20}| {\"POS\":10}| {\"Tag\":10}| {\"Dep\":10}| {\"Shape\":10}| {\"IsAlpha?\":10}| {\"Stop Word\":10}{BOLD_END}\"\"\")\n",
        "\n",
        "for token in doc:\n",
        "    rprint(f\"\"\"{token.text:20}| {token.lemma_:20}| {token.pos_:10}| {token.tag_:10}| {token.dep_:10}| {token.shape_:10}| {token.is_alpha:10}| {token.is_stop:10}| \"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYLOgEOX_4Nf"
      },
      "source": [
        "### Named Entity Recognition (NER)\n",
        "\n",
        "A named entity is an object’s assigned name, for example, a person’s name, a film, a book title, or a song’s name. spaCy can recognize these named entities in a document by asking the model for a prediction. Because the performance of the models depends on the examples they were trained on, NEF might not always work perfectly and you might need to adjust the tuning based on your case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yblGGYgRBOME"
      },
      "outputs": [],
      "source": [
        "print(f\"\"\"{BOLD_START}{\"Label\":25} | {\"Definition\":200}{BOLD_END}\"\"\")\n",
        "for label in nlp.get_pipe('ner').labels:\n",
        "    rprint(f\"{label:25} | {spacy.explain(label):200}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMXP0BtY9Red"
      },
      "outputs": [],
      "source": [
        "# Find content based on it's nature (like people, places, and things).  Useful for data harvesting and PII stuff.\n",
        "\n",
        "print(f\"\"\"{BOLD_START}{\"Text\":40} | {\"Start\":6} | {\"End\":6} | {\"Label\":6} | {\"Description\":45}{BOLD_END}\"\"\")\n",
        "for ent in doc.ents:\n",
        "    rprint(f\"\"\"{ent.text:40} | {ent.start_char:6} | {ent.end_char:6} | {ent.label_:6} | {spacy.explain(ent.label_):45}\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWlWABk0DGST"
      },
      "source": [
        "### Dependency parsing\n",
        "\n",
        "Dependency parsing is the process of extracting the dependency parsing of a sentence to represent its grammatical structure. It defines the dependency relationship between headwords and their dependents. It helps you understand what role a word plays in the text and how words relate to each other.\n",
        "\n",
        "In dependency parsing, the head of a sentence is called the root of the sentence and has no dependency. The main verb or action is usually the head of the sentence and is denoted by the dependency tag ROOT. Other words are directly or indirectly to the headword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8uzpYSbAcRg"
      },
      "outputs": [],
      "source": [
        "# Dependency Parsing\n",
        "for token in doc:\n",
        "    # Print the tokens and their dependency tag\n",
        "    print(token.text, \"-->\", token.dep_)\n",
        "\n",
        "print(\"\")\n",
        "spacy.explain(\"nsubj\"), spacy.explain(\"ROOT\"), spacy.explain(\"aux\"), spacy.explain(\"advcl\"), spacy.explain(\"dobj\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6oa3YDYFgBw"
      },
      "source": [
        "### Phrase Matching vice using your own Regular Expressions\n",
        "\n",
        "The PhraseMatcher lets you efficiently match large terminology lists. While the Matcher lets you match sequences based on lists of token descriptions, the PhraseMatcher accepts match patterns in the form of Doc objects.\n",
        "\n",
        "Why might you use this? spaCy excels at NLP work and using the semantic meaning of context whereas regular expressions are simply focused on \"just the matching pattern\" without understanding of context.  Is one better than the other?  Depends on what you're trying to accomplish.\n",
        "\n",
        "The concept of lemma, parts of speech or context are lost on regular expressions.\n",
        "\n",
        "Reference:\n",
        "\n",
        "+ https://spacy.io/api/phrasematcher\n",
        "+ https://igbo-dev.medium.com/regex-vs-spacy-8b481b172653#:~:text=Regex%20can%20be%20employed%20to,understanding%20and%20processing%20human%20language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb2GmuY_HPOw"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.matcher import Matcher\n",
        "import re\n",
        "\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Collect data of matched sentences to be visualized\n",
        "matched_sents = []\n",
        "\n",
        "def collect_sents(matcher, doc, i, matches):\n",
        "    match_id, start, end = matches[i]\n",
        "    # Matched span\n",
        "    span = doc[start:end]\n",
        "\n",
        "     # Sentence containing matched span\n",
        "    sent = span.sent\n",
        "\n",
        "    # Append mock entity for match in displaCy style to matched_sents\n",
        "    # get the match span by ofsetting the start and end of the span with the\n",
        "    # start and end of the sentence in the doc\n",
        "    match_ents = [{\n",
        "        \"start\": span.start_char - sent.start_char,\n",
        "        \"end\": span.end_char - sent.start_char,\n",
        "        \"label\": \"MATCH\",\n",
        "    }]\n",
        "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
        "\n",
        "pattern_purpose = [{\"LOWER\": \"service\"}, {\"LEMMA\": \"be\"}]\n",
        "pattern_tasks =  [{\"LOWER\": \"service\"}, {\"POS\": \"VERB\"}]\n",
        "\n",
        "matcher.add(\"ForestServicePurpose\", [pattern_purpose], on_match=collect_sents)\n",
        "matcher.add(\"ForestServiceTasks\", [pattern_tasks], on_match=collect_sents)\n",
        "\n",
        "doc = nlp(body_of_text)\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "rprint(\"Match the concept of Forest Service purpose/being and tasking:\")\n",
        "# Serve visualization of sentences containing match with displaCy\n",
        "# set manual=True to make displaCy render straight from a dictionary\n",
        "# (if you're not running the code within a Jupyter environment, you can\n",
        "# use displacy.serve instead)\n",
        "displacy.render(matched_sents, style=\"ent\", manual=True)\n",
        "\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "\n",
        "# Now use regular expression to attempt the same thing\n",
        "rprint(\"Now use regular expressions to attempt the same task.  When you look for regex matching you'll get only the matching part, meaning, in case you're looking for a VERB you'll have to iterate or combine multiple verbs to accomplish what the spaCy matcher did.\")\n",
        "\n",
        "#you have to identify a range of potential verbs and their lemma either yourself or go ahead and use NLP to calculate the lemma\n",
        "tasking=[\"assign\", \"delegate\", \"undertake\", \"execute\", \"perform\", \"accomplish\", \"manage\", \"handle\", \"tackle\", \"carry out\", \"complete\", \"finalize\", \"initiate\", \"implement\", \"monitor\", \"review\", \"oversee\", \"address\", \"plan\", \"schedule\", \"prioritize\", \"analyze\", \"assess\", \"resolve\"]\n",
        "for idx, verb in enumerate(tasking):\n",
        "    the_word=wnl.lemmatize(verb, pos=\"v\")\n",
        "    x = re.search(f\"\"\"{the_word}*\"\"\", body_of_text)\n",
        "    rprint(f\"\"\"{idx:5} - {the_word:20} - {x}\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaqGO0aTFjpg"
      },
      "outputs": [],
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "matcher.add(\"ForestService\", [nlp(\"Forest Service\")])\n",
        "matches = matcher(doc)\n",
        "for the_match in matches:\n",
        "  #print(the_match[2])\n",
        "  match_id=the_match[0]\n",
        "  match_id_string = nlp.vocab.strings[match_id]\n",
        "  rprint(match_id_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL51vZhxnvT0"
      },
      "source": [
        "### Extracting Relations between phrases and entitiees\n",
        "\n",
        "This example shows extracting relations between phrases and entities using spaCy's named entity recognizer and the dependency parse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DFkbzzRnxIr"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Need to train an entity such as \"SERVICE\" against examples of service first.\n",
        "\n",
        "from __future__ import unicode_literals, print_function\n",
        "import spacy\n",
        "\n",
        "\n",
        "def filter_spans(spans):\n",
        "    # Filter a sequence of spans so they don't contain overlaps\n",
        "    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()\n",
        "    get_sort_key = lambda span: (span.end - span.start, -span.start)\n",
        "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
        "    result = []\n",
        "    seen_tokens = set()\n",
        "    for span in sorted_spans:\n",
        "        # Check for end - 1 here because boundaries are inclusive\n",
        "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
        "            result.append(span)\n",
        "        seen_tokens.update(range(span.start, span.end))\n",
        "    result = sorted(result, key=lambda span: span.start)\n",
        "    return result\n",
        "\n",
        "\n",
        "def extract_currency_relations(doc):\n",
        "    # Merge entities and noun chunks into one token\n",
        "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
        "    spans = filter_spans(spans)\n",
        "    with doc.retokenize() as retokenizer:\n",
        "        for span in spans:\n",
        "            retokenizer.merge(span)\n",
        "\n",
        "    relations = []\n",
        "    for money in filter(lambda w: w.ent_type_ == \"MONEY\", doc):\n",
        "        if money.dep_ in (\"attr\", \"dobj\"):\n",
        "            subject = [w for w in money.head.lefts if w.dep_ == \"nsubj\"]\n",
        "            if subject:\n",
        "                subject = subject[0]\n",
        "                relations.append((subject, money))\n",
        "        elif money.dep_ == \"pobj\" and money.head.dep_ == \"prep\":\n",
        "            relations.append((money.head.head, money))\n",
        "    return relations\n",
        "\n",
        "TEXTS=[\n",
        "       body_of_text,\n",
        "       \"Protection and management of natural resources on lands we manage.\",\n",
        "       \"Research on all aspects of forestry, rangeland management, and forest resource utilization.\",\n",
        "       \"Community assistance and cooperation with State and local governments, forest industries, and private landowners to help protect and manage non-Federal forest and associated range and watershed lands to improve conditions in rural areas.\",\n",
        "       \"Achievement and support of an effective workforce that reflects the diversity of the American people.\",\n",
        "       \"International assistance to formulate policy and coordinate U.S. support for the protection and sound management of the world's forest resources.\",\n",
        "      ]\n",
        "\n",
        "model=(\"Model to load (needs parser and NER)\", \"positional\", None, str)\n",
        "nlp = spacy.load(SPACY_MODEL_NAME)\n",
        "rprint(f\"Loaded model {model}.\")\n",
        "rprint(f\"Processing {len(TEXTS)} texts.\")\n",
        "for text in TEXTS:\n",
        "    doc = nlp(text)\n",
        "    relations = extract_currency_relations(doc)\n",
        "    for r1, r2 in relations:\n",
        "        print(\"{:<10}\\t{}\\t{}\".format(r1.text, r2.ent_type_, r2.text))\n",
        "\n",
        "\n",
        "    # Expected output:\n",
        "    # Net income      MONEY   $9.4 million\n",
        "    # the prior year  MONEY   $2.7 million\n",
        "    # Revenue         MONEY   twelve billion dollars\n",
        "    # a loss          MONEY   1b\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQEnVVNVqY0I"
      },
      "source": [
        "### Cosine Similarity\n",
        "\n",
        "Similarity is determined by comparing word vectors or “word embeddings”, multi-dimensional meaning representations of a word.\n",
        "\n",
        "Reference(s):\n",
        "+ https://v2.spacy.io/usage/vectors-similarity\n",
        "+ https://spencerporter2.medium.com/understanding-cosine-similarity-and-word-embeddings-dbf19362a3c\n",
        "+ https://memgraph.com/blog/cosine-similarity-python-scikit-learn\n",
        "+ https://medium.com/@milana.shxanukova15/cosine-distance-and-cosine-similarity-a5da0e4d9ded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AaCeuzpo1ka"
      },
      "outputs": [],
      "source": [
        "# Show the Input\n",
        "print(f\"{BOLD_START}Original:{BOLD_END}\")\n",
        "rprint(body_of_text)\n",
        "print(\"\")\n",
        "\n",
        "# Marshal resources to clean up the input\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "exclude = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()\n",
        "whiteSpaceRegex = \"\\\\s\";\n",
        "def clean(doc):\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop_words])\n",
        "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "    words      = normalized.split()\n",
        "    # Create a set from the list of words\n",
        "    word_set = set(words)\n",
        "    return \" \".join(word_set)\n",
        "\n",
        "clean_body_of_text=clean(body_of_text)\n",
        "print(f\"{BOLD_START}Clean:{BOLD_END}\")\n",
        "rprint(clean_body_of_text)\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "tokens = nlp(clean_body_of_text)\n",
        "\n",
        "THE_THRESHOLD=0.75\n",
        "print(f\"{BOLD_START}Cosine Similary of Word Vectors at {THE_THRESHOLD:.0%} or greater relationship{BOLD_END}\")\n",
        "print(f\"{BOLD_START}{'Token 1':20} | {'Token 2':20} | {'Similarity of 1 to 2':20}{BOLD_END}\")\n",
        "\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        similarity=token1.similarity(token2)\n",
        "        if similarity > THE_THRESHOLD and similarity < 1.0:\n",
        "          rprint(f\"{token1.text:20} | {token2.text:20} | {similarity:.0%}\")\n",
        "\n",
        "print(\"\")\n",
        "print(f\"{BOLD_START}Closest Matching Terms{BOLD_END}\")\n",
        "\n",
        "\n",
        "token1_value=\"\"\n",
        "token2_value=\"\"\n",
        "highest_sim=0.0\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        the_sim=token1.similarity(token2)\n",
        "        if the_sim > highest_sim and the_sim < 1.0:\n",
        "            highest_sim=the_sim\n",
        "            token1_value=token1\n",
        "            token2_value=token2\n",
        "\n",
        "rprint(f\"{str(token1_value):20} | {str(token2_value):20} | {highest_sim:.0%}\")\n",
        "rprint(f\"The word {token1_value} matches to {token2_value} at {highest_sim:.0%}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgkj52JYvQlL"
      },
      "source": [
        "### Visualize Embeddings\n",
        "\n",
        "**Cosine distance** for vector words refers to a method of measuring the similarity between two word vectors by calculating the cosine of the angle between them, essentially indicating how closely aligned the vectors are in a high-dimensional space; a smaller cosine distance signifies greater similarity between the words, while a larger distance means they are less related semantically.\n",
        "\n",
        "Key points about cosine distance for word vectors:\n",
        "\n",
        "#### How it works:\n",
        "\n",
        "+ Cosine distance is calculated by taking the dot product of two word vectors and dividing it by the product of their magnitudes.\n",
        "\n",
        "#### Interpretation:\n",
        "+ A cosine distance closer to 1 signifies very similar words, while a value closer to 0 indicates words with almost no semantic relationship.\n",
        "\n",
        "#### Advantage over Euclidean distance:\n",
        "+ Unlike Euclidean distance, cosine distance is not affected by the magnitude of the vectors, making it suitable for comparing words with different vector lengths.\n",
        "\n",
        "#### Applications in NLP:\n",
        "\n",
        "#### Synonym detection:\n",
        "\n",
        "+ Identifying words with similar meanings by finding those with a low cosine distance.\n",
        "\n",
        "#### Information retrieval:\n",
        "\n",
        "+ Ranking documents based on their similarity to a query by calculating the cosine distance between the query vector and document vectors.\n",
        "\n",
        "#### Text clustering:\n",
        "\n",
        "+ Grouping similar documents together by analyzing their cosine distances.\n",
        "\n",
        "#### Sentiment analysis:\n",
        "\n",
        "+ Determining the sentiment of a text by comparing its vector to known positive or negative sentiment vectors.\n",
        "\n",
        "#### Reference(s):\n",
        "\n",
        "+ https://github.com/koaning/cluestar/tree/main\n",
        "+https://www.kaggle.com/code/colinmorris/visualizing-embeddings-with-t-sne/notebook#Did-it-work?\n",
        "+ https://github.com/uber-research/parallax\n",
        "\n",
        "#### Reference(s) for various algorithms:\n",
        "+ https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/\n",
        "+ https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
        "+ https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n",
        "+ https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJDL3FlEcku2"
      },
      "source": [
        "### Prepare Various Data Structures for processing / visualizing results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Idl7khOq77F0"
      },
      "outputs": [],
      "source": [
        "# Prepare the data from a nlp document into an array of terms, then select a single word that you want to compare in the visualization\n",
        "the_words=[]\n",
        "the_categories=[]\n",
        "\n",
        "# Assign Values\n",
        "the_words = [str(x) for x in tokens]\n",
        "target_words=[\"wild\"]\n",
        "\n",
        "# Now make a Panda DataFrame\n",
        "df = pd.DataFrame()\n",
        "df[\"text\"]=the_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tYgiKS1VXA2"
      },
      "outputs": [],
      "source": [
        "# Do these words have vectors in the current model?\n",
        "print(f\"\"\"{BOLD_START}{\"Word\":15}{BOLD_END} | {BOLD_START}{\"Out of Vocabulary(OOV)?\"}{BOLD_END}\"\"\")\n",
        "for token in tokens:\n",
        "    print(f\"{BOLD_START}{str(token):15}{BOLD_END} | {BOLD_START}{token.is_oov}{BOLD_END}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLeTF2bOvGBS"
      },
      "source": [
        "### TF-IDF Vectorization\n",
        "\n",
        "TF-IDF vectorization is a technique in Natural Language Processing (NLP) that converts text data into numerical vectors by assigning a weight to each word based on its \"Term Frequency\" (how often it appears within a document) and \"Inverse Document Frequency\" (how rare it is across the entire corpus of documents), essentially giving more importance to words that are unique to a specific document while downplaying common words across many documents; this allows for better comparison and analysis of text data, particularly in tasks like document classification and information retrieval.\n",
        "\n",
        "#### Key points about TF-IDF vectorization:\n",
        "\n",
        "TF (Term Frequency):\n",
        "+ Measures how often a word appears within a single document.\n",
        "\n",
        "#### IDF (Inverse Document Frequency):\n",
        "+ Measures how rare a word is across all documents in a corpus.\n",
        "\n",
        "#### Calculation:\n",
        "+ To calculate the TF-IDF score for a word, you multiply its Term Frequency by its Inverse Document Frequency.\n",
        "\n",
        "#### Benefits:\n",
        "\n",
        "+ Highlights important words: By weighting words based on their relevance within a document and across the corpus, TF-IDF helps identify the most important keywords.\n",
        "\n",
        "#### Normalization:\n",
        "+  TF-IDF helps to normalize the impact of document length by considering the frequency of words relative to the total number of words in a document.\n",
        "\n",
        "#### Example Use Cases:\n",
        "\n",
        "+ Search engine ranking:When searching for a query, TF-IDF can be used to rank documents based on how relevant the query terms are within each document.\n",
        "+ Text classification: By representing documents as TF-IDF vectors, machine learning models can be trained to classify documents based on their content.\n",
        "+ Document similarity analysis: Comparing the TF-IDF vectors of two documents can reveal how similar they are in terms of their word usage.\n",
        "\n",
        "***In short, tf-idf vectorization gives a numerical representation of words entirely dependent on the nature and number of documents being considered. The same words will have different vector representations in another corpus.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaNgLqqG70yK"
      },
      "outputs": [],
      "source": [
        "# Create the setup for comparison of terms using Tfid\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "pipe = make_pipeline(TfidfVectorizer(), TruncatedSVD())\n",
        "\n",
        "X = pipe.fit_transform(the_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD_S0wWSD3lB"
      },
      "outputs": [],
      "source": [
        "# Map the data from fit-transform into more traditional arrays for plotting\n",
        "xl=[]\n",
        "yl=[]\n",
        "labels=[]\n",
        "\n",
        "for value in X[:]:\n",
        "  xl.append(value[0])\n",
        "  yl.append(value[1])\n",
        "\n",
        "xn=np.array(xl)\n",
        "yn=np.array(yl)\n",
        "\n",
        "for value in tokens:\n",
        "  labels.append(value.text)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def normalize_array(arr):\n",
        "    scaler = MinMaxScaler()\n",
        "    arr = arr.reshape(-1, 1)  # Reshape to 2D array for MinMaxScaler\n",
        "    normalized_arr = scaler.fit_transform(arr)\n",
        "    return normalized_arr.flatten()  # Flatten back to 1D array\n",
        "\n",
        "\n",
        "xs = normalize_array(xn)\n",
        "ys = normalize_array(yn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80G3xJvbCtt2"
      },
      "outputs": [],
      "source": [
        "# Destroy any previous plots so things don't buffer with Agg and what not\n",
        "%matplotlib inline\n",
        "plt.close()\n",
        "\n",
        "# Caption\n",
        "caption_text=f\"XY coordinates plot showing relative similarity of vectorized words with {token1_value} matches to {token2_value} at {highest_sim:.0%}.\"\n",
        "# Plot setup\n",
        "fig, ax = plt.subplots(figsize=(FIGURE_WIDTH,FIGURE_HEIGHT));\n",
        "\n",
        "# Hide the axis value because they mean nothing to us\n",
        "bit_bucket=plt.xticks([]);\n",
        "bit_bucket=plt.yticks([]);\n",
        "\n",
        "bit_bucket=plt.xlabel('');\n",
        "bit_bucket=plt.ylabel('');\n",
        "bit_bucket=plt.title(f'Cosine Similarity using TF-IDF Vectorization');\n",
        "bit_bucket=ax.scatter(x=xs, y=ys,label=labels);\n",
        "bit_bucket=ax.grid(False)\n",
        "#plt.axis('off');\n",
        "\n",
        "\n",
        "# Make the Labels\n",
        "for idx, txt in enumerate(labels):\n",
        "    bit_bucket=ax.annotate(txt,\n",
        "                           (xs[idx], ys[idx]),\n",
        "                           xytext=(xs[idx] + 0.02, ys[idx] + 0.02),\n",
        "                           fontsize=IMG_FONT_SIZE_MIN * 0.75, fontfamily='Serif',\n",
        "                           bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'),\n",
        "                           arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"arc3\"),\n",
        "                          );\n",
        "plt.show()\n",
        "rprint(caption_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ivpoWQuvIl"
      },
      "source": [
        "### Consider Cluestar\n",
        "\n",
        "This library contains visualisation tools that might help you get started with classification tasks. The idea is that if you can inspect clusters easily, you might gain a clue on what good labels for your dataset might be!\n",
        "\n",
        "Reference: https://github.com/koaning/cluestar/tree/main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwUYYpUX72Wc"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "from cluestar import plot_text\n",
        "\n",
        "caption_text=f\"\"\"Interactive graphic showing the distance of each word to another based on TF-IDF Vectorization.\"\"\"\n",
        "plot_text(X, the_words)\n",
        "rprint(caption_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TrZo8hT9g3h"
      },
      "outputs": [],
      "source": [
        "# Another plot using targeted words in a different color to help identify differences in cosine similarity\n",
        "plot_text(X, the_words, color_words=target_words)\n",
        "rprint(caption_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ODTwFLgv3ZF"
      },
      "source": [
        "### t-SNE\n",
        "\n",
        "t-SNE is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map.\n",
        "\n",
        "t-SNE, or t-Distributed Stochastic Neighbor Embedding, is a statistical method for visualizing high-dimensional data by reducing it to lower- dimensional spaces, typically two or three dimensions.\n",
        "\n",
        "This makes it easier to visualize and interpret the data, especially when dealing with complex datasets like those in machine learning and data science.\n",
        "\n",
        "#### Core idea of t-SNE :\n",
        "\n",
        "The core idea behind t-SNE is to map high-dimensional data points to a lower-dimensional space, typically two or three dimensions, in a way that preserves the local relationships between points. It achieves this by measuring the similarity between data points in the high-dimensional space and representing this similarity as probabilities. Then, it constructs a similar probability distribution in the lower-dimensional space and minimizes the difference between the two distributions using a technique called gradient descent. This process allows t-SNE to effectively capture the local structure of the data, making it particularly useful for visualizing complex datasets and discovering meaningful patterns.\n",
        "\n",
        "What is the meaning of preserving local relationships between points ?\n",
        "“Preserving the local relationships between points” in the context of t-SNE refers to maintaining the relative distances and similarities between neighboring data points when they are mapped from a high-dimensional space to a lower-dimensional space.\n",
        "\n",
        "Imagine you have a dataset where similar data points are located close to each other in the high-dimensional space. Preserving local relationships means that after the dimensionality reduction, these similar points should still be positioned closely together in the lower-dimensional space. Similarly, if two data points are dissimilar or distant from each other in the original space, they should remain relatively far apart in the reduced-dimensional space.\n",
        "\n",
        "#### Geometrical Intuition of t-SNE :\n",
        "\n",
        "The concept behind t-SNE involves computing the similarity between each data point and all others points in the dataset. Leveraging these similarity scores, the algorithm reduces the dimensionality of the data to 2D or 1D while preserving the local relationships(i.e. finding the nearest neighbors) among points.\n",
        "\n",
        "#### Reference(s):\n",
        "+ https://medium.com/@sachinsoni600517/mastering-t-sne-t-distributed-stochastic-neighbor-embedding-0e365ee898ea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck0IeEF5SuRK"
      },
      "outputs": [],
      "source": [
        "# Reference: https://grahamholker.com/posts/document-vectors-with-spacy/\n",
        "\n",
        "# Load each word from the clean text into a data structure\n",
        "the_words = [{\"word\":word} for word in clean_body_of_text.split()]\n",
        "\n",
        "# Iterate through the words and store NLP spaCy data\n",
        "for word in the_words:\n",
        "  doc = nlp(word[\"word\"])\n",
        "  word['word']=word[\"word\"]\n",
        "  word['vector'] = doc.vector\n",
        "  word['entities'] = list(doc.ents)\n",
        "\n",
        "# Create a dictionary sufficient for Pandas build\n",
        "word_dict = dict((word['word'], word['vector']) for word in the_words if 'vector' in word)\n",
        "\n",
        "# Build a Panda's data frame\n",
        "df = pd.DataFrame.from_dict(word_dict, orient='index')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKkQb_PNwhFE"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE()\n",
        "tsne_vectors = tsne.fit_transform(df)\n",
        "tsne_vectors = pd.DataFrame(index=df.index, data=tsne_vectors)\n",
        "test = {\n",
        "    'word'    : word_dict.keys(),\n",
        "    'x_coord' : tsne_vectors[0].values,\n",
        "    'y_coord' : tsne_vectors[1].values\n",
        "}\n",
        "tsne_vectors = pd.DataFrame(test, index=pd.Index(df.index))\n",
        "tsne_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cu6pZ135w5kO"
      },
      "outputs": [],
      "source": [
        "# Import Bokeh to diplay solution\n",
        "\n",
        "rprint(\"Make sure Bokeh is installed.\")\n",
        "\n",
        "# Encountered problems in session where GPU was utilized and System encoding, in session, changes to ANSI*, normalize back to UTF-8\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# Identify the libraries you'd like to add to this Runtime environment.\n",
        "libraries=[\"bokeh\",]\n",
        "\n",
        "# Loop through each library and test for existence, if not present install quietly\n",
        "for library in libraries:\n",
        "    if library == \"Pillow\":\n",
        "      spec = importlib.util.find_spec(\"PIL\")\n",
        "    else:\n",
        "      spec = importlib.util.find_spec(library)\n",
        "    if spec is None:\n",
        "      print(\"Installing library \" + library)\n",
        "      subprocess.run([\"pip\", \"install\" , library, \"--quiet\"], check=True)\n",
        "    else:\n",
        "      print(\"Library \" + library + \" already installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdMGYMHqwx4R"
      },
      "outputs": [],
      "source": [
        "from bokeh.plotting import figure, show, output_notebook\n",
        "from bokeh.models import HoverTool, ColumnDataSource,  Label, LabelSet, Range1d\n",
        "\n",
        "output_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKAj6QKzxKRA"
      },
      "outputs": [],
      "source": [
        "# Source -> https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb\n",
        "\n",
        "plot_data = ColumnDataSource(data=tsne_vectors)\n",
        "\n",
        "# create the plot and configure the\n",
        "# title, dimensions, and tools\n",
        "tsne_plot = figure(\n",
        "                   title=u't-SNE Word Embeddings',\n",
        "                   sizing_mode=\"stretch_width\",\n",
        "                   max_width=1024,\n",
        "                   height=768,\n",
        "                   tools= (u'pan, wheel_zoom, box_zoom,'\n",
        "                           u'box_select, reset'),\n",
        "                   active_scroll=u'wheel_zoom'\n",
        "                  )\n",
        "\n",
        "# add a hover tool to display words on roll-over\n",
        "tsne_plot.add_tools( HoverTool(tooltips = u'@index') )\n",
        "\n",
        "\n",
        "# draw the words as circles on the plot\n",
        "tsne_plot.scatter(x=u'x_coord',\n",
        "                 y=u'y_coord',\n",
        "                 source=plot_data,\n",
        "                 line_alpha=0.2,\n",
        "                 fill_alpha=1,\n",
        "                 size=10,\n",
        "                 hover_line_color=u'black')\n",
        "\n",
        "# configure visual elements of the plot\n",
        "labels = LabelSet(x='x_coord', y='y_coord', text='word',\n",
        "              x_offset=5, y_offset=5, source=plot_data)\n",
        "\n",
        "tsne_plot.add_layout(labels)\n",
        "\n",
        "tsne_plot.title.text_font_size = u'16pt'\n",
        "tsne_plot.xaxis.visible = True\n",
        "tsne_plot.yaxis.visible = True\n",
        "tsne_plot.grid.grid_line_color = u'black'\n",
        "tsne_plot.outline_line_color = u'black'\n",
        "\n",
        "\n",
        "# engage!\n",
        "show(tsne_plot);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPqYNEaU2lhj"
      },
      "source": [
        "# Same functions can be performed on sentences, just use a sentence transformer."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "STEM-001_WordClouds.ipynb",
      "provenance": []
    },
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m125",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bc70dba7b1b74daeb7514e961bb8b598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e86c32652bad4585a43e71be5355912c",
              "IPY_MODEL_110cb1fed2c84ed89ec0a33e1b711f2e",
              "IPY_MODEL_ea07eed901d24c489c652ffe19e4138d"
            ],
            "layout": "IPY_MODEL_4d0018f6d7ea47bcaa3cade07a7ca9fa"
          }
        },
        "e86c32652bad4585a43e71be5355912c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e54cb10db5f94b238084afe35556898b",
            "placeholder": "​",
            "style": "IPY_MODEL_e8e7e7c506e94d399825313ad1bb62ff",
            "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n\n        <table><tr><td>\n            <span style=\"font-family: Tahoma;font-size: 18\">\n              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n              Please verify that you are in the appropriate project and that the:</br>\n              <center><code><b>PROJECT_ID</b></code> </br></center>\n              aligns with the Project Id in the upper left corner of this browser and that the location:\n              <center><code><b>LOCATION</b></code> </br></center>\n              aligns with the instructions provided.\n            </span>\n          </td></tr></table></br></br>\n\n    "
          }
        },
        "110cb1fed2c84ed89ec0a33e1b711f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26929618524e4d179a3fd1b89a1827e9",
              "IPY_MODEL_acfb80d1c3bb492e9316a8b70547fd43"
            ],
            "layout": "IPY_MODEL_a1ef0e195dbe4c21ab0ca765941347fd"
          }
        },
        "ea07eed901d24c489c652ffe19e4138d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef6627886544486ab7cdce43dcc1ecad",
            "placeholder": "​",
            "style": "IPY_MODEL_c7b9e052f53b44e3bbc81deec236ebc3",
            "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n          "
          }
        },
        "4d0018f6d7ea47bcaa3cade07a7ca9fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "e54cb10db5f94b238084afe35556898b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8e7e7c506e94d399825313ad1bb62ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26929618524e4d179a3fd1b89a1827e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "ai-bootcamp",
              "I will setup my own"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Set Your Project:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_e21f816f968e47378912a2f2842e10d0",
            "style": "IPY_MODEL_97434169ba3449e688592ecfd589f556"
          }
        },
        "acfb80d1c3bb492e9316a8b70547fd43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Accept",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_7ab73300cabd4b578800afe02059d7a2",
            "style": "IPY_MODEL_65ee23dcd53040519d09c94be9fcb027",
            "tooltip": ""
          }
        },
        "a1ef0e195dbe4c21ab0ca765941347fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef6627886544486ab7cdce43dcc1ecad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7b9e052f53b44e3bbc81deec236ebc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e21f816f968e47378912a2f2842e10d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97434169ba3449e688592ecfd589f556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ab73300cabd4b578800afe02059d7a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65ee23dcd53040519d09c94be9fcb027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}