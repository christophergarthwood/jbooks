{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "Jg3iJooMQjWA",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Artificial Intelligence\n",
    "## AI Ready Data - 006\n",
    "###  Process Profile data using various techniques of each dataset loaded\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<table align=\"center\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/christophergarthwood/jbooks/blob/main/STEM-006_AIReadyData-Speed-Tests-003.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/notebooks?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Link to Colab Enterprise\n",
    "    </a>\n",
    "  </td>   \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/christophergarthwood/jbooks/blob/main/STEM-006_AIReadyData-Speed-Tests-003.ipynb\">\n",
    "      <img width=32 src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances?referrer=search&hl=en&project=usfs-ai-bootcamp\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Link to Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "</center>\n",
    "</br></br></br>\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Christopher G Wood](https://github.com/christophergarthwood)  |\n",
    "\n",
    "# Overview\n",
    "\n",
    "Using output from 002, read the pickled profile results and gather some metrics for additional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1737665372909,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "M3qlCehNBu-_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's define some variables (information holders) for our project overall\n",
    "\n",
    "global PROJECT_ID, BUCKET_NAME, LOCATION\n",
    "BUCKET_NAME = \"ai-bootcamp-vertex-colab\"\n",
    "PROJECT_ID = \"ai-bootcamp\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "BOLD_START = \"\\033[1m\"\n",
    "BOLD_END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zramkw-P93C-"
   },
   "source": [
    "## Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1737666212893,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "shY7a4DVQjWB",
    "outputId": "606fe1d2-f3de-47cf-f457-bb449cca5b52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# - Google Colab Check\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "import datetime\n",
    "\n",
    "RunningInCOLAB = False\n",
    "RunningInCOLAB = \"google.colab\" in str(get_ipython())\n",
    "current_time = datetime.datetime.now()\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    print(\n",
    "        f\"You are running this notebook in Google Colab at {current_time} in the {BOLD_START}{PROJECT_ID}{BOLD_END}lab.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"You are likely running this notebook with Jupyter iPython runtime at {current_time} in the {PROJECT_ID} lab.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZVkISRuLURi"
   },
   "source": [
    "## Library Management\n",
    "### Load Libraries necessary for this operation via pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1737665373426,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "logDyNfnLURj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import key libraries necessary to support dynamic installation of additional libraries\n",
    "import sys\n",
    "\n",
    "# Use subprocess to support running operating system commands from the program, using the \"bang\" (!)\n",
    "# symbology is supported, however that does not translate to an actual python script, this is a more\n",
    "# agnostic approach.\n",
    "import subprocess\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74882,
     "status": "ok",
     "timestamp": 1737665448288,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "ldXG-5fhsV1e",
    "outputId": "5941b5ee-93f4-41df-ccc2-8915e0e73ed7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the libraries you'd like to add to this Runtime environment.\n",
    "# Commented out as this adds time but is critical for initial run.\n",
    "\"\"\"\n",
    "libraries = [\n",
    "    \"backoff\",\n",
    "    \"python-dotenv\",\n",
    "    \"seaborn\",\n",
    "    \"piexif\",\n",
    "    \"unidecode\",\n",
    "    \"icecream\",\n",
    "    \"watermark\",\n",
    "    \"watermark[GPU]\",\n",
    "    \"rich\",\n",
    "    \"rich[jupyter]\",\n",
    "    \"numpy\",\n",
    "    \"pydot\",\n",
    "    \"polars[all]\",\n",
    "    \"dask[complete]\",\n",
    "    \"xarray\",\n",
    "    \"pandas\",\n",
    "    \"pystac\",\n",
    "    \"pystac[jinja2]\",\n",
    "    \"pystac[orjson]\",\n",
    "    \"pystac[validation]\",\n",
    "    \"fastparquet\",\n",
    "    \"zarr\",\n",
    "    \"gdown\",\n",
    "    \"wget\",\n",
    "]\n",
    "\n",
    "# Loop through each library and test for existence, if not present install quietly\n",
    "for library in libraries:\n",
    "    if library == \"Pillow\":\n",
    "        spec = importlib.util.find_spec(\"PIL\")\n",
    "    else:\n",
    "        spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "        print(\"Installing library \" + library)\n",
    "        subprocess.run([\"pip\", \"install\", library, \"--quiet\"], check=True)\n",
    "    else:\n",
    "        print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "# Specialized install for GPU enabled capability with CUDF\n",
    "# pip install --extra-index-url=https://pypi.nvidia.com \"cudf-cu12==25.2.*\" \"dask-cudf-cu12==25.2.*\" \"cuml-cu12==25.2.*\" \"cugraph-cu12==25.2.*\" \"nx-cugraph-cu12==25.2.*\" \"cuspatial-cu12==25.2.*\"     \"cuproj-cu12==25.2.*\" \"cuxfilter-cu12==25.2.*\" \"cucim-cu12==25.2.*\"\n",
    "try:\n",
    "    library=\"cudf-cu12\"\n",
    "    spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"pip\",\n",
    "                \"install\",\n",
    "                \"--extra-index-url=https://pypi.nvidia.com\",\n",
    "                library,\n",
    "                \"--quiet\",\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "    library=\"dask-cudf-cu12\"\n",
    "    spec = importlib.util.find_spec(library)\n",
    "    if spec is None:\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"pip\",\n",
    "                \"install\",\n",
    "                \"--extra-index-url=https://pypi.nvidia.com\",\n",
    "                library,\n",
    "                \"--quiet\",\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Library \" + library + \" already installed.\")\n",
    "\n",
    "except (subprocess.CalledProcessError, RuntimeError, Exception) as e:\n",
    "    print(repr(e))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO_Hq5eq9joH"
   },
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5041,
     "status": "ok",
     "timestamp": 1737665453315,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "PJuXEPlkSo9p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Import additional libraries that add value to the project related to NLP\n",
    "\n",
    "# - Set of libraries that perhaps should always be in Python source\n",
    "import backoff\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "import gc\n",
    "import getopt\n",
    "import glob\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "import socket\n",
    "import sys\n",
    "import textwrap\n",
    "import tqdm\n",
    "import traceback\n",
    "import warnings\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "#- Datastructures\n",
    "from dataclasses import dataclass, fields, field\n",
    "from typing import List\n",
    "\n",
    "#- Profiling\n",
    "from time import perf_counter\n",
    "import gc\n",
    "import io\n",
    "import tracemalloc\n",
    "import psutil\n",
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "#- Text formatting\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.traceback import install\n",
    "from tabulate import tabulate\n",
    "import locale\n",
    "\n",
    "# - Displays system info\n",
    "from watermark import watermark as the_watermark\n",
    "from py3nvml import py3nvml\n",
    "\n",
    "# - Additional libraries for this work\n",
    "import math\n",
    "from base64 import b64decode\n",
    "from IPython.display import Image, Markdown\n",
    "import pandas, IPython.display as display, io, jinja2, base64\n",
    "from IPython.display import clear_output  # used to support real-time plotting\n",
    "import requests\n",
    "import unidecode\n",
    "import pydot\n",
    "import wget\n",
    "\n",
    "# - Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import dask as da\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "import xarray as xr\n",
    "import cupy_xarray  # never actually invoked in source itself use ds=ds.cupy.as_cupy()\n",
    "import pystac as pys\n",
    "import pystac\n",
    "from pystac.utils import datetime_to_str\n",
    "\n",
    "# - Statistics\n",
    "import statistics\n",
    "\n",
    "# from stacframes import df_from\n",
    "import fastparquet as fq\n",
    "import zarr\n",
    "from zarr import Group\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "try:\n",
    "    import cudf\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import cupy\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Tensorflow and related AI libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import data as tf_data\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "\n",
    "# - Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.cbook import get_sample_data\n",
    "from matplotlib.offsetbox import AnnotationBbox, DrawingArea, OffsetImage, TextArea\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.patches import Circle\n",
    "from PIL import Image as PIL_Image\n",
    "import PIL.ImageOps\n",
    "import matplotlib.image as mpimg\n",
    "from imageio import imread\n",
    "import seaborn as sns\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from pylab import *\n",
    "\n",
    "# - Image meta-data for Section 508 compliance\n",
    "import piexif\n",
    "from piexif.helper import UserComment\n",
    "\n",
    "# - Progress bar\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataclass used to represent each metric used during execution\n",
    "#\n",
    "@dataclass\n",
    "class aggregate_metrics:\n",
    "    id: str\n",
    "\n",
    "    runtime: List[float] = field(default_factory=list)\n",
    "    \n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_time: List[float] = field(default_factory=list)\n",
    "    \n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_time: List[float] = field(default_factory=list)\n",
    "    \n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_count: List[float] = field(default_factory=list)\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_count: List[float] = field(default_factory=list)\n",
    "\n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_throughput: List[float] = field(default_factory=list)\n",
    "\n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_throughput: List[float] = field(default_factory=list)\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_read_count: List[float] = field(default_factory=list)\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_write_count: List[float] = field(default_factory=list)\n",
    "    \n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_read_throughput: List[float] = field(default_factory=list)\n",
    "    \n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_write_throughput: List[float] = field(default_factory=list)\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_read_time: List[float] = field(default_factory=list)\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_write_time: List[float] = field(default_factory=list)\n",
    "\n",
    "    # calculated in MBs, reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "    mem_current: List[float] = field(default_factory=list)\n",
    "\n",
    "    # calculated in MBs, reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "    mem_peak: List[float] = field(default_factory=list)\n",
    "\n",
    "    def mean_excluding_zero(self,data):\n",
    "        \"\"\"Calculates the mean of a list, excluding zero values.\"\"\"\n",
    "        filtered_data = [x for x in data if x != 0]\n",
    "        # Alternatively: filtered_data = list(filter(lambda x: x != 0, data))\n",
    "        if not filtered_data:\n",
    "            return 0 # Or raise an exception if no non-zero values exist\n",
    "        return statistics.mean(filtered_data)\n",
    "\n",
    "    def calculate_stats(self, field_name) -> List:\n",
    "       my_stats=[]\n",
    "       my_field_data=getattr(self, field_name)\n",
    "       my_field_data = [float(x) for x in my_field_data]\n",
    "       #my_stats.append(statistics.mean(my_field_data))\n",
    "       my_stats.append(self.mean_excluding_zero(my_field_data))\n",
    "       my_stats.append(statistics.median(my_field_data))\n",
    "       my_stats.append(statistics.mode(my_field_data))\n",
    "       my_stats.append(statistics.stdev(my_field_data))\n",
    "       my_stats.append(statistics.variance(my_field_data))\n",
    "       #return my_stats\n",
    "\n",
    "       #resultant=[\"^\".join(str(element) for element in my_stats)]\n",
    "       resultant=\"^\".join(str(element) for element in my_stats)\n",
    "       return resultant\n",
    "        \n",
    "    def __str__(self):\n",
    "             #id, runtime, disk read counts, disk write counts, general disk read counts, general disk write counts, target disk read time, target disk write time, general disk read time, general disk write time, memory current, memory peak\n",
    "             #{self.id}^{self.calculate_stats(\"runtime\")}^{self.calculate_stats(\"io_disk_read_count\")}^{self.calculate_stats(\"io_disk_write_count\")}^{self.calculate_stats(\"io_os_read_count\")}^{self.calculate_stats(\"io_os_write_count\")}^{self.calculate_stats(\"io_disk_read_time\")}^{self.calculate_stats(\"io_disk_write_time\")}^{self.calculate_stats(\"io_os_read_time\")}^{self.calculate_stats(\"io_os_write_time\")}^{self.calculate_stats(\"mem_current\")}^{self.calculate_stats(\"mem_peak\")} \n",
    "     return f\"\"\"\n",
    "             {self.id}^{self.calculate_stats(\"runtime\")}^{self.calculate_stats(\"mem_current\")}^{self.calculate_stats(\"mem_peak\")} \n",
    "             \"\"\" \n",
    "\n",
    "    #def __strs__(self):\n",
    "    # return f\"\"\"\n",
    "    #         Id---------------------------------------------\n",
    "    #                               Id: {self.id}\n",
    "    #         Runtime----------------------------------------\n",
    "    #               Runtime Stats:    {self.calculate_stats(\"runtime\")} milliseconds\n",
    "#\n",
    "#             I/O Counts-------------------------------------\n",
    "#                   Targeted disk read: {self.calculate_stats(\"io_disk_read_count\")} counts\n",
    "#                  Targeted disk write: {self.calculate_stats(\"io_disk_write_count\")} counts\n",
    "#                    General disk read: {self.calculate_stats(\"io_os_read_count\")} counts\n",
    "#                   General disk write: {self.calculate_stats(\"io_os_write_count\")} counts\n",
    "#\n",
    "#             I/O Time---------------------------------------\n",
    "#              Targeted disk read time: {self.calculate_stats(\"io_disk_read_time\")} milliseconds\n",
    "#             Targeted disk write time: {self.calculate_stats(\"io_disk_write_time\")} milliseconds\n",
    "#               General disk read time: {self.calculate_stats(\"io_os_read_time\")} milliseconds\n",
    "#              General disk write time: {self.calculate_stats(\"io_os_write_time\")} milliseconds\n",
    "#\n",
    "#             Memory------------------------------------------\n",
    "#                              Current: {self.calculate_stats(\"mem_current\")} MB\n",
    "#                                 Peak: {self.calculate_stats(\"mem_peak\")} MB\n",
    "#\n",
    "#             \"\"\"         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class runtime_metrics:\n",
    "    id: str\n",
    "\n",
    "    runtime: float = field(default=0.0)\n",
    "\n",
    "    # reference: https://docs.python.org/4/library/profile.html\n",
    "    profile_data: cProfile.Profile = field(init=False)\n",
    "\n",
    "    # reference: https://www.geeksforgeeks.org/how-to-get-file-size-in-python/\n",
    "    file_size: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_time: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_time: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "    \n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_count: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_count: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_read_throughput: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # bytes read [end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_disk_write_throughput: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_read_count: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # number read operations[end-begin], reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_write_count: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_read_time: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # milliseconds, reference: https://stackoverflow.com/questions/24723092/using-python-to-measure-in-situ-read-write-speed-for-files\n",
    "    io_os_write_time: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # calculated in MBs, reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "    mem_current: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # calculated in MBs, reference: https://www.geeksforgeeks.org/monitoring-memory-usage-of-a-running-python-program/\n",
    "    mem_peak: float = field(\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "                Id---------------------------------------------\n",
    "                                      Id: {self.id}\n",
    "                Runtime----------------------------------------\n",
    "                                 Runtime: {self.runtime:,.8f} milliseconds\n",
    "\n",
    "                I/O Size---------------------------------------\n",
    "                               File Size: {self.file_size:,.8f} bytes\n",
    "\n",
    "                I/O Counts-------------------------------------\n",
    "                      Targeted disk read: {self.io_disk_read_count:,.8f} counts\n",
    "                     Targeted disk write: {self.io_disk_write_count:,.8f} counts\n",
    "                       General disk read: {self.io_os_read_count:,.8f} counts\n",
    "                      General disk write: {self.io_os_write_count:,.8f} counts\n",
    "\n",
    "                I/O Throughput----------------------------------\n",
    "                 Targeted disk read bytes: {self.io_disk_read_throughput:,.8f} bytes\n",
    "                Targeted disk write bytes: {self.io_disk_write_throughput:,.8f} bytes\n",
    "                  General disk read bytes: {self.io_os_write_throughput:,.8f} bytes\n",
    "                 General disk write bytes: {self.io_os_write_throughput:,.8f} bytes                      \n",
    "\n",
    "                I/O Time---------------------------------------\n",
    "                 Targeted disk read time: {self.io_disk_read_time:,.8f} milliseconds\n",
    "                Targeted disk write time: {self.io_disk_write_time:,.8f} milliseconds\n",
    "                  General disk read time: {self.io_os_read_time:,.8f} milliseconds\n",
    "                 General disk write time: {self.io_os_write_time:,.8f} milliseconds\n",
    "\n",
    "                Memory------------------------------------------\n",
    "                                 Current: {self.mem_current:,.8f} MB\n",
    "                                    Peak: {self.mem_peak:,.8f} MB\n",
    "\n",
    "                \"\"\"\n",
    "    #def __repr__(self):\n",
    "    #    return f'{self.__class__.__name__}(name={self.name!r}, unit_price={self.unit_price!r}, quantity={self.quantity_on_hand!r})'\n",
    "\n",
    "    # TODO - CGW\n",
    "    # def __post_init__(self):\n",
    "    #    self.id = f'{self.phrase}_{self.word_type.name.lower()}'\n",
    "\n",
    "    # worthy consideration - https://www.geeksforgeeks.org/psutil-module-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FUa8QJT9tw_"
   },
   "source": [
    "## Function Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lib Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1737665453699,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "v_CqUVLZ98Mz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lib_diagnostics() -> None:\n",
    "\n",
    "    import pkg_resources\n",
    "\n",
    "    package_name_length = 20\n",
    "    package_version_length = 10\n",
    "\n",
    "    # Show notebook details\n",
    "    #%watermark?\n",
    "    #%watermark --github_username christophergwood --email christopher.g.wood@gmail.com --date --time --iso8601 --updated --python --conda --hostname --machine --githash --gitrepo --gitbranch --iversions --gpu\n",
    "    # Watermark\n",
    "    print(\n",
    "        the_watermark(\n",
    "            author=f\"{AUTHOR_NAME}\",\n",
    "            github_username=f\"GITHUB_USERNAME\",\n",
    "            email=f\"{AUTHOR_EMAIL}\",\n",
    "            iso8601=True,\n",
    "            datename=True,\n",
    "            current_time=True,\n",
    "            python=True,\n",
    "            updated=True,\n",
    "            hostname=True,\n",
    "            machine=True,\n",
    "            gitrepo=True,\n",
    "            gitbranch=True,\n",
    "            githash=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f\"{BOLD_START}Packages:{BOLD_END}\")\n",
    "    print(\"\")\n",
    "    # Get installed packages\n",
    "    the_packages = [\n",
    "        \"nltk\",\n",
    "        \"numpy\",\n",
    "        \"os\",\n",
    "        \"pandas\",\n",
    "        \"keras\",\n",
    "        \"seaborn\",\n",
    "        \"fastparquet\",\n",
    "        \"zarr\",\n",
    "        \"dask\",\n",
    "        \"pystac\",\n",
    "        \"polars\",\n",
    "        \"xarray\",\n",
    "    ]  # Functions are like legos that do one thing, this function outputs library version history of effort.\n",
    "\n",
    "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "    for package_idx, package_name in enumerate(installed):\n",
    "        if package_name in the_packages:\n",
    "            installed_version = installed[package_name]\n",
    "            print(\n",
    "                f\"{package_name:<40}#: {str(pkg_resources.parse_version(installed_version)):<20}\"\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        print(f\"{'TensorFlow version':<40}#: {str(tf.__version__):<20}\")\n",
    "        print(\n",
    "            f\"{'     gpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{'     cpu.count:':<40}#: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        print(f\"{'Torch version':<40}#: {str(torch.__version__):<20}\")\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(f\"{'     GPUs available?':<40}#: {torch.cuda.is_available()}\")\n",
    "            print(f\"{'     count':<40}#: {torch.cuda.device_count()}\")\n",
    "            print(f\"{'     current':<40}#: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"No GPU available, using CPU.\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        print(f\"{'OpenAI Azure Version':<40}#: {str(the_openai_version):<20}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libary Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_library_configuration() -> None:\n",
    "\n",
    "    ############################################\n",
    "    # - JUPYTER NOTEBOOK OUTPUT CONTROL / FORMATTING\n",
    "    ############################################\n",
    "    # pandas set floating point to 4 places to things don't run loose\n",
    "    debug.msg_info(\"Setting Pandas and Numpy library options.\")\n",
    "    pd.set_option(\n",
    "        \"display.max_colwidth\", 10\n",
    "    )  # None if you want to view the full json blob in the printed dataframe, use this\n",
    "    pd.options.display.float_format = \"{:,.4f}\".format\n",
    "    np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Exception Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this function displays the stack trace on errors from a central location making adjustments to the display on an error easier to manage\n",
    "# functions perform useful solutions for highly repetitive code\n",
    "def process_exception(inc_exception: Exception) -> None:\n",
    "    if DEBUG_STACKTRACE == 1:\n",
    "        traceback.print_exc()\n",
    "        console.print_exception(show_locals=True)\n",
    "    else:\n",
    "        rprint(repr(inc_exception))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTrnAspo3CWR"
   },
   "source": [
    "#### Check your resources from a CPU/GPU perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1737665454280,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 360
    },
    "id": "hmGUvC7M3B0H",
    "outputId": "638ecead-d943-42f2-a2eb-99790d7fe3bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hardware_stats() -> None:\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    print(\n",
    "        f\"{BOLD_START}List Devices{BOLD_END} #########################################\"\n",
    "    )\n",
    "    try:\n",
    "        from tensorflow.python.client import device_lib\n",
    "\n",
    "        rprint(device_lib.list_local_devices())\n",
    "        print(\"\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    print(\n",
    "        f\"{BOLD_START}Devices Counts{BOLD_END} ########################################\"\n",
    "    )\n",
    "    try:\n",
    "        rprint(\n",
    "            f\"Num GPUs Available: {str(len(tf.config.experimental.list_physical_devices('GPU')))}\"\n",
    "        )\n",
    "        rprint(\n",
    "            f\"Num CPUs Available: {str(len(tf.config.experimental.list_physical_devices('CPU')))}\"\n",
    "        )\n",
    "        print(\"\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    print(\n",
    "        f\"{BOLD_START}Optional Enablement{BOLD_END} ####################################\"\n",
    "    )\n",
    "    try:\n",
    "        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        rprint(str(repr(e)))\n",
    "\n",
    "    if gpus:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        try:\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "            rprint(\n",
    "                str(\n",
    "                    str(len(gpus))\n",
    "                    + \" Physical GPUs,\"\n",
    "                    + str(len(logical_gpus))\n",
    "                    + \" Logical GPU\"\n",
    "                )\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            # Visible devices must be set before GPUs have been initialized\n",
    "            rprint(str(repr(e)))\n",
    "        print(\"\")\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46JMSTY2QjWD"
   },
   "source": [
    "## Input Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_profiles(inc_source_filenames: []) -> []:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    the_list = []\n",
    "    failed_read = []\n",
    "\n",
    "    rprint(f\"...reading pickled profile data from list of {len(inc_source_filenames)} files:\")\n",
    "    for target_filename in inc_source_filenames:\n",
    "        try:\n",
    "            rprint(f\"......reading profile ({target_filename})\")\n",
    "            #the_netcdf = Dataset(target_filename, \"r\", format=\"NETCDF4\")\n",
    "            with (open(target_filename, \"rb\")) as openfile:\n",
    "                the_list.append(pickle.load(openfile))\n",
    "        except Exception as e:\n",
    "            process_exception(e)\n",
    "            print(f\"...ERROR, investigate this failed read.\")\n",
    "            failed_read.append(target_filename)\n",
    "\n",
    "    print(f\"......{len(the_list)}  of {len(inc_source_filenames)} files successfully read in.\")\n",
    "    print(f\"......{len(failed_read)}  of {len(inc_source_filenames)} files failed to read in.\")\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "    return the_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def add_dataclass_values(a: aggregate_metrics, b: runtime_metrics) -> aggregate_metrics:\n",
    "def add_dataclass_values(a: aggregate_metrics, b: runtime_metrics):\n",
    "    \"\"\"Modified to append to an array. \"\"\"\n",
    "    field_metadata = a.__dataclass_fields__\n",
    "    #for field in fields(a):\n",
    "    for field_name in field_metadata:\n",
    "        if field_name not in \"id\":\n",
    "            field_list = getattr(a, field_name)\n",
    "            new_value  = getattr(b, field_name)\n",
    "            field_list.append(float(new_value))\n",
    "            #setattr(a, field_name, field_list)\n",
    "            #print(f\"{field_list} - {new_value}\")\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_dataclass_values(a: aggregate_metrics, b: runtime_metrics) -> runtime_metrics:\n",
    "    \"\"\"Calculates the statistics of all data class values added to this point\"\"\"\n",
    "    for field in fields(a):\n",
    "        if field.name not in \"profile_data\" and field.name not in \"id\":\n",
    "            setattr(b, field.name, getattr(a,statistics.mean(field.name)))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_outputs(inc_files:[], inc_pattern:str) -> None:\n",
    "    \n",
    " current_pattern=[]\n",
    " for idx, filename in enumerate(inc_files):\n",
    "     match = re.search(f\"{inc_pattern}\", filename)\n",
    "     if match:\n",
    "         current_pattern.append(filename)\n",
    "\n",
    " dataset_aggregated=aggregate_metrics(id=inc_pattern)\n",
    " for profiler_data in current_pattern:\n",
    "     with open(profiler_data, 'rb') as file:\n",
    "         single_profile = pickle.load(file)\n",
    "         #print(single_profile)\n",
    "         dataset_aggregrated=add_dataclass_values(dataset_aggregated, single_profile)\n",
    "         \n",
    " return dataset_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_patterns(inc_files:[]) -> []:\n",
    "  delimiter=\"_\"\n",
    "  split_index=2\n",
    "  patterns=set() \n",
    "  for idx, filename in enumerate(inc_files):\n",
    "      filename_pattern=filename.split(delimiter, )[split_index:-1]\n",
    "      #print(\"_\".join(filename_pattern))\n",
    "      patterns.add(\"_\".join(filename_pattern))\n",
    "      \n",
    "  return list(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main routine that executes all code, does return a data frame of data for further analysis if desired.\n",
    "#\n",
    "#  @param (None)\n",
    "def process(inc_input_directory: str, ) -> {}:\n",
    "\n",
    "    rprint(f\"Entering {__name__} {inspect.stack()[0][3]}\")\n",
    "\n",
    "    # identify target files\n",
    "    for idx, value in enumerate([\"write\", \"read\"]):\n",
    "        #iterate through each data profile saved and gather metrics\n",
    "        #create a list of unique profiles (per type) and process them\n",
    "        source_filenames_list = []\n",
    "        unique_patterns = []\n",
    "        stats = []\n",
    "        print(f\"...marshaling {value} data files:\")\n",
    "        target_directory = f\"{inc_input_directory}{os.sep}\"\n",
    "        if os.path.isdir(target_directory):\n",
    "            for file in os.listdir(target_directory):\n",
    "                filename, file_extension = os.path.splitext(file)\n",
    "                if OUTPUT_PICKLE_EXT.lower() in file_extension.lower():\n",
    "                    if filename.find(f\"_{value}_\") > -1:\n",
    "                        source_filenames_list.append(os.path.join(target_directory, file))\n",
    "        else:\n",
    "            print(\n",
    "                \"Target directory ({target_directory}) does not exist, cannot continue execution.  Check your paths.\"\n",
    "            )\n",
    "            raise SystemError\n",
    "    \n",
    "        source_filenames_list = sorted(source_filenames_list)\n",
    "        print(f\"Found {len(source_filenames_list)} potential target files.\")\n",
    "        unique_patterns = get_unique_patterns(source_filenames_list)\n",
    "        print(\"    Patterns found are:\")\n",
    "        for idx, value in enumerate(unique_patterns):\n",
    "            #print(f\"    ...{value}\")\n",
    "            stats.append(analyze_outputs(source_filenames_list, value))\n",
    "            #print(\"    ##########################################################################################\")\n",
    "            print(f\"    {stats[-1]}\")\n",
    "            #print(\"    ##########################################################################################\")\n",
    "            #print(f\"{stats[-1]}\")\n",
    "            #print(\"\")\n",
    "            #print(\"\")\n",
    "\n",
    "    rprint(f\"Exiting {__name__} {inspect.stack()[0][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Routine (call all other routines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # note that this design now deviates from previous methods.\n",
    "    # Implementation will assume a single execution of a single PIID folder, scanning results and\n",
    "    # appending metrics to a single ASCII file as the code proceeds thus ensuring multi-processor, *nix driven execution.\n",
    "\n",
    "    start_t = perf_counter()\n",
    "    print(\"BEGIN PROGRAM\")\n",
    "\n",
    "    ############################################\n",
    "    # CONSTANTS\n",
    "    ############################################\n",
    "\n",
    "    # Semantic Versioning\n",
    "    VERSION_NAME = \"MLDATAREADY_ANALYSIS\"\n",
    "    VERSION_MAJOR = 0\n",
    "    VERSION_MINOR = 0\n",
    "    VERSION_RELEASE = 2\n",
    "\n",
    "    DATA_VERSION_RELEASE = \"-\".join(\n",
    "        [\n",
    "            str(VERSION_NAME),\n",
    "            str(VERSION_MAJOR),\n",
    "            str(VERSION_MINOR),\n",
    "            str(VERSION_RELEASE),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # OUTPUT EXTENSIONS\n",
    "    OUTPUT_PICKLE_EXT = \"pkl\"\n",
    "    OUTPUT_PANDAS_EXT = \"pkl\"\n",
    "    OUTPUT_NUMPY_EXT = \"npy\"\n",
    "    OUTPUT_TORCH_EXT = \"pt\"\n",
    "    OUTPUT_XARRAY_EXT = \"xr\"\n",
    "    OUTPUT_ZARR_EXT = \"zarr\"\n",
    "    OUTPUT_PARQUET_EXT = \"parquet\"\n",
    "    OUTPUT_TENSORFLOW_EXT = \"tf\"\n",
    "    OUTPUT_PYSTAC_EXT = \"psc\"\n",
    "    OUTPUT_DASK_EXT = \"dask\"\n",
    "    # location of our working files\n",
    "    # WORKING_FOLDER=\"/content/folderOnColab\"\n",
    "    WORKING_FOLDER = \"./folderOnColab/ANALYSIS3/test\"\n",
    "    input_directory = \"./folderOnColab/ANALYSIS3/test/\"\n",
    "    output_directory = \"./folderOnColab/ANALYSIS3/test/\"\n",
    "\n",
    "    # Notebook Author details\n",
    "    AUTHOR_NAME = \"Christopher G Wood\"\n",
    "    GITHUB_USERNAME = \"christophergarthwood\"\n",
    "    AUTHOR_EMAIL = \"christopher.g.wood@gmail.com\"\n",
    "\n",
    "    # GEOSPATIAL NAMES\n",
    "    LAT_LNAME = \"latitude\"\n",
    "    LAT_SNAME = \"lat\"\n",
    "    LONG_LNAME = \"longitude\"\n",
    "    LONG_SNAME = \"lon\"\n",
    "    #PRODUCT_LNAME = \"chlor_a\"\n",
    "    #PRODUCT_SNAME = \"chlor_a\"\n",
    "    PRODUCT_LNAME = \"cld_amt\"\n",
    "    PRODUCT_SNAME = \"cld_amt\"\n",
    "\n",
    "    # PRODUCT_LNAME=\"salinity\"\n",
    "    # PRODUCT_SNAME=\"salinity\"\n",
    "\n",
    "    # Encoding\n",
    "    ENCODING = \"utf-8\"\n",
    "    os.environ[\"PYTHONIOENCODING\"] = ENCODING\n",
    "\n",
    "    BOLD_START = \"\\033[1m\"\n",
    "    BOLD_END = \"\\033[0;0m\"\n",
    "    TEXT_WIDTH = 77\n",
    "\n",
    "    # You can also adjust the verbosity by changing the value of TF_CPP_MIN_LOG_LEVEL:\n",
    "    #\n",
    "    # 0 = all messages are logged (default behavior)\n",
    "    # 1 = INFO messages are not printed\n",
    "    # 2 = INFO and WARNING messages are not printed\n",
    "    # 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "    TF_CPP_MIN_LOG_LEVEL_SETTING = 0\n",
    "\n",
    "    # Set the Seed for the experiment (ask me why?)\n",
    "    # seed the pseudorandom number generator\n",
    "    # THIS IS ESSENTIAL FOR CONSISTENT MODEL OUTPUT, remember these are random in nature.\n",
    "    # SEED_INIT = 7\n",
    "    # random.seed(SEED_INIT)\n",
    "    # tf.random.set_seed(SEED_INIT)\n",
    "    # np.random.seed(SEED_INIT)\n",
    "\n",
    "    DEBUG_STACKTRACE = 0\n",
    "    DEBUG_USING_GPU = 0   #no gpu utilization on 0, 1 is gpu utilization\n",
    "    NUM_PROCESSORS = 10\n",
    "    ITERATIONS = 20\n",
    "\n",
    "    # make comparisons lower case and include wild card character at the end of each to catch anomalous file extensions like xlsx, etc.\n",
    "    EXTENSIONS = [\".nc\"]\n",
    "    LOWER_EXTENSIONS = [x.lower() for x in EXTENSIONS]\n",
    "\n",
    "    THE_DEVICE_NAME = \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
    "    if DEBUG_USING_GPU == 1:\n",
    "        THE_DEVICE_NAME = \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    # GPU Setup (for multiple GPU devices)\n",
    "    device = torch.cuda.current_device()\n",
    "\n",
    "    # softare watermark\n",
    "    lib_diagnostics()\n",
    "\n",
    "    # hardware specs\n",
    "    get_hardware_stats()\n",
    "\n",
    "    # - Core workhorse routine\n",
    "    process(input_directory)\n",
    "\n",
    "    # - Save the results\n",
    "    # save_output()\n",
    "\n",
    "    end_t = perf_counter()\n",
    "    print(\"END PROGRAM\")\n",
    "    print(f\"Elapsed time: {end_t - start_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{self.id}^{self.calculate_stats(\"runtime\")}^{self.calculate_stats(\"io_disk_read_count\")}^{self.calculate_stats(\"io_disk_write_count\")}^{self.calculate_stats(\"io_os_read_count\")}^{self.calculate_stats(\"io_os_write_count\")}^{self.calculate_stats(\"io_disk_read_time\")}^{self.calculate_stats(\"io_disk_write_time\")}^{self.calculate_stats(\"io_os_read_time\")}^{self.calculate_stats(\"io_os_write_time\")}^{self.calculate_stats(\"mem_current\")}^{self.calculate_stats(\"mem_peak\")} "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "STEM-004_ComputerVision.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "033cb5a8c21c4ca5a13b5d03e8b78fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebb9ca01cf52473699f21f15c5ec7d34",
      "placeholder": "​",
      "style": "IPY_MODEL_689fe090492947608518efc9f3bd6d5d",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n          "
     }
    },
    "0d8988d435644a928829ef71435a3e83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1015b1ce380943d48c7a6386444c768a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f5405303114146a28663cb1c46db7beb",
       "IPY_MODEL_bdc74a6d20a041819b6fc6d1c3f85a25",
       "IPY_MODEL_033cb5a8c21c4ca5a13b5d03e8b78fa0"
      ],
      "layout": "IPY_MODEL_2b447186de7a4571821ca59295744ce3"
     }
    },
    "202c8798fdda4778bf09a2124c37a6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "usfs-ai-bootcamp",
       "usfa-ai-advanced-training",
       "I will setup my own"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Set Your Project:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_dd3de71c792f4762af3b280ac9b7f5e6",
      "style": "IPY_MODEL_fc3fc48bdebe4501998819fbda36152c"
     }
    },
    "2b447186de7a4571821ca59295744ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "31b012d7d41a4d06933aa38e8e3bd74a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "479233081fbc44c8afe9a044ebb95faf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "689fe090492947608518efc9f3bd6d5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad3b9f4855d541e49303c03fb9f07db7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Accept",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_f196621910824a00a97d06e5433cb294",
      "style": "IPY_MODEL_31b012d7d41a4d06933aa38e8e3bd74a",
      "tooltip": ""
     }
    },
    "bdc74a6d20a041819b6fc6d1c3f85a25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_202c8798fdda4778bf09a2124c37a6c7",
       "IPY_MODEL_ad3b9f4855d541e49303c03fb9f07db7"
      ],
      "layout": "IPY_MODEL_0d8988d435644a928829ef71435a3e83"
     }
    },
    "cf363ab273bf4eeba7fbea4d79ed3a64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd3de71c792f4762af3b280ac9b7f5e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebb9ca01cf52473699f21f15c5ec7d34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f196621910824a00a97d06e5433cb294": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5405303114146a28663cb1c46db7beb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf363ab273bf4eeba7fbea4d79ed3a64",
      "placeholder": "​",
      "style": "IPY_MODEL_479233081fbc44c8afe9a044ebb95faf",
      "value": "\n        <center><table><tr><td><h1 style=\"font-family: Roboto;font-size: 24px\"><b>&#128721; &#9888;&#65039; WARNING &#9888;&#65039;\t&#128721; </b></h1></td></tr></table</center></br></br>\n\n        <table><tr><td>\n            <span style=\"font-family: Tahoma;font-size: 18\">\n              This notebook was designed to work in Jupyter Notebook or Google Colab with the understnading that certain permissions might be enabled.</br>\n              Please verify that you are in the appropriate project and that the:</br>\n              <center><code><b>PROJECT_ID</b></code> </br></center>\n              aligns with the Project Id in the upper left corner of this browser and that the location:\n              <center><code><b>LOCATION</b></code> </br></center>\n              aligns with the instructions provided.\n            </span>\n          </td></tr></table></br></br>\n\n    "
     }
    },
    "fc3fc48bdebe4501998819fbda36152c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
