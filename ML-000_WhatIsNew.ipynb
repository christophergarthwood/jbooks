{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's New for TensorFlow 2.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.tensorflow.org/2022/05/whats-new-in-tensorflow-29.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 2.9 has been released! Highlights include performance improvements with oneDNN, and the release of DTensor, a new API for model distribution that can be used to seamlessly move from data parallelism to model parallelism\n",
    "\n",
    "Improvements to the core library, including Eigen and tf.function unification, deterministic behavior, and new support for Windows' WSL2. \n",
    "\n",
    "The oneDNN improvements are applicable to all Linux x86 packages and for CPUs with neural-network-focused hardware features found on 2nd Gen Intel Xeon Scalable processors and newer CPUs. Intel calls this performance optimization “software AI acceleration” and says it can make a measurable impact in certain cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved CPU performance: oneDNN by default\n",
    "\n",
    "The oneDNN performance library with TensorFlow for Intel CPUs. Since TensorFlow 2.5, TensorFlow has had experimental support for oneDNN, which could provide up to a 4x performance improvement. In TensorFlow 2.9, they are turning on oneDNN optimizations by default on Linux x86 packages and for CPUs with neural-network-focused hardware features such as AVX512_VNNI, AVX512_BF16, AMX, and others, which are found on Intel Cascade Lake and newer CPUs.\n",
    "\n",
    "Users running TensorFlow with oneDNN optimizations enabled might observe slightly different numerical results from when the optimizations are off. This is because floating-point round-off approaches and order differ, and can create slight errors. If this causes issues for you, turn the optimizations off by setting TF_ENABLE_ONEDNN_OPTS=0 before running your TensorFlow programs. To enable or re-enable them, set TF_ENABLE_ONEDNN_OPTS=1 before running your TensorFlow program. To verify that the optimizations are on, look for a message beginning with \"oneDNN custom operations are on\" in your program log. \n",
    "\n",
    "\n",
    "To enable ONEDNN for Intel:\n",
    "`export TF_ENABLE_ONEDNN_OPTS=1`\n",
    "\n",
    "to disable ONEDNN for Intel:\n",
    "\n",
    "`export TF_ENABLE_ONEDNN_OPTS=0`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Implementation\n",
    "\n",
    "Watch the output displayed below.  Restart the session (Python Kernel), change the environment variables and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 17:50:56.567951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version  2.4.1\n",
      "Num Physical GPU's Available: 0 \n",
      "Num Logical  GPU's Available: 0 \n",
      "Num CPU's Available: 1 \n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 21632)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2769024   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 2,770,634\n",
      "Trainable params: 2,770,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 17:50:57.852238: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-12-16 17:50:57.852291: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-16 17:50:57.852310: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-0a9740): /proc/driver/nvidia/version does not exist\n",
      "2022-12-16 17:50:57.853091: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 17:50:57.859780: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2793435000 Hz\n",
      "2022-12-16 17:50:57.860518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x31c5ce0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-16 17:50:57.860542: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SEED_INIT=7\n",
    "\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"]= \"1\"       #turn off OneDNN Intel Optimization, already on by default as of 2.9\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"      #turn of utilization of any GPU's\n",
    "os.environ['TF_XLA_FLAGS']         = \"--tf_xla_enable_xla_devices\"\n",
    "                                               #XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes.\n",
    "os.environ[\"XLA_FLAGS\"]            = \"--xla_gpu_cuda_data_dir=/usr/local/cuda-11.8/lib64/libcudart.so.11.0\"\n",
    "\n",
    "    \n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED_INIT)\n",
    "\n",
    "print(\"Version \",tf.__version__)\n",
    "\n",
    "tf.config.set_soft_device_placement(False)      #True to see verbose details\n",
    "tf.debugging.set_log_device_placement(False)    #True to see verbose details\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num Physical GPU's Available: {} \".format(len(tf.config.experimental.list_physical_devices('GPU'))))\n",
    "print(\"Num Logical  GPU's Available: {} \".format(len(tf.config.experimental.list_logical_devices('GPU'))))\n",
    "print(\"Num CPU's Available: {} \".format(len(tf.config.experimental.list_physical_devices('CPU'))))\n",
    "\n",
    "with tf.device(f\"/job:localhost/replica:0/task:0/device:CPU:0\"):\n",
    "    #sample Model\n",
    "    model = tf.keras.Sequential([\n",
    "          tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "          tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "          tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          tf.keras.layers.Dense(128, activation='relu'),\n",
    "          tf.keras.layers.Dense(10)\n",
    "      ])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parallelism with DTensor\n",
    "\n",
    "DTensor is a new TensorFlow API for distributed model processing that allows models to seamlessly move from data parallelism to single program multiple data (SPMD) based model parallelism, including spatial partitioning. This means you have tools to easily train models where the model weights or inputs are so large they don’t fit on a single device. (If you are familiar with Mesh TensorFlow in TF1, DTensor serves a similar purpose.)\n",
    "\n",
    "DTensor is designed with the following principles at its core:\n",
    "\n",
    "A device-agnostic API: This allows the same model code to be used on CPU, GPU, or TPU, including models partitioned across device types.\n",
    "Multi-client execution: Removes the coordinator and leaves each task to drive its locally attached devices, allowing scaling a model with no impact to startup time.\n",
    "A global perspective vs. per-replica: Traditionally with TensorFlow, distributed model code is written around replicas, but with DTensor, model code is written from the global perspective and per replica code is generated and run by the DTensor runtime. Among other things, this means no uncertainty about whether batch normalization is happening at the global level or the per replica level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Implementation\n",
    "\n",
    "https://www.tensorflow.org/guide/dtensor_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for WSL2\n",
    "\n",
    "The Windows Subsystem for Linux lets developers run a Linux environment directly on Windows, without the overhead of a traditional virtual machine or dual boot setup. TensorFlow now supports WSL2 out of the box, including GPU acceleration. Please see the documentation for more details about the requirements and how to install WSL2 on Windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Training with Keras\n",
    "\n",
    "In TensorFlow 2.9, a new experimental version of the Keras Optimizer API was released, tf.keras.optimizers.experimental. The API provides a more unified and expanded catalog of built-in optimizers which can be more easily customized and extended.\n",
    "\n",
    "In a future release, tf.keras.optimizers.experimental.Optimizer (and subclasses) will replace tf.keras.optimizers.Optimizer (and subclasses), which means that workflows using the legacy Keras optimizer will automatically switch to the new optimizer. The current (legacy) tf.keras.optimizers.* API will still be accessible via tf.keras.optimizers.legacy.*, such as tf.keras.optimizers.legacy.Adam.\n",
    "\n",
    "Here are some highlights of the new optimizer class:\n",
    "\n",
    "Incrementally faster training for some models.\n",
    "Easier to write customized optimizers.\n",
    "Built-in support for moving average of model weights (\"Polyak averaging\").\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "    + https://blog.tensorflow.org/2022/05/whats-new-in-tensorflow-29.html\n",
    "    + https://www.tensorflow.org/xla\n",
    "    + https://github.com/oneapi-src/oneDNN\n",
    "    + https://venturebeat.com/ai/tensorflow-now-defaults-to-intel-onednn-ai-optimizations/\n",
    "    + https://www.intel.com/content/www/us/en/newsroom/news/intel-onednn-speeds-ai-optimizations-in-tensorflow.html#gs.kqp0t8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
